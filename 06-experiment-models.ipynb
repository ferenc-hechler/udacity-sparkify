{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15fbf17d-596c-419b-ac66-e8126275f46a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# \\[06\\] Experiment Models\n",
    "\n",
    "## Setup Spark Session\n",
    "\n",
    "for a detailed description what is done here see [01-setup-spark-session.ipynb](01-setup-spark-session.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "df5acf66-8ff9-4d6f-9d94-f36e3a90c38d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### SETUP SPARK SESSION \"Sparkify\"\n",
      "Spark version: 3.3.2\n"
     ]
    }
   ],
   "source": [
    "EVENT_DATA_URL = \"s3a://udacity-dsnd/sparkify/sparkify_event_data.json\"\n",
    "# EVENT_DATA_URL = \"s3a://udacity-dsnd/sparkify/mini_sparkify_event_data.json\"\n",
    "\n",
    "CLEAN_DATA_URL = EVENT_DATA_URL.replace(\"/sparkify/\", \"/sparkify/output/02-cleaned-\")\n",
    "WEEK_AGGREGATED_DATA_URL = EVENT_DATA_URL.replace(\"/sparkify/\", \"/sparkify/output/04-week-aggregated-\")\n",
    "MODEL_URL = EVENT_DATA_URL.replace(\"/sparkify/\", \"/sparkify/output/05-model-\").replace(\".json\", \"\")\n",
    "TESTTRAIN_DATA_URL = EVENT_DATA_URL.replace(\"/sparkify/\", \"/sparkify/output/06-testtrain-\")\n",
    "\n",
    "# CHURN=\"cancel\"\n",
    "# CHURN=\"down\"\n",
    "CHURN=\"canceldown\"\n",
    "\n",
    "EXECUTOR_INSTANCES = 2\n",
    "EXECUTOR_MEM = '6g'\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from cryptography.fernet import Fernet\n",
    "import base64\n",
    "import socket\n",
    "\n",
    "!./install-s3-jars.sh\n",
    "\n",
    "def decrypt(encrypted_text):\n",
    "    \"\"\"\n",
    "    decrypts an encrypted text. The seed (master-password) for decryption is read from the file \".seed.txt\"\n",
    "    \n",
    "    Input: encrypted_text\n",
    "    \n",
    "    Output: the decrypted text. If the text was not encrypted with the same seed, \n",
    "            an exception is raised.\n",
    "    \"\"\"\n",
    "    with open('.seed.txt') as f:\n",
    "        seed = f.read().strip()\n",
    "    return Fernet(base64.b64encode((seed*32)[:32].encode('ascii')).decode('ascii')).decrypt(encrypted_text.encode('ascii')).decode('ascii')\n",
    "\n",
    "AWS_ACCESS_KEY_ID='V6ge1JcQpvyYGJjb'\n",
    "AWS_SECRET_ACCESS_KEY = decrypt('gAAAAABkDFI6865LaVJVgtTYo0aMx9-JTPbTo6cwOUjg5eNNPsZhBDoHbRZ8xuXQT0ImNfvqcecZuoJd1VzYQEpBaxyCnKvosii8O1KeqoL2NwKdKtL_AUfT4eW4dvJVP--VjEvc0gB4')\n",
    "OWN_IP=socket.gethostbyname(socket.gethostname())\n",
    "APP_NAME = \"Sparkify\"\n",
    "SPARK_MASTER = \"spark://bit-spark-master-svc.spark.svc.cluster.local:7077\"\n",
    "S3_HOST = \"minio-api-service.minio.svc\"\n",
    "\n",
    "print(f'### SETUP SPARK SESSION \"{APP_NAME}\"')\n",
    "spark = SparkSession.builder \\\n",
    "    .master(SPARK_MASTER) \\\n",
    "    .config(\"spark.jars\",\"/home/jovyan/jars/aws-java-sdk-bundle-1.11.1026.jar,/home/jovyan/jars/hadoop-aws-3.3.2.jar\") \\\n",
    "    .config(\"spark.driver.host\", OWN_IP) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", S3_HOST) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", AWS_ACCESS_KEY_ID) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", AWS_SECRET_ACCESS_KEY) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.executor.instances\", EXECUTOR_INSTANCES) \\\n",
    "    .config(\"spark.executor.memory\", EXECUTOR_MEM) \\\n",
    "    .appName(APP_NAME).getOrCreate()\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"WARN\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1281929a-6fdb-40bb-927e-db177696289f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### LOAD DATA s3a://udacity-dsnd/sparkify/output/04-week-aggregated-sparkify_event_data.json\n",
      "### PERSIST\n"
     ]
    }
   ],
   "source": [
    "print(f\"### LOAD DATA {WEEK_AGGREGATED_DATA_URL}\")\n",
    "df_userweek = spark.read.json(WEEK_AGGREGATED_DATA_URL)\n",
    "print(f\"### PERSIST\")\n",
    "df_userweek_persist = df_userweek.persist()\n",
    "df_userweek = df_userweek_persist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d445f9-0f20-4f27-8d20-b30b50356599",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "75e62365-bffe-4b21-827b-087d467adc6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier, RandomForestClassificationModel, DecisionTreeClassifier, DecisionTreeClassificationModel, LinearSVC\n",
    "from pyspark.ml.feature import RegexTokenizer, VectorAssembler, Normalizer, StandardScaler, MinMaxScaler, MaxAbsScaler\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.types import IntegerType\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import datetime\n",
    "\n",
    "# timestamp constants for ts in milliseconds\n",
    "one_hour =        60*60*1000  #     3.600.000\n",
    "one_day =      24*60*60*1000  #    86.400.000\n",
    "one_week =   7*24*60*60*1000  #   604.800.000\n",
    "one_month = 28*24*60*60*1000  # 2.419.200.000\n",
    "\n",
    "\n",
    "# weeks to look into the future from the predict-timestamp for label\n",
    "FUTURE_LOOKAHEAD_WEEKS = 1\n",
    "# weeks to look into the past from the predict-timestamp for new history\n",
    "PAST_NEAR_HISTORY_WEEKS = 1\n",
    "# weeks to look into the past from the predict-timestamp for old history\n",
    "PAST_OLD_HISTORY_WEEKS = 4\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "70cbefce-3e78-4a5e-b2b4-b82f70482dbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prefix_columns(df_orig, prefix, do_not_change_cols):\n",
    "    newcols = [prefix+col if not col in do_not_change_cols else col for col in df_orig.columns]\n",
    "    return df_orig.toDF(*newcols)\n",
    "\n",
    "def aggregate_week_data(from_week, to_week):\n",
    "    \"\"\"\n",
    "    Input: from_week, to_week\n",
    "    Output: aggregated sum data for the weeks from_week..to_week (both including)\n",
    "    \"\"\"\n",
    "    dropcols = [\"paid\", \"usermale\", \"userregistration\", \"wid\"]\n",
    "    df_weeks = df_userweek.where((F.col(\"wid\")>=from_week)&(F.col(\"wid\")<=to_week))\n",
    "    if from_week == to_week:\n",
    "        # no aggregation necessary, if there is only one week\n",
    "        return df_weeks.drop(*dropcols)\n",
    "    aggs = [F.sum(F.col(col)).alias(col) for col in df_weeks.columns if not col in [\"userId\", *dropcols]]\n",
    "    df_weeks = df_weeks.groupBy(\"userId\").agg(*aggs)\n",
    "    return df_weeks    \n",
    "\n",
    "\n",
    "def create_test_data(current_week):\n",
    "\n",
    "    label_week_min = current_week-FUTURE_LOOKAHEAD_WEEKS\n",
    "    label_week_max = current_week-1\n",
    "\n",
    "    newhistory_week_min = current_week\n",
    "    newhistory_week_max = current_week+PAST_NEAR_HISTORY_WEEKS-1\n",
    "\n",
    "    oldhistory_week_min = newhistory_week_max+1\n",
    "    oldhistory_week_max = current_week+PAST_OLD_HISTORY_WEEKS-1\n",
    "    \n",
    "    df_user = df_userweek.where(F.col(\"wid\") == newhistory_week_min).select(\"userId\", \"wid\", \"paid\", \"usermale\", \"userregistration\")\n",
    "    df_user = df_user.withColumn(\"userregistration\", F.col(\"userregistration\")-7*newhistory_week_min)\n",
    "\n",
    "    df_label = aggregate_week_data(label_week_min, label_week_max)\n",
    "    df_newhistory = aggregate_week_data(newhistory_week_min, newhistory_week_max)\n",
    "    df_oldhistory = aggregate_week_data(oldhistory_week_min, oldhistory_week_max)\n",
    "\n",
    "    if CHURN==\"cancel\":\n",
    "        print(f\"CHURNCANCEL\")\n",
    "        df_label = df_label.withColumn(\"label\", F.when(F.col(\"pg_cancellation_confirmation\")>0, F.lit(1)).otherwise(F.lit(0))).select(\"userid\", \"label\")\n",
    "    elif CHURN==\"down\":\n",
    "        print(f\"CHURNDOWN\")\n",
    "        df_label = df_label.withColumn(\"label\", F.when(F.col(\"pg_submit_downgrade\")>0, F.lit(1)).otherwise(F.lit(0))).select(\"userid\", \"label\")\n",
    "    else: \n",
    "        print(f\"CHURNCANCELDOWN\")\n",
    "        df_label = df_label.withColumn(\"label\", F.when(F.col(\"pg_cancellation_confirmation\")+F.col(\"pg_submit_downgrade\")>0, F.lit(1)).otherwise(F.lit(0))).select(\"userid\", \"label\")\n",
    "    df_user = df_user.join(df_label, \"userId\")\n",
    "\n",
    "    df_user = df_user.join(prefix_columns(df_newhistory, \"nh_\", [\"userId\"]), \"userId\")\n",
    "    df_user = df_user.join(prefix_columns(df_oldhistory, \"oh_\", [\"userId\"]), \"userId\")\n",
    "\n",
    "    for c in df_oldhistory.columns:\n",
    "        if not c in [\"userId\", \"session_hours\", \"session_start\"]:\n",
    "            df_user = df_user.withColumn(\"ohn_\"+c, F.col(\"oh_\"+c)/F.greatest(F.col(\"oh_session_hours\"), F.lit(0.01)))\n",
    "    df_user = df_user.withColumn(\"ohn_session_hours\", F.col(\"oh_session_hours\"))\n",
    "    df_user = df_user.withColumn(\"ohn_session_start\", F.col(\"oh_session_start\"))\n",
    "    for c in df_newhistory.columns:\n",
    "        if not c in [\"userId\", \"session_hours\", \"session_start\"]:\n",
    "            df_user = df_user.withColumn(\"nhn_\"+c, F.col(\"nh_\"+c)/F.greatest(F.col(\"nh_session_hours\"), F.lit(0.01)))\n",
    "    df_user = df_user.withColumn(\"nhn_session_hours\", F.col(\"nh_session_hours\"))\n",
    "    df_user = df_user.withColumn(\"nhn_session_start\", F.col(\"nh_session_start\"))\n",
    "    for c in df_newhistory.columns:\n",
    "        if not c in [\"userId\"]:\n",
    "            df_user = df_user.withColumn(\"r_\"+c, F.col(\"nhn_\"+c)/F.greatest(F.lit(0.01), F.col(\"ohn_\"+c)))\n",
    "    \n",
    "    df_user = df_user.persist()\n",
    "    return df_user\n",
    "\n",
    "\n",
    "def oversample(df_train):\n",
    "    df_lab0 = df_train.where(F.col(\"label\") == 0)\n",
    "    df_lab1 = df_train.where(F.col(\"label\") == 1)\n",
    "    train0cnt = df_lab0.count()\n",
    "    train1cnt = df_lab1.count()\n",
    "    oversampled_train = df_train\n",
    "    sum1cnt = train1cnt\n",
    "    while sum1cnt <= train0cnt:\n",
    "        sum1cnt = sum1cnt+train1cnt\n",
    "        print(f\"oversampling to: {sum1cnt}/{train0cnt}\")\n",
    "        oversampled_train = oversampled_train.union(df_lab1)\n",
    "    return oversampled_train\n",
    "\n",
    "def downsample(df_train):\n",
    "    df_lab0 = df_train.where(F.col(\"label\") == 0)\n",
    "    df_lab1 = df_train.where(F.col(\"label\") == 1)\n",
    "    train0cnt = df_lab0.count()\n",
    "    print(f\"orig-label-0: {train0cnt}\")\n",
    "    train1cnt = df_lab1.count()\n",
    "    print(f\"orig-label-1: {train1cnt}\")\n",
    "    df_downsampled = df_lab0.sample(fraction = train1cnt/(train0cnt+1), seed=42)\n",
    "    df_downsampled = df_downsampled.union(df_lab1)\n",
    "    print(f\"downsampled label-1 = {train1cnt}, label-0 ~ {train0cnt*train1cnt/(train0cnt+1)}\")\n",
    "    return df_downsampled\n",
    "\n",
    "\n",
    "def confuse(df_test_pred):\n",
    "    n00 = df_test_pred.where((F.col(\"label\")==0)&(F.col(\"prediction\")==0)).count()\n",
    "    n01 = df_test_pred.where((F.col(\"label\")==0)&(F.col(\"prediction\")==1)).count()\n",
    "    n10 = df_test_pred.where((F.col(\"label\")==1)&(F.col(\"prediction\")==0)).count()\n",
    "    n11 = df_test_pred.where((F.col(\"label\")==1)&(F.col(\"prediction\")==1)).count()\n",
    "    s00 = \"{:5d}\".format(n00)\n",
    "    s01 = \"{:5d}\".format(n01)\n",
    "    s10 = \"{:5d}\".format(n10)\n",
    "    s11 = \"{:5d}\".format(n11)\n",
    "    print(f\"                  \")\n",
    "    print(f\" Confusion Matrix: \")\n",
    "    print(f\"                  \")\n",
    "    print(f\"     | prediction| \")\n",
    "    print(f\"     |   0 |  1  | \")\n",
    "    print(f\" ----+-----+-----+ \")\n",
    "    print(f\" l 0 |{s00}|{s01}| \")\n",
    "    print(f\" b --+-----+-----+ \")\n",
    "    print(f\" l 1 |{s10}|{s11}| \")\n",
    "    print(f\" ----+-----+-----+ \")\n",
    "    print(f\"                   \")\n",
    "    TP = n11\n",
    "    TN = n00\n",
    "    FP = n01\n",
    "    FN = n10\n",
    "    accuracy = 0\n",
    "    if TP+TN+FP+FN!=0:\n",
    "        accuracy = (TP+TN)/(TP+TN+FP+FN)\n",
    "    precision = 0\n",
    "    if TP+FP!=0:\n",
    "        precision = TP/(TP+FP)\n",
    "    recall = 0\n",
    "    if TP+FN!=0:\n",
    "        recall = TP/(TP+FN)\n",
    "    f1 = 0\n",
    "    if precision+recall!=0:\n",
    "        f1 = 2*precision*recall/(precision+recall)\n",
    "    print(f\"CALC\")\n",
    "    print(f\"  accuraccy: {accuracy}\")\n",
    "    print(f\"  precision: {precision}\")\n",
    "    print(f\"  recall:    {recall}\")\n",
    "    print(f\"  f1:        {f1}\")\n",
    "    # https://towardsdatascience.com/matthews-correlation-coefficient-when-to-use-it-and-when-to-avoid-it-310b3c923f7e\n",
    "    mcc = -9\n",
    "    nenn = (TN+FN)*(FP+TP)*(TN+FP)*(FN+TP)\n",
    "    if nenn!=0:   \n",
    "        mcc = (TN*TP-FP*FN)/math.sqrt(nenn)\n",
    "    print(f\"  mcc:       {mcc}\")\n",
    "    return (accuracy, precision, recall, f1)\n",
    "    \n",
    "    \n",
    "def hyper_tune_rf(num_tree_values, max_depth_values):\n",
    "    best_f1 = -1\n",
    "    best_model = None\n",
    "    best_model_name = \"?\"\n",
    "    for num_trees in num_tree_values:\n",
    "        for max_depth in max_depth_values:\n",
    "            model_name = f\"rf_{num_trees}_{max_depth}\"\n",
    "            rf = RandomForestClassifier(featuresCol=\"features\", numTrees=num_trees, maxDepth=max_depth, seed=42)\n",
    "            rf_model = rf.fit(df_train)\n",
    "            predict_test  = rf_model.transform(df_test)\n",
    "            accuracy, precision, recall, f1 = confuse(predict_test)\n",
    "            print(f\"  {model_name}: f1 {f1}\")\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                best_model = rf_model\n",
    "                best_model_name = model_name\n",
    "    print(f\"best f1 {f1} for {best_model_name}\")\n",
    "    return (best_model, best_f1, best_model_name)\n",
    "\n",
    "\n",
    "def hyper_tune_lr(max_iters, reg_params, elastic_net_params):\n",
    "    # https://towardsdatascience.com/beginners-guide-to-linear-regression-with-pyspark-bfc39b45a9e9\n",
    "    evaluator = RegressionEvaluator(predictionCol=\"prediction_orig\", labelCol=\"label\", metricName=\"rmse\") \n",
    "    \n",
    "    best_err = 9999\n",
    "    best_model = None\n",
    "    best_model_name = \"?\"\n",
    "    for  max_iter in  max_iters:\n",
    "        for reg_param in reg_params:\n",
    "            for elastic_net_param in elastic_net_params:\n",
    "                model_name = f\"lr_{max_iter}_{reg_param}_{elastic_net_param}\"\n",
    "                lr = LinearRegression(featuresCol=\"features\", maxIter= max_iter, regParam=reg_param, elasticNetParam=elastic_net_param)\n",
    "                model = lr.fit(df_train)\n",
    "                predict_test  = model.transform(df_test)\n",
    "                predict_test = predict_test.withColumnRenamed(\"prediction\", \"prediction_orig\")\n",
    "                err = evaluator.evaluate(predict_test)\n",
    "                print(f\"err: {err}\")\n",
    "                thr = 0.15\n",
    "                predict_test = predict_test.withColumn(\"prediction\", F.when(F.col(\"prediction_orig\")>=thr,1).otherwise(0))\n",
    "                accuracy, precision, recall, f1 = confuse(predict_test)\n",
    "                print(f\"  {model_name}: f1 {f1}\")\n",
    "                if err < best_err:\n",
    "                    best_err = err\n",
    "                    best_model = model\n",
    "                    best_model_name = model_name\n",
    "    print(f\"best f1 {f1} for {best_model_name}\")\n",
    "    return (best_model, best_err, best_model_name)\n",
    "\n",
    "\n",
    "def hyper_tune_dt(max_depths, max_bins_list):\n",
    "    best_f1 = -1\n",
    "    best_model = None\n",
    "    best_model_name = \"?\"\n",
    "    for  max_depth in max_depths:\n",
    "        for max_bins in max_bins_list:\n",
    "            model_name = f\"dt_{max_depth}_{max_bins}\"\n",
    "            dt = DecisionTreeClassifier(featuresCol=\"features\", maxDepth=max_depth, maxBins=max_bins)\n",
    "            model = dt.fit(df_train)\n",
    "            predict_test  = model.transform(df_test)\n",
    "            accuracy, precision, recall, f1 = confuse(predict_test)\n",
    "            print(f\"  {model_name}: f1 {f1}\")\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                best_model = model\n",
    "                best_model_name = model_name\n",
    "    print(f\"best f1 {f1} for {best_model_name}\")\n",
    "    return (best_model, best_f1, best_model_name)\n",
    "\n",
    "\n",
    "def hyper_tune_sv(max_iters, reg_params):\n",
    "    best_f1 = -1\n",
    "    best_model = None\n",
    "    best_model_name = \"?\"\n",
    "    for  max_iter in max_iters:\n",
    "        for reg_param in reg_params:\n",
    "            model_name = f\"svm_{max_iter}_{reg_param}\"\n",
    "            lsvc = LinearSVC(featuresCol=\"features\", maxIter=max_iter, regParam=reg_param)\n",
    "            model = lsvc.fit(df_train)\n",
    "            predict_test  = model.transform(df_test)\n",
    "            accuracy, precision, recall, f1 = confuse(predict_test)\n",
    "            print(f\"  {model_name}: f1 {f1}\")\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                best_model = model\n",
    "                best_model_name = model_name\n",
    "    print(f\"best f1 {f1} for {best_model_name}\")\n",
    "    return (best_model, best_f1, best_model_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "689b6026-689e-4dac-a1fb-63406cfccf9a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "week1\n",
      "CHURNDOWN\n",
      "week2\n",
      "CHURNDOWN\n",
      "week3\n",
      "CHURNDOWN\n",
      "week4\n",
      "CHURNDOWN\n"
     ]
    }
   ],
   "source": [
    "print(f\"week1\")\n",
    "df_testtrain = create_test_data(1)\n",
    "print(f\"week2\")\n",
    "df_testtrain = df_testtrain.union(create_test_data(2))\n",
    "print(f\"week3\")\n",
    "df_testtrain = df_testtrain.union(create_test_data(3))\n",
    "print(f\"week4\")\n",
    "df_testtrain = df_testtrain.union(create_test_data(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2c23e48-4b7f-4af6-b41d-73c3f403b896",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### SAVING TESTTRAIN DATA s3a://udacity-dsnd/sparkify/output/06-testtrain-sparkify_event_data-churndown.json\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "# -----------------\n",
    "\n",
    "ttd_url = TESTTRAIN_DATA_URL.replace(\".json\",f\"-churn{CHURN}.json\")\n",
    "print(f\"### SAVING TESTTRAIN DATA {ttd_url}\")\n",
    "df_testtrain.write.format('json').mode('overwrite').save(ttd_url)\n",
    "print(f\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c26b914a-60cf-46c0-ac62-2203143bed51",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### LOAD TESTTRAIN DATA s3a://udacity-dsnd/sparkify/output/06-testtrain-sparkify_event_data-churncanceldown.json\n",
      "### PERSIST\n"
     ]
    }
   ],
   "source": [
    "# -----------------\n",
    "\n",
    "ttd_url = TESTTRAIN_DATA_URL.replace(\".json\",f\"-churn{CHURN}.json\")\n",
    "print(f\"### LOAD TESTTRAIN DATA {ttd_url}\")\n",
    "df_testtrain = spark.read.json(ttd_url)\n",
    "print(f\"### PERSIST\")\n",
    "df_testtrain_persist = df_testtrain.persist()\n",
    "df_testtrain = df_testtrain_persist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b5eb1a69-82fa-4190-b5b0-ad959dce6619",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_testtrain_orig = df_testtrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f4cb27d2-abea-4795-be45-0578bba434e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig-label-0: 33310\n",
      "orig-label-1: 3761\n",
      "downsampled label-1 = 3761, label-0 ~ 3760.8870943532165\n"
     ]
    }
   ],
   "source": [
    "# df_testtrain = oversample(df_testtrain)\n",
    "df_testtrain = downsample(df_testtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f3d4de6c-9882-44d0-9076-11f96fcaf2d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### CREATE FEATURE COLUMN\n",
      "### TRAIN / TEST SPLIT\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(f\"### CREATE FEATURE COLUMN\")\n",
    "\n",
    "featureCols = [\"paid\", \"usermale\", \"userregistration\"]    \n",
    "\n",
    "featureCols = [*featureCols, *[col for col in df_testtrain.columns if col.startswith(\"r_\")]]\n",
    "featureCols = [*featureCols, *[col for col in df_testtrain.columns if col.startswith(\"nh_\")]]\n",
    "\n",
    "#featureCols = [col for col in df_testtrain.columns if not col in [\"userId\", \"wid\", \"label\"]]\n",
    "assembler = VectorAssembler(inputCols=featureCols, outputCol=\"features\")\n",
    "df_testtrain_vec=assembler.transform(df_testtrain).select(\"userId\", \"wid\", \"label\",\"features\")\n",
    "\n",
    "\n",
    "df_testtrain_vec_persist = df_testtrain_vec.persist()\n",
    "df_testtrain_vec = df_testtrain_vec_persist\n",
    "\n",
    "# -----------------\n",
    "\n",
    "print(f\"### TRAIN / TEST SPLIT\")\n",
    "df_train, df_test = df_testtrain_vec.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "#df_train_orig = df_train\n",
    "#df_train = downsample(df_train)\n",
    "\n",
    "#print(f\"train: {df_train.count()}\")\n",
    "#print(f\"  l1: {df_train.where(df_train.label==1).count()}\")\n",
    "#print(f\"  l0: {df_train.where(df_train.label==0).count()}\")\n",
    "#print(f\"test: {df_test.count()}\")\n",
    "#print(f\"  l1: {df_test.where(df_test.label==1).count()}\")\n",
    "#print(f\"  l0: {df_test.where(df_test.label==0).count()}\")\n",
    "\n",
    "## Fit scaler to train dataset\n",
    "#scaler = MaxAbsScaler().setInputCol('features').setOutputCol('scaled_features')\n",
    "#df_train = df_train.drop(\"scaled_features\")\n",
    "#scaler_model = scaler.fit(df_train)\n",
    "## Scale train and test features\n",
    "#df_train = scaler_model.transform(df_train)\n",
    "#df_test = df_test.drop(\"scaled_features\")\n",
    "#df_test = scaler_model.transform(df_test)\n",
    "\n",
    "# -----------------\n",
    "df_test_orig = df_test\n",
    "df_train_orig = df_train\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "21448c99-cb4f-4d10-96ef-4631baa13554",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oversampling to: 3164/24556\n",
      "oversampling to: 4746/24556\n",
      "oversampling to: 6328/24556\n",
      "oversampling to: 7910/24556\n",
      "oversampling to: 9492/24556\n",
      "oversampling to: 11074/24556\n",
      "oversampling to: 12656/24556\n",
      "oversampling to: 14238/24556\n",
      "oversampling to: 15820/24556\n",
      "oversampling to: 17402/24556\n",
      "oversampling to: 18984/24556\n",
      "oversampling to: 20566/24556\n",
      "oversampling to: 22148/24556\n",
      "oversampling to: 23730/24556\n",
      "oversampling to: 25312/24556\n"
     ]
    }
   ],
   "source": [
    "df_train = oversample(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c523e445-7fe4-4f7f-b091-b0f496edc929",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 5173\n",
      "  l1: 2585\n",
      "  l0: 2588\n",
      "test: 2274\n",
      "  l1: 1176\n",
      "  l0: 1098\n"
     ]
    }
   ],
   "source": [
    "print(f\"train: {df_train.count()}\")\n",
    "print(f\"  l1: {df_train.where(df_train.label==1).count()}\")\n",
    "print(f\"  l0: {df_train.where(df_train.label==0).count()}\")\n",
    "print(f\"test: {df_test.count()}\")\n",
    "print(f\"  l1: {df_test.where(df_test.label==1).count()}\")\n",
    "print(f\"  l0: {df_test.where(df_test.label==0).count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6ed1fc9a-ef90-482b-983b-67361ce2e193",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oversampling to: 1400/10233\n",
      "oversampling to: 2100/10233\n",
      "oversampling to: 2800/10233\n",
      "oversampling to: 3500/10233\n",
      "oversampling to: 4200/10233\n",
      "oversampling to: 4900/10233\n",
      "oversampling to: 5600/10233\n",
      "oversampling to: 6300/10233\n",
      "oversampling to: 7000/10233\n",
      "oversampling to: 7700/10233\n",
      "oversampling to: 8400/10233\n",
      "oversampling to: 9100/10233\n",
      "oversampling to: 9800/10233\n",
      "oversampling to: 10500/10233\n"
     ]
    }
   ],
   "source": [
    "df_test = oversample(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be7e432a-4c0a-48fc-ab2f-c7a932cf4b80",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oversampling to: 2230/25009\n",
      "oversampling to: 3345/25009\n",
      "oversampling to: 4460/25009\n",
      "oversampling to: 5575/25009\n",
      "oversampling to: 6690/25009\n",
      "oversampling to: 7805/25009\n",
      "oversampling to: 8920/25009\n",
      "oversampling to: 10035/25009\n",
      "oversampling to: 11150/25009\n",
      "oversampling to: 12265/25009\n",
      "oversampling to: 13380/25009\n",
      "oversampling to: 14495/25009\n",
      "oversampling to: 15610/25009\n",
      "oversampling to: 16725/25009\n",
      "oversampling to: 17840/25009\n",
      "oversampling to: 18955/25009\n",
      "oversampling to: 20070/25009\n",
      "oversampling to: 21185/25009\n",
      "oversampling to: 22300/25009\n",
      "oversampling to: 23415/25009\n",
      "oversampling to: 24530/25009\n",
      "oversampling to: 25645/25009\n"
     ]
    }
   ],
   "source": [
    "df_train_orig = df_train\n",
    "df_train = oversample(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "98acd19d-53e1-4c78-a732-d8984c479011",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  \n",
      " Confusion Matrix: \n",
      "                  \n",
      "     | prediction| \n",
      "     |   0 |  1  | \n",
      " ----+-----+-----+ \n",
      " l 0 |  667|  431| \n",
      " b --+-----+-----+ \n",
      " l 1 |  413|  763| \n",
      " ----+-----+-----+ \n",
      "                   \n",
      "CALC\n",
      "  accuraccy: 0.6288478452066842\n",
      "  precision: 0.6390284757118928\n",
      "  recall:    0.6488095238095238\n",
      "  f1:        0.6438818565400843\n",
      "  mcc:       0.2564493005019037\n",
      "  rf_20_5: f1 0.6438818565400843\n",
      "best f1 0.6438818565400843 for rf_20_5\n"
     ]
    }
   ],
   "source": [
    "model, f1, model_name = hyper_tune_rf([20], [5])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "19ed8fda-31d2-473f-8359-b11544e5a18d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "| userId|count|\n",
      "+-------+-----+\n",
      "|1554956|    4|\n",
      "|1339528|    4|\n",
      "|1538485|    3|\n",
      "|1125943|    3|\n",
      "|1831733|    3|\n",
      "|1586895|    3|\n",
      "|1392770|    3|\n",
      "|1373602|    3|\n",
      "|1178026|    3|\n",
      "|1602181|    3|\n",
      "|1390064|    3|\n",
      "|1591353|    3|\n",
      "|1812177|    3|\n",
      "|1141231|    3|\n",
      "|1888253|    3|\n",
      "|1116029|    3|\n",
      "|1386578|    3|\n",
      "|1558736|    3|\n",
      "|1996408|    3|\n",
      "|1037209|    3|\n",
      "+-------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train.groupBy(\"userId\").count().sort(F.desc(F.col(\"count\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "242cdc8f-9cac-410f-a091-034052a32078",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.where(F.col(\"userId\")==\"1655208\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "63cd2a28-73fb-4545-8a47-154a6fa60e23",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### PREDICT TRAIN\n",
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       1.0|\n",
      "|    0|       1.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "### PREDICT TEST\n",
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "|    0|       1.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       1.0|\n",
      "|    0|       1.0|\n",
      "|    0|       1.0|\n",
      "|    0|       1.0|\n",
      "|    0|       0.0|\n",
      "|    0|       1.0|\n",
      "|    0|       0.0|\n",
      "+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "### EVALUATE PREDICTION\n",
      "+-----+--------------------+----------+--------------------+\n",
      "|label|       rawPrediction|prediction|         probability|\n",
      "+-----+--------------------+----------+--------------------+\n",
      "|    0|[8.59056034958328...|       1.0|[0.42952801747916...|\n",
      "|    0|[12.1124409849263...|       0.0|[0.60562204924631...|\n",
      "|    0|[13.7858852326191...|       0.0|[0.68929426163095...|\n",
      "|    0|[8.29733438713462...|       1.0|[0.41486671935673...|\n",
      "|    0|[9.09423883661818...|       1.0|[0.45471194183090...|\n",
      "+-----+--------------------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "The area under ROC for train set is 0.7060216921425774\n",
      "The area under ROC for test set is 0.6850268112709564\n",
      "### EVAL TRAIN:\n",
      "                  \n",
      " Confusion Matrix: \n",
      "                  \n",
      "     | prediction| \n",
      "     |   0 |  1  | \n",
      " ----+-----+-----+ \n",
      " l 0 | 1594|  994| \n",
      " b --+-----+-----+ \n",
      " l 1 |  865| 1720| \n",
      " ----+-----+-----+ \n",
      "                   \n",
      "CALC\n",
      "  accuraccy: 0.640634061473033\n",
      "  precision: 0.6337509211495947\n",
      "  recall:    0.6653771760154739\n",
      "  f1:        0.6491790903944141\n",
      "  mcc:       0.2816391488947921\n",
      "### EVAL TEST:\n",
      "                  \n",
      " Confusion Matrix: \n",
      "                  \n",
      "     | prediction| \n",
      "     |   0 |  1  | \n",
      " ----+-----+-----+ \n",
      " l 0 |  667|  431| \n",
      " b --+-----+-----+ \n",
      " l 1 |  413|  763| \n",
      " ----+-----+-----+ \n",
      "                   \n",
      "CALC\n",
      "  accuraccy: 0.6288478452066842\n",
      "  precision: 0.6390284757118928\n",
      "  recall:    0.6488095238095238\n",
      "  f1:        0.6438818565400843\n",
      "  mcc:       0.2564493005019037\n"
     ]
    }
   ],
   "source": [
    "# -----------------\n",
    "\n",
    "print(f\"### PREDICT TRAIN\")\n",
    "predict_train = model.transform(df_train)\n",
    "predict_train.select(\"label\", \"prediction\").show(10)\n",
    "print(f\"### PREDICT TEST\")\n",
    "predict_test  = model.transform(df_test)\n",
    "predict_test.select(\"label\", \"prediction\").show(10)\n",
    "\n",
    "# -----------------\n",
    "\n",
    "print(f\"### EVALUATE PREDICTION\")\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol ='rawPrediction', labelCol ='label')\n",
    "predict_test.select(\"label\", \"rawPrediction\", \"prediction\", \"probability\").show(5)\n",
    "print(\"The area under ROC for train set is {}\".format(evaluator.evaluate(predict_train)))\n",
    "print(\"The area under ROC for test set is {}\".format(evaluator.evaluate(predict_test)))\n",
    "\n",
    "print(f\"### EVAL TRAIN:\")\n",
    "confuse(predict_train)\n",
    "print(f\"### EVAL TEST:\")\n",
    "acc, prec, rec, f1 = confuse(predict_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "54651be2-0cae-437d-b15c-05bd63d6bb27",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### SAVE MODEL rf_20_5 22.481265611990008\n",
      "model saved to s3a://udacity-dsnd/sparkify/output/05-model-sparkify_event_data_rf_20_5_f1val0.225\n"
     ]
    }
   ],
   "source": [
    "# -----------------\n",
    "\n",
    "print(f\"### SAVE MODEL {model_name} {f1*100}\")\n",
    "model_url = f'{MODEL_URL}_{model_name}_f1val{round(f1,3)}'\n",
    "model.write().overwrite().save(model_url)\n",
    "print(f\"model saved to {model_url}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "b008cf24-a1ba-47a0-9358-030e895948e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('paid', 0.09554455155152178),\n",
       " ('oh_session_hours', 0.09302707160037064),\n",
       " ('oh_pg_thumbs_up', 0.07582709757031283),\n",
       " ('nh_pg_home', 0.07340830594860079),\n",
       " ('oh_pg_home', 0.0725284688157008),\n",
       " ('oh_session_start', 0.06713461595629495),\n",
       " ('oh_pg_settings', 0.050296622196977205),\n",
       " ('oh_pg_nextsong', 0.03470888922561419),\n",
       " ('nh_pg_logout', 0.031418303669647894),\n",
       " ('oh_pg_logout', 0.029338603181943555),\n",
       " ('oh_pg_thumbs_down', 0.02669508164427074),\n",
       " ('oh_pg_downgrade', 0.0263905768134274),\n",
       " ('oh_pg_add_to_playlist', 0.02189388493915634),\n",
       " ('nh_pg_thumbs_up', 0.021718938628132543),\n",
       " ('nh_pg_downgrade', 0.021610587051586076),\n",
       " ('oh_pg_login', 0.021310639336320675),\n",
       " ('nh_pg_thumbs_down', 0.020975045098593534),\n",
       " ('nh_pg_nextsong', 0.01929290425786421),\n",
       " ('nh_pg_add_to_playlist', 0.01864976547102134),\n",
       " ('nh_status_307', 0.016846133774795467),\n",
       " ('nh_session_hours', 0.016745889155579934),\n",
       " ('nh_pg_add_friend', 0.014856832748766466),\n",
       " ('oh_pg_roll_advert', 0.014159788446922988),\n",
       " ('nh_pg_settings', 0.011455073384124942),\n",
       " ('oh_status_307', 0.011261520204120721),\n",
       " ('oh_pg_about', 0.01081888688907817),\n",
       " ('oh_pg_add_friend', 0.010449979278293572),\n",
       " ('userregistration', 0.010331715693484279),\n",
       " ('nh_session_start', 0.009619201226052369),\n",
       " ('oh_pg_help', 0.00593734543223226),\n",
       " ('nh_pg_upgrade', 0.005483083691995258),\n",
       " ('nh_pg_help', 0.0054691825812952415),\n",
       " ('nh_pg_login', 0.005171923839646339),\n",
       " ('oh_status_404', 0.004981750333382591),\n",
       " ('nh_pg_roll_advert', 0.004403692638058564),\n",
       " ('nh_pg_submit_downgrade', 0.004249893599462164),\n",
       " ('oh_pg_save_settings', 0.0034216769552920688),\n",
       " ('nh_pg_error', 0.0024428874818185698),\n",
       " ('oh_pg_submit_upgrade', 0.0019265281149218618),\n",
       " ('oh_pg_error', 0.0019210946597746476),\n",
       " ('oh_pg_upgrade', 0.0014654235781669683),\n",
       " ('nh_pg_about', 0.0013718261158678826),\n",
       " ('oh_pg_submit_downgrade', 0.0011572777978853636),\n",
       " ('nh_status_404', 0.0010957622431648402),\n",
       " ('usermale', 0.0007544194723368592),\n",
       " ('nh_pg_submit_upgrade', 0.00043125770612211737),\n",
       " ('nh_pg_cancel', 0.0),\n",
       " ('nh_pg_cancellation_confirmation', 0.0),\n",
       " ('nh_pg_register', 0.0),\n",
       " ('nh_pg_save_settings', 0.0),\n",
       " ('nh_pg_submit_registration', 0.0),\n",
       " ('oh_pg_cancel', 0.0),\n",
       " ('oh_pg_cancellation_confirmation', 0.0),\n",
       " ('oh_pg_register', 0.0),\n",
       " ('oh_pg_submit_registration', 0.0)]"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featimp = model.featureImportances\n",
    "nameimp = {}\n",
    "for i in range(len(featimp)):\n",
    "    nameimp[featureCols[i]] = featimp[i]\n",
    "sorted(nameimp.items(), key=lambda x:-x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2614fba5-caf1-4f40-a7f9-7481366b5d03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fcc868-f8e5-4498-87c0-7fd090587025",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1087fe0-1964-4c85-a5de-83be14d5fc6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "87d4cb43-ef96-4630-9844-f463b7b03c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### STOP SPARK SESSION\n"
     ]
    }
   ],
   "source": [
    "print(\"### STOP SPARK SESSION\")\n",
    "spark.stop()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899ac3c4-e4ee-42f3-88c8-a46ae17737df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
