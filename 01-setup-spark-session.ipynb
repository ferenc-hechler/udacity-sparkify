{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15fbf17d-596c-419b-ac66-e8126275f46a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Setup Spark session\n",
    "\n",
    "## Variables\n",
    "\n",
    "Define important configuration variables here. \n",
    "They are used later for setting up the Spark session.\n",
    "Also define the URL to the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f59da53f-d255-47e3-a5f3-df842f3b3069",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# number of worker instances to be used in this session\n",
    "EXECUTOR_INSTANCES = 2\n",
    "\n",
    "# memory per worker\n",
    "EXECUTOR_MEM = '6g'\n",
    "\n",
    "# S3 URL for accessing the data (switch here to change from big to mini data)\n",
    "# EVENT_DATA_URL = \"s3a://udacity/sparkify/sparkify_event_data.json\"\n",
    "EVENT_DATA_URL = \"s3a://udacity-dsnd/sparkify/mini_sparkify_event_data.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f054ee9f-4e64-40b3-8231-2a58363d2932",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Install S3 Jars\n",
    "\n",
    "For accessing S3 two additional jar files have to be downloaded.\n",
    "This is done in a simple shell script, which first checks for the existence.\n",
    "If the JAR files are missing they are downloaded from the central Maven repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd6dac9a-4dea-4875-9b63-7d33d0333f37",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOWNLOADING necessary jar files for accessing S3 buckets for Spark 3.3.2\n"
     ]
    }
   ],
   "source": [
    "!./install-s3-jars.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411ae6d1-2015-4602-836b-1d5cd3b48854",
   "metadata": {
    "tags": []
   },
   "source": [
    "## S3 credentials\n",
    "\n",
    "For accessing the S3 bucket, credentials are needed.  \n",
    "To avoid checking in the credentials into the GIT repository,  \n",
    "the credentials are encrypted with the following two helper functions:  \n",
    "<sup>(Note: The \".seed.txt\" file contains the master-password for en-/decryption. It is not checked into the GIT repository, see .gitignore)<sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74e368e1-1e22-4cd5-a399-333ae22e7ce3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from cryptography.fernet import Fernet\n",
    "import base64\n",
    "\n",
    "def decrypt(encrypted_text):\n",
    "    \"\"\"\n",
    "    decrypts an encrypted text. The seed (master-password) for decryption is read from the file \".seed.txt\"\n",
    "    \n",
    "    Input: encrypted_text\n",
    "    \n",
    "    Output: the decrypted text. If the text was not encrypted with the same seed, \n",
    "            an exception is raised.\n",
    "    \"\"\"\n",
    "    with open('.seed.txt') as f:\n",
    "        seed = f.read().strip()\n",
    "    return Fernet(base64.b64encode((seed*32)[:32].encode('ascii')).decode('ascii')).decrypt(encrypted_text.encode('ascii')).decode('ascii')\n",
    "\n",
    "def encrypt(plain_text):\n",
    "    \"\"\"\n",
    "    encrypts a given text. The seed (master-password) for encryption is read from the file \".seed.txt\"\n",
    "    \n",
    "    Input: plain_text\n",
    "    \n",
    "    Output: the encrypted text\n",
    "    \"\"\"\n",
    "    with open('.seed.txt') as f:\n",
    "        seed = f.read().strip()\n",
    "    return Fernet(base64.b64encode((seed*32)[:32].encode('ascii')).decode('ascii')).encrypt(plain_text.encode('ascii')).decode('ascii')\n",
    "\n",
    "\n",
    "# technical account with read S3 buckets permission\n",
    "AWS_ACCESS_KEY_ID='V6ge1JcQpvyYGJjb'\n",
    "AWS_SECRET_ACCESS_KEY = decrypt('gAAAAABkDFI6865LaVJVgtTYo0aMx9-JTPbTo6cwOUjg5eNNPsZhBDoHbRZ8xuXQT0ImNfvqcecZuoJd1VzYQEpBaxyCnKvosii8O1KeqoL2NwKdKtL_AUfT4eW4dvJVP--VjEvc0gB4')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc46847-2ccc-48b4-bdf4-b2aa19cf0e60",
   "metadata": {},
   "source": [
    "## Jupyter Host IP for communication\n",
    "Running the Jupyter notebook in a Kubernetes cluster together with Spark, \n",
    "the communication with the worker nodes needs to know the IP address of the submitter, \n",
    "because the local hostname (POD) is not resolvable (at least in my environment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e914736-5a30-49fa-b760-06ea18d4cd9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import socket\n",
    "\n",
    "OWN_IP=socket.gethostbyname(socket.gethostname())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4c2e04-7a5c-429e-907f-77bcc47764cb",
   "metadata": {},
   "source": [
    "## Create Spark Session\n",
    "\n",
    "Create a new Spark Session \"Sparkify\". The configuration is specific for my cluster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df5acf66-8ff9-4d6f-9d94-f36e3a90c38d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### SETUP SPARK SESSION \"Sparkify\"\n",
      "Spark version: 3.3.2\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "## Name of the Spark Session to create\n",
    "APP_NAME = \"Sparkify\"\n",
    "\n",
    "## Master of the Spark Cluster\n",
    "SPARK_MASTER = \"spark://bit-spark-master-svc.spark.svc.cluster.local:7077\"\n",
    "\n",
    "## S3 Host (MinIO is a S3 compatible storage)\n",
    "S3_HOST = \"minio-api-service.minio.svc\"\n",
    "\n",
    "print(f'### SETUP SPARK SESSION \"{APP_NAME}\"')\n",
    "spark = SparkSession.builder \\\n",
    "    .master(SPARK_MASTER) \\\n",
    "    .config(\"spark.jars\",\"/home/jovyan/jars/aws-java-sdk-bundle-1.11.1026.jar,/home/jovyan/jars/hadoop-aws-3.3.2.jar\") \\\n",
    "    .config(\"spark.driver.host\", OWN_IP) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", S3_HOST) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", AWS_ACCESS_KEY_ID) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", AWS_SECRET_ACCESS_KEY) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.executor.instances\", EXECUTOR_INSTANCES) \\\n",
    "    .config(\"spark.executor.memory\", EXECUTOR_MEM) \\\n",
    "    .appName(APP_NAME).getOrCreate()\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "\n",
    "## set log level to WARN\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"WARN\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ad1c1e-828b-4d85-b06e-093a4787d957",
   "metadata": {},
   "source": [
    "## Load Data \n",
    "\n",
    "A Dataframe `df` is loaded from the given S3 url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d21af32-b058-44e8-8769-4049cd82974d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### LOAD DATA s3a://udacity-dsnd/sparkify/mini_sparkify_event_data.json\n",
      "finished loading data\n",
      "### PERSIST\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[artist: string, auth: string, firstName: string, gender: string, itemInSession: bigint, lastName: string, length: double, level: string, location: string, method: string, page: string, registration: bigint, sessionId: bigint, song: string, status: bigint, ts: bigint, userAgent: string, userId: string]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"### LOAD DATA {EVENT_DATA_URL}\")\n",
    "df = spark.read.json(EVENT_DATA_URL)\n",
    "print(f\"finished loading data\")\n",
    "\n",
    "print(f\"### PERSIST\")\n",
    "df = df.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49c82bb-9daf-40ed-889e-ef2cc8d07245",
   "metadata": {},
   "source": [
    "# Action\n",
    "\n",
    "Spark session is started, data is accessible via `df`.\n",
    "Anything can be done now   :-)\n",
    "...\n",
    "\n",
    "# Stop Session\n",
    "\n",
    "Do free up allocated resources, the session has to be stopped.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "324aa417-4e03-41d2-84ec-919f428c0d59",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### STOP SPARK SESSION\n"
     ]
    }
   ],
   "source": [
    "print(\"### STOP SPARK SESSION\")\n",
    "spark.stop()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62389675-655e-4f83-95ba-228647049888",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
