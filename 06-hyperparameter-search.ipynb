{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15fbf17d-596c-419b-ac66-e8126275f46a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# \\[06\\] Hyperparameter Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df5acf66-8ff9-4d6f-9d94-f36e3a90c38d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### SETUP SPARK SESSION \"Sparkify\"\n",
      "Spark version: 3.3.2\n"
     ]
    }
   ],
   "source": [
    "EVENT_DATA_URL = \"s3a://udacity-dsnd/sparkify/sparkify_event_data.json\"\n",
    "# EVENT_DATA_URL = \"s3a://udacity-dsnd/sparkify/mini_sparkify_event_data.json\"\n",
    "\n",
    "CLEAN_DATA_URL = EVENT_DATA_URL.replace(\"/sparkify/\", \"/sparkify/output/02-cleaned-\")\n",
    "WEEK_AGGREGATED_DATA_URL = EVENT_DATA_URL.replace(\"/sparkify/\", \"/sparkify/output/04-week-aggregated-\")\n",
    "\n",
    "\n",
    "EXECUTOR_INSTANCES = 2\n",
    "EXECUTOR_MEM = '6g'\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from cryptography.fernet import Fernet\n",
    "import base64\n",
    "import socket\n",
    "\n",
    "!./install-s3-jars.sh\n",
    "\n",
    "def decrypt(encrypted_text):\n",
    "    \"\"\"\n",
    "    decrypts an encrypted text. The seed (master-password) for decryption is read from the file \".seed.txt\"\n",
    "    \n",
    "    Input: encrypted_text\n",
    "    \n",
    "    Output: the decrypted text. If the text was not encrypted with the same seed, \n",
    "            an exception is raised.\n",
    "    \"\"\"\n",
    "    with open('.seed.txt') as f:\n",
    "        seed = f.read().strip()\n",
    "    return Fernet(base64.b64encode((seed*32)[:32].encode('ascii')).decode('ascii')).decrypt(encrypted_text.encode('ascii')).decode('ascii')\n",
    "\n",
    "AWS_ACCESS_KEY_ID='V6ge1JcQpvyYGJjb'\n",
    "AWS_SECRET_ACCESS_KEY = decrypt('gAAAAABkDFI6865LaVJVgtTYo0aMx9-JTPbTo6cwOUjg5eNNPsZhBDoHbRZ8xuXQT0ImNfvqcecZuoJd1VzYQEpBaxyCnKvosii8O1KeqoL2NwKdKtL_AUfT4eW4dvJVP--VjEvc0gB4')\n",
    "OWN_IP=socket.gethostbyname(socket.gethostname())\n",
    "APP_NAME = \"Sparkify\"\n",
    "SPARK_MASTER = \"spark://bit-spark-master-svc.spark.svc.cluster.local:7077\"\n",
    "S3_HOST = \"minio-api-service.minio.svc\"\n",
    "\n",
    "print(f'### SETUP SPARK SESSION \"{APP_NAME}\"')\n",
    "spark = SparkSession.builder \\\n",
    "    .master(SPARK_MASTER) \\\n",
    "    .config(\"spark.jars\",\"/home/jovyan/jars/aws-java-sdk-bundle-1.11.1026.jar,/home/jovyan/jars/hadoop-aws-3.3.2.jar\") \\\n",
    "    .config(\"spark.driver.host\", OWN_IP) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", S3_HOST) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", AWS_ACCESS_KEY_ID) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", AWS_SECRET_ACCESS_KEY) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.executor.instances\", EXECUTOR_INSTANCES) \\\n",
    "    .config(\"spark.executor.memory\", EXECUTOR_MEM) \\\n",
    "    .appName(APP_NAME).getOrCreate()\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"WARN\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1281929a-6fdb-40bb-927e-db177696289f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### LOAD DATA s3a://udacity-dsnd/sparkify/output/04-week-aggregated-sparkify_event_data.json\n",
      "### PERSIST\n"
     ]
    }
   ],
   "source": [
    "print(f\"### LOAD DATA {WEEK_AGGREGATED_DATA_URL}\")\n",
    "df_userweek = spark.read.json(WEEK_AGGREGATED_DATA_URL)\n",
    "print(f\"### PERSIST\")\n",
    "df_userweek_persist = df_userweek.persist()\n",
    "df_userweek = df_userweek_persist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "75e62365-bffe-4b21-827b-087d467adc6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier, RandomForestClassificationModel, DecisionTreeClassifier, DecisionTreeClassificationModel, LinearSVC\n",
    "from pyspark.ml.feature import RegexTokenizer, VectorAssembler, Normalizer, StandardScaler, MinMaxScaler, MaxAbsScaler\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.types import IntegerType\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import datetime\n",
    "\n",
    "# timestamp constants for ts in milliseconds\n",
    "one_hour =        60*60*1000  #     3.600.000\n",
    "one_day =      24*60*60*1000  #    86.400.000\n",
    "one_week =   7*24*60*60*1000  #   604.800.000\n",
    "\n",
    "def logresult(text):\n",
    "    \"\"\"\n",
    "    Input: text\n",
    "    Print given text to console and also write it at the end of the file \"result.log\".\n",
    "    This allows persisting the output of longer /multiple train runs.\n",
    "    \"\"\"\n",
    "    print(text)\n",
    "    with open(\"result.log\", \"a\") as logf:\n",
    "        logf.write(text+\"\\n\")\n",
    "\n",
    "def oversample(df_train):\n",
    "    df_lab0 = df_train.where(F.col(\"label\") == 0)\n",
    "    df_lab1 = df_train.where(F.col(\"label\") == 1)\n",
    "    train0cnt = df_lab0.count()\n",
    "    train1cnt = df_lab1.count()\n",
    "    oversampled_train = df_train\n",
    "    sum1cnt = train1cnt\n",
    "    while sum1cnt <= train0cnt:\n",
    "        sum1cnt = sum1cnt+train1cnt\n",
    "        print(f\"oversampling to: {sum1cnt}/{train0cnt}\")\n",
    "        oversampled_train = oversampled_train.union(df_lab1)\n",
    "    return oversampled_train\n",
    "\n",
    "def downsample(df_train, factor):\n",
    "    df_lab0 = df_train.where(F.col(\"label\") == 0)\n",
    "    df_lab1 = df_train.where(F.col(\"label\") == 1)\n",
    "    train0cnt = df_lab0.count()\n",
    "    print(f\"orig-label-0: {train0cnt}\")\n",
    "    train1cnt = df_lab1.count()\n",
    "    print(f\"orig-label-1: {train1cnt}\")\n",
    "    frac = train1cnt/(factor*train0cnt+1)\n",
    "    df_downsampled = df_lab0.sample(fraction = frac, seed=42)\n",
    "    df_downsampled = df_downsampled.union(df_lab1)\n",
    "    print(f\"downsampled label-1 = {train1cnt}, label-0 ~ {train0cnt*frac}\")\n",
    "    return df_downsampled\n",
    "\n",
    "def add_weight_col(df_train):\n",
    "    \"\"\"\n",
    "    Input:  Training DataFrame with a column \"label\" containing a binary classification (1 or 0)\n",
    "    Output: Newly created DataFrame, which contains an additional \"weight\" column, which compensates \n",
    "            the different frequency of both partitions. The sum of weights for label \"1\" is equal to the \n",
    "            sum of weights for the label \"0\".\n",
    "    \"\"\"\n",
    "    label_counts = df_train.agg(F.sum(F.col(\"label\")).alias(\"l1\"), F.sum(1-F.col(\"label\")).alias(\"l0\")).collect()[0]\n",
    "    l0 = label_counts.l0\n",
    "    l1 = label_counts.l1\n",
    "    w1 = l0 / (l0+l1)\n",
    "    w0 = l1 / (l0+l1)\n",
    "    print(f\"label 0: {l0}, label 1: {l1}\")\n",
    "    df_result = df_train.withColumn(\"weight\", F.when(F.col(\"label\")==1, F.lit(w1)).otherwise(F.lit(w0)))\n",
    "    return df_result\n",
    "    \n",
    "def prefix_columns(df_orig, prefix, do_not_change_cols):\n",
    "    \"\"\"\n",
    "    Input:  df_orig - original DataFrame\n",
    "            prefix - string to be added to all column names\n",
    "            do_not_change_cols - columns which should be excluded from beeing prefixed\n",
    "    Output: new DataFrame with renamed columns. All columns now start with the given prefix,\n",
    "            with the exception of the columns named in do_not_change_cols\n",
    "    \"\"\"\n",
    "    newcols = [prefix+col if not col in do_not_change_cols else col for col in df_orig.columns]\n",
    "    return df_orig.toDF(*newcols)\n",
    "\n",
    "def aggregate_week_data(from_week, to_week):\n",
    "    \"\"\"\n",
    "    Input: from_week, to_week\n",
    "    Output: aggregated sum data for the weeks from_week..to_week (both including)\n",
    "    \"\"\"\n",
    "    dropcols = [\"paid\", \"usermale\", \"userregistration\", \"wid\"]\n",
    "    df_weeks = df_userweek.where((F.col(\"wid\")>=from_week)&(F.col(\"wid\")<=to_week))\n",
    "    if from_week == to_week:\n",
    "        # no aggregation necessary, if there is only one week\n",
    "        return df_weeks.drop(*dropcols)\n",
    "    aggs = [F.sum(F.col(col)).alias(col) for col in df_weeks.columns if not col in [\"userId\", *dropcols]]\n",
    "    df_weeks = df_weeks.groupBy(\"userId\").agg(*aggs)\n",
    "    return df_weeks    \n",
    "\n",
    "\n",
    "def create_test_data(CF, current_week):\n",
    "    \"\"\"\n",
    "    Input:  CF - configuration to be used for aggregations\n",
    "            current_week - the latest week of history data\n",
    "    split data into three timeslots (future/new-history/old-histor)\n",
    "    and then use ######\n",
    "    \"\"\"\n",
    "\n",
    "    label_week_min = current_week-CF[\"FUTURE_LOOKAHEAD_WEEKS\"]\n",
    "    label_week_max = current_week-1\n",
    "\n",
    "    newhistory_week_min = current_week\n",
    "    newhistory_week_max = newhistory_week_min+CF[\"PAST_NEAR_HISTORY_WEEKS\"]-1\n",
    "\n",
    "    oldhistory_week_min = newhistory_week_max+1\n",
    "    oldhistory_week_max = oldhistory_week_min+CF[\"PAST_OLD_HISTORY_WEEKS\"]-1\n",
    "    \n",
    "    df_user = df_userweek.where(F.col(\"wid\") == newhistory_week_min).select(\"userId\", \"wid\", \"paid\", \"usermale\", \"userregistration\")\n",
    "    df_user = df_user.withColumn(\"userregistration\", F.col(\"userregistration\")-7*newhistory_week_min)\n",
    "\n",
    "    df_label = aggregate_week_data(label_week_min, label_week_max)\n",
    "    df_newhistory = aggregate_week_data(newhistory_week_min, newhistory_week_max)\n",
    "    df_oldhistory = aggregate_week_data(oldhistory_week_min, oldhistory_week_max)\n",
    "\n",
    "    if CF[\"CHURN\"]==\"canceldown\":\n",
    "        df_label = df_label.withColumn(\"label\", F.when(F.col(\"pg_cancellation_confirmation\")+F.col(\"pg_submit_downgrade\")>0, F.lit(1)).otherwise(F.lit(0))).select(\"userid\", \"label\")\n",
    "    elif CF[\"CHURN\"]==\"cancel\":\n",
    "        df_label = df_label.withColumn(\"label\", F.when(F.col(\"pg_cancellation_confirmation\")>0, F.lit(1)).otherwise(F.lit(0))).select(\"userid\", \"label\")\n",
    "    elif CF[\"CHURN\"]==\"down\":\n",
    "        df_label = df_label.withColumn(\"label\", F.when(F.col(\"pg_submit_downgrade\")>0, F.lit(1)).otherwise(F.lit(0))).select(\"userid\", \"label\")\n",
    "    else: \n",
    "        raise Exception(f'invalid value for CHURN {CF[\"CHURN\"]}')\n",
    "    df_user = df_user.join(df_label, \"userId\")\n",
    "\n",
    "    df_user = df_user.join(prefix_columns(df_newhistory, \"nh_\", [\"userId\"]), \"userId\")\n",
    "    df_user = df_user.join(prefix_columns(df_oldhistory, \"oh_\", [\"userId\"]), \"userId\")\n",
    "\n",
    "    for c in df_newhistory.columns:\n",
    "        if not c in [\"userId\", \"session_hours\", \"session_start\"]:\n",
    "            df_user = df_user.withColumn(\"nhn_\"+c, F.col(\"nh_\"+c)/F.greatest(F.col(\"nh_session_hours\"), F.lit(0.01)))\n",
    "    df_user = df_user.withColumn(\"nhn_session_hours\", F.col(\"nh_session_hours\")/CF[\"PAST_NEAR_HISTORY_WEEKS\"])\n",
    "    df_user = df_user.withColumn(\"nhn_session_start\", F.col(\"nh_session_start\")/CF[\"PAST_NEAR_HISTORY_WEEKS\"])\n",
    "\n",
    "    for c in df_oldhistory.columns:\n",
    "        if not c in [\"userId\", \"session_hours\", \"session_start\"]:\n",
    "            df_user = df_user.withColumn(\"ohn_\"+c, F.col(\"oh_\"+c)/F.greatest(F.col(\"oh_session_hours\"), F.lit(0.01)))\n",
    "    df_user = df_user.withColumn(\"ohn_session_hours\", F.col(\"oh_session_hours\")/CF[\"PAST_OLD_HISTORY_WEEKS\"])\n",
    "    df_user = df_user.withColumn(\"ohn_session_start\", F.col(\"oh_session_start\")/CF[\"PAST_OLD_HISTORY_WEEKS\"])\n",
    "    \n",
    "    for c in df_newhistory.columns:\n",
    "        if not c in [\"userId\"]:\n",
    "            df_user = df_user.withColumn(\"r_\"+c, F.col(\"nhn_\"+c)/F.greatest(F.lit(0.01), F.col(\"ohn_\"+c)))\n",
    "    for c in df_newhistory.columns:\n",
    "        if not c in [\"userId\"]:\n",
    "            df_user = df_user.withColumn(\"d_\"+c, F.col(\"nhn_\"+c)-F.col(\"ohn_\"+c))\n",
    "    \n",
    "    return df_user\n",
    "\n",
    "\n",
    "\n",
    "def confuse(df_test_pred):\n",
    "    n00 = df_test_pred.where((F.col(\"label\")==0)&(F.col(\"prediction\")==0)).count()\n",
    "    n01 = df_test_pred.where((F.col(\"label\")==0)&(F.col(\"prediction\")==1)).count()\n",
    "    n10 = df_test_pred.where((F.col(\"label\")==1)&(F.col(\"prediction\")==0)).count()\n",
    "    n11 = df_test_pred.where((F.col(\"label\")==1)&(F.col(\"prediction\")==1)).count()\n",
    "    s00 = \"{:5d}\".format(n00)\n",
    "    s01 = \"{:5d}\".format(n01)\n",
    "    s10 = \"{:5d}\".format(n10)\n",
    "    s11 = \"{:5d}\".format(n11)\n",
    "    logresult(f\"                  \")\n",
    "    logresult(f\" Confusion Matrix: \")\n",
    "    logresult(f\"                  \")\n",
    "    logresult(f\"     | prediction| \")\n",
    "    logresult(f\"     |   0 |  1  | \")\n",
    "    logresult(f\" ----+-----+-----+ \")\n",
    "    logresult(f\" l 0 |{s00}|{s01}| \")\n",
    "    logresult(f\" b --+-----+-----+ \")\n",
    "    logresult(f\" l 1 |{s10}|{s11}| \")\n",
    "    logresult(f\" ----+-----+-----+ \")\n",
    "    logresult(f\"                   \")\n",
    "    TP = n11\n",
    "    TN = n00\n",
    "    FP = n01\n",
    "    FN = n10\n",
    "    accuracy = 0\n",
    "    if TP+TN+FP+FN!=0:\n",
    "        accuracy = (TP+TN)/(TP+TN+FP+FN)\n",
    "    precision = 0\n",
    "    if TP+FP!=0:\n",
    "        precision = TP/(TP+FP)\n",
    "    recall = 0\n",
    "    if TP+FN!=0:\n",
    "        recall = TP/(TP+FN)\n",
    "    f1 = 0\n",
    "    if precision+recall!=0:\n",
    "        f1 = 2*precision*recall/(precision+recall)\n",
    "    logresult(f\"  accuraccy: {accuracy}\")\n",
    "    logresult(f\"  precision: {precision}\")\n",
    "    logresult(f\"  recall:    {recall}\")\n",
    "    logresult(f\"  f1:        {f1}\")\n",
    "    # https://towardsdatascience.com/matthews-correlation-coefficient-when-to-use-it-and-when-to-avoid-it-310b3c923f7e\n",
    "    mcc = -9\n",
    "    nenn = (TN+FN)*(FP+TP)*(TN+FP)*(FN+TP)\n",
    "    if nenn!=0:   \n",
    "        mcc = (TN*TP-FP*FN)/math.sqrt(nenn)\n",
    "    logresult(f\"  mcc:       {mcc}\")\n",
    "    return (accuracy, precision, recall, f1)\n",
    "    \n",
    "    \n",
    "def hyper_tune_rf(df_train, df_test, configstr, num_tree_values, max_depth_values):   # 20, 5\n",
    "    best_f1 = -1\n",
    "    best_model = None\n",
    "    best_model_name = \"?\"\n",
    "    for num_trees in num_tree_values:\n",
    "        for max_depth in max_depth_values:\n",
    "            model_name = f\"rf_{num_trees}_{max_depth}\"\n",
    "            logresult(f\"\")\n",
    "            logresult(f\"------------------------------------\")\n",
    "            logresult(f\"TRAINING {configstr} {model_name}\")\n",
    "            logresult(f\"------------------------------------\")\n",
    "            rf = RandomForestClassifier(featuresCol=\"features\", numTrees=num_trees, maxDepth=max_depth, weightCol=\"weight\", seed=42)\n",
    "            model = rf.fit(df_train)\n",
    "            predict_test  = model.transform(df_test)\n",
    "            accuracy, precision, recall, f1 = confuse(predict_test)\n",
    "            print(f\"  {model_name}: f1 {f1}\")\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                best_model = model\n",
    "                best_model_name = model_name\n",
    "    print(f\"best f1 {best_f1} for {best_model_name}\")\n",
    "    return (best_model, best_f1, best_model_name)\n",
    "\n",
    "\n",
    "def hyper_tune_dt(df_train, df_test, configstr, max_depths, max_bins_list):  # 5, 32\n",
    "    best_f1 = -1\n",
    "    best_model = None\n",
    "    best_model_name = \"?\"\n",
    "    for  max_depth in max_depths:\n",
    "        for max_bins in max_bins_list:\n",
    "            model_name = f\"dt_{max_depth}_{max_bins}\"\n",
    "            logresult(f\"\")\n",
    "            logresult(f\"------------------------------------\")\n",
    "            logresult(f\"TRAINING {configstr} {model_name}\")\n",
    "            logresult(f\"------------------------------------\")\n",
    "            dt = DecisionTreeClassifier(featuresCol=\"features\", maxDepth=max_depth, maxBins=max_bins, weightCol=\"weight\", seed=42)\n",
    "            model = dt.fit(df_train)\n",
    "            predict_test  = model.transform(df_test)\n",
    "            accuracy, precision, recall, f1 = confuse(predict_test)\n",
    "            print(f\"  {model_name}: f1 {f1}\")\n",
    "            if f1 > best_f1:\n",
    "                print(f\"new best f1\")\n",
    "                best_f1 = f1\n",
    "                best_model = model\n",
    "                best_model_name = model_name\n",
    "    print(f\"best f1 {best_f1} for {best_model_name}\")\n",
    "    return (best_model, best_f1, best_model_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def hyper_tune_lr(df_train, df_test, configstr, max_iters, reg_params, elastic_net_params): # 100, 0, 0\n",
    "    # https://towardsdatascience.com/beginners-guide-to-linear-regression-with-pyspark-bfc39b45a9e9\n",
    "    evaluator = RegressionEvaluator(predictionCol=\"prediction_orig\", labelCol=\"label\", metricName=\"rmse\") \n",
    "    \n",
    "    best_f1 = -1\n",
    "    best_model = None\n",
    "    best_model_name = \"?\"\n",
    "    for  max_iter in  max_iters:\n",
    "        for reg_param in reg_params:\n",
    "            for elastic_net_param in elastic_net_params:\n",
    "                model_name = f\"lr_{max_iter}_{reg_param}_{elastic_net_param}\"\n",
    "                logresult(f\"\")\n",
    "                logresult(f\"------------------------------------\")\n",
    "                logresult(f\"TRAINING {configstr} {model_name}\")\n",
    "                logresult(f\"------------------------------------\")\n",
    "                lr = LinearRegression(featuresCol=\"features\", maxIter= max_iter, regParam=reg_param, elasticNetParam=elastic_net_param, weightCol=\"weight\")\n",
    "                model = lr.fit(df_train)\n",
    "                predict_test  = model.transform(df_test)\n",
    "                predict_test = predict_test.withColumnRenamed(\"prediction\", \"prediction_orig\")\n",
    "                err = evaluator.evaluate(predict_test)\n",
    "                logresult(f\"err: {err}\")\n",
    "                thr = 0.5\n",
    "                predict_test = predict_test.withColumn(\"prediction\", F.when(F.col(\"prediction_orig\")>=thr,1).otherwise(0))\n",
    "                accuracy, precision, recall, f1 = confuse(predict_test)\n",
    "                print(f\"  {model_name}: f1 {f1}\")\n",
    "                if f1 > best_f1:\n",
    "                    print(f\"new best f1\")\n",
    "                    best_f1 = f1\n",
    "                    best_model = model\n",
    "                    best_model_name = model_name\n",
    "    print(f\"best f1 {best_f1} for {best_model_name}\")\n",
    "    return (best_model, best_f1, best_model_name)\n",
    "\n",
    "\n",
    "def hyper_tune_sv(df_train, df_test, configstr, max_iters, reg_params):   # 100, 0\n",
    "    best_f1 = -1\n",
    "    best_model = None\n",
    "    best_model_name = \"?\"\n",
    "    for  max_iter in max_iters:\n",
    "        for reg_param in reg_params:\n",
    "            model_name = f\"svm_{max_iter}_{reg_param}\"\n",
    "            logresult(f\"\")\n",
    "            logresult(f\"------------------------------------\")\n",
    "            logresult(f\"TRAINING {configstr} {model_name}\")\n",
    "            logresult(f\"------------------------------------\")\n",
    "            lsvc = LinearSVC(featuresCol=\"features\", maxIter=max_iter, regParam=reg_param, weightCol=\"weight\")\n",
    "            model = lsvc.fit(df_train)\n",
    "            predict_test  = model.transform(df_test)\n",
    "            accuracy, precision, recall, f1 = confuse(predict_test)\n",
    "            print(f\"  {model_name}: f1 {f1}\")\n",
    "            if f1 > best_f1:\n",
    "                print(f\"new best f1\")\n",
    "                best_f1 = f1\n",
    "                best_model = model\n",
    "                best_model_name = model_name\n",
    "    print(f\"best f1 {best_f1} for {best_model_name}\")\n",
    "    return (best_model, best_f1, best_model_name)\n",
    "    \n",
    "\n",
    "def hyper_tune(df_trainw, df_traind, df_test, configstr, model_config):\n",
    "    if \"rf\" in model_config:\n",
    "          model, f1, model_name = hyper_tune_rf(df_trainw, df_test, configstr, *(model_config[\"rf\"]))\n",
    "    if \"dt\" in model_config:\n",
    "          model, f1, model_name = hyper_tune_dt(df_trainw, df_test, configstr, *(model_config[\"dt\"]))\n",
    "    if \"lr\" in model_config:\n",
    "          model, f1, model_name = hyper_tune_lr(df_trainw, df_test, configstr, *(model_config[\"lr\"]))\n",
    "    if \"sv\" in model_config:\n",
    "          model, f1, model_name = hyper_tune_sv(df_trainw, df_test, configstr, *(model_config[\"sv\"]))\n",
    "    return (model, f1, model_name)\n",
    "\n",
    "MAX_WID = 8\n",
    "\n",
    "def create_train_test_data(CF):\n",
    "    current_week = CF[\"FUTURE_LOOKAHEAD_WEEKS\"]\n",
    "    history_weeks = CF[\"PAST_NEAR_HISTORY_WEEKS\"]+CF[\"PAST_OLD_HISTORY_WEEKS\"]\n",
    "    df_testtrain = create_test_data(CF, current_week)\n",
    "    while current_week+history_weeks < MAX_WID:\n",
    "        current_week = current_week+1\n",
    "        df_testtrain = df_testtrain.union(create_test_data(CF, current_week))\n",
    "    return df_testtrain\n",
    "\n",
    "def create_train_test_vector(CF, df_testtrain):\n",
    "    featureCols = [\"paid\", \"usermale\", \"userregistration\"]\n",
    "    for prefix in CF[\"FEATURE_COLS\"]:\n",
    "        featureCols = [*featureCols, *[col for col in df_testtrain.columns if col.startswith(prefix)]]\n",
    "    assembler = VectorAssembler(inputCols=featureCols, outputCol=\"features\")\n",
    "    select_cols = [\"userId\", \"wid\", \"label\",\"features\"]\n",
    "    if \"weight\" in df_testtrain.columns:\n",
    "        select_cols = [*select_cols, \"weight\"]\n",
    "    df_testtrain_vec=assembler.transform(df_testtrain).select(*select_cols)\n",
    "    return df_testtrain_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55ffae24-b569-4e7a-80ea-e10d8304c58e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "canceldown-2-1-4-nhn_ohn_\n",
      "### TRAIN / TEST SPLIT based on userId, to avoid overlapping test and train data\n",
      "### SAVING TEST DATA s3a://udacity-dsnd/sparkify/output/07-test-4-1-2-canceldown-sparkify_event_data.json\n",
      "orig-label-0: 13036\n",
      "orig-label-1: 2448\n",
      "downsampled label-1 = 2448, label-0 ~ 2447.8122267392805\n",
      "### SAVING TRAIND DATA s3a://udacity-dsnd/sparkify/output/07-traind-4-1-2-canceldown-sparkify_event_data.json\n",
      "label 0: 13036, label 1: 2448\n",
      "### SAVING TRAINW DATA s3a://udacity-dsnd/sparkify/output/07-trainw-4-1-2-canceldown-sparkify_event_data.json\n",
      "\n",
      "------------------------------------\n",
      "TRAINING canceldown-2-1-4-nhn_ohn_ dt_13_1024\n",
      "------------------------------------\n",
      "                  \n",
      " Confusion Matrix: \n",
      "                  \n",
      "     | prediction| \n",
      "     |   0 |  1  | \n",
      " ----+-----+-----+ \n",
      " l 0 | 3422| 1934| \n",
      " b --+-----+-----+ \n",
      " l 1 |  524|  540| \n",
      " ----+-----+-----+ \n",
      "                   \n",
      "  accuraccy: 0.6171339563862929\n",
      "  precision: 0.21827000808407437\n",
      "  recall:    0.5075187969924813\n",
      "  f1:        0.30525720746184287\n",
      "  mcc:       0.11187630623911869\n",
      "  dt_13_1024: f1 0.30525720746184287\n",
      "new best f1\n",
      "\n",
      "------------------------------------\n",
      "TRAINING canceldown-2-1-4-nhn_ohn_ dt_13_2048\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/home/jovyan/.condaenvs/py38/lib/python3.8/socket.py\", line 669, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 62\u001b[0m\n\u001b[1;32m     59\u001b[0m df_trainw \u001b[38;5;241m=\u001b[39m df_trainw\u001b[38;5;241m.\u001b[39munpersist()\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m MODEL \u001b[38;5;129;01min\u001b[39;00m train_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMODEL\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m---> 62\u001b[0m     model, f1, model_name \u001b[38;5;241m=\u001b[39m \u001b[43mhyper_tune\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_trainw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_traind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfigstr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMODEL\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m df_test\u001b[38;5;241m.\u001b[39munpersist()\n\u001b[1;32m     65\u001b[0m df_traind\u001b[38;5;241m.\u001b[39munpersist()\n",
      "Cell \u001b[0;32mIn[3], line 310\u001b[0m, in \u001b[0;36mhyper_tune\u001b[0;34m(df_trainw, df_traind, df_test, configstr, model_config)\u001b[0m\n\u001b[1;32m    308\u001b[0m       model, f1, model_name \u001b[38;5;241m=\u001b[39m hyper_tune_rf(df_trainw, df_test, configstr, \u001b[38;5;241m*\u001b[39m(model_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrf\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdt\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m model_config:\n\u001b[0;32m--> 310\u001b[0m       model, f1, model_name \u001b[38;5;241m=\u001b[39m \u001b[43mhyper_tune_dt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_trainw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfigstr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m model_config:\n\u001b[1;32m    312\u001b[0m       model, f1, model_name \u001b[38;5;241m=\u001b[39m hyper_tune_lr(df_traind, df_test, configstr, \u001b[38;5;241m*\u001b[39m(model_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n",
      "Cell \u001b[0;32mIn[3], line 242\u001b[0m, in \u001b[0;36mhyper_tune_dt\u001b[0;34m(df_train, df_test, configstr, max_depths, max_bins_list)\u001b[0m\n\u001b[1;32m    240\u001b[0m logresult(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m------------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    241\u001b[0m dt \u001b[38;5;241m=\u001b[39m DecisionTreeClassifier(featuresCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m, maxDepth\u001b[38;5;241m=\u001b[39mmax_depth, maxBins\u001b[38;5;241m=\u001b[39mmax_bins, weightCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m\"\u001b[39m, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m--> 242\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mdt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    243\u001b[0m predict_test  \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtransform(df_test)\n\u001b[1;32m    244\u001b[0m accuracy, precision, recall, f1 \u001b[38;5;241m=\u001b[39m confuse(predict_test)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py:383\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[0;32m--> 383\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py:380\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 380\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1320\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1313\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m~/.condaenvs/py38/lib/python3.8/socket.py:669\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    668\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 669\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    670\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    671\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_configs = [\n",
    "    {\n",
    "        \"CHURN\":                   [\"canceldown\"],\n",
    "        \"FUTURE_LOOKAHEAD_WEEKS\":  [2],\n",
    "        \"PAST_NEAR_HISTORY_WEEKS\": [1],\n",
    "        \"PAST_OLD_HISTORY_WEEKS\":  [4],\n",
    "        \"FEATURE_COLS\":            [[\"nhn_\", \"ohn_\"]],\n",
    "        \"MODEL\":                   [{\"dt\": [[13,14,15], [1024,2048]]}]\n",
    "    }\n",
    "]\n",
    "\n",
    "CF = {}\n",
    "for train_config in train_configs:\n",
    " for CHURN in train_config[\"CHURN\"]:\n",
    "  for FUTURE_LOOKAHEAD_WEEKS in train_config[\"FUTURE_LOOKAHEAD_WEEKS\"]:\n",
    "   for PAST_NEAR_HISTORY_WEEKS in train_config[\"PAST_NEAR_HISTORY_WEEKS\"]:\n",
    "    for PAST_OLD_HISTORY_WEEKS in train_config[\"PAST_OLD_HISTORY_WEEKS\"]:\n",
    "     for FEATURE_COLS in train_config[\"FEATURE_COLS\"]:\n",
    "        CF[\"CHURN\"] = CHURN\n",
    "        CF[\"FUTURE_LOOKAHEAD_WEEKS\"] = FUTURE_LOOKAHEAD_WEEKS\n",
    "        CF[\"PAST_NEAR_HISTORY_WEEKS\"] = PAST_NEAR_HISTORY_WEEKS\n",
    "        CF[\"PAST_OLD_HISTORY_WEEKS\"] = PAST_OLD_HISTORY_WEEKS\n",
    "        CF[\"FEATURE_COLS\"] = FEATURE_COLS\n",
    "        CF[\"MODEL_URL\"] = EVENT_DATA_URL.replace(\"/sparkify/\", \"/sparkify/output/07-model-{PAST_OLD_HISTORY_WEEKS}-{PAST_NEAR_HISTORY_WEEKS}-{FUTURE_LOOKAHEAD_WEEKS}-{CHURN}\").replace(\".json\", \"\")\n",
    "        CF[\"TESTTRAIN_DATA_URL\"] = EVENT_DATA_URL.replace(\"/sparkify/\", f\"/sparkify/output/07-testtrain-{PAST_OLD_HISTORY_WEEKS}-{PAST_NEAR_HISTORY_WEEKS}-{FUTURE_LOOKAHEAD_WEEKS}-{CHURN}-\")\n",
    "        prefixes = str(FEATURE_COLS).replace(\"[\",\"\").replace(\"]\",\"\").replace(\"'\",\"\").replace(\",\",\"\").replace(\" \",\"\")\n",
    "        configstr = f\"{CHURN}-{FUTURE_LOOKAHEAD_WEEKS}-{PAST_NEAR_HISTORY_WEEKS}-{PAST_OLD_HISTORY_WEEKS}-{prefixes}\"\n",
    "        print(f\"{configstr}\")\n",
    "        df_testtrain = create_train_test_data(CF)\n",
    "\n",
    "        print(f\"### TRAIN / TEST SPLIT based on userId, to avoid overlapping test and train data\")\n",
    "        df_testtrain_userids = df_testtrain.select(\"userId\").dropDuplicates()\n",
    "        df_train_userids, df_test_userids = df_testtrain_userids.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "        df_train_feat = df_testtrain.join(df_train_userids, \"userId\", \"inner\")\n",
    "        df_test_feat = df_testtrain.join(df_test_userids, \"userId\", \"inner\")\n",
    "        \n",
    "        test_url = CF[\"TESTTRAIN_DATA_URL\"].replace(\"-testtrain-\", \"-test-\")\n",
    "        traind_url = CF[\"TESTTRAIN_DATA_URL\"].replace(\"-testtrain-\", \"-traind-\")\n",
    "        trainw_url = CF[\"TESTTRAIN_DATA_URL\"].replace(\"-testtrain-\", \"-trainw-\")\n",
    "\n",
    "        print(f\"### SAVING TEST DATA {test_url}\")\n",
    "        df_test_feat.write.format('json').mode('overwrite').save(test_url)\n",
    "\n",
    "        df_traind_feat = downsample(df_train_feat, 1)\n",
    "        print(f\"### SAVING TRAIND DATA {traind_url}\")\n",
    "        df_traind_feat.write.format('json').mode('overwrite').save(traind_url)\n",
    "\n",
    "        df_trainw_feat = add_weight_col(df_train_feat)\n",
    "        print(f\"### SAVING TRAINW DATA {trainw_url}\")\n",
    "        df_trainw_feat.write.format('json').mode('overwrite').save(trainw_url)\n",
    "\n",
    "        df_test = create_train_test_vector(CF, df_test_feat)\n",
    "        df_traind = create_train_test_vector(CF, df_traind_feat)\n",
    "        df_trainw = create_train_test_vector(CF, df_trainw_feat)\n",
    "\n",
    "        df_test = df_test.unpersist()\n",
    "        df_traind = df_traind.unpersist()\n",
    "        df_trainw = df_trainw.unpersist()\n",
    "        \n",
    "        for MODEL in train_config[\"MODEL\"]:\n",
    "            model, f1, model_name = hyper_tune(df_trainw, df_traind, df_test, configstr, MODEL)\n",
    "        \n",
    "        df_test.unpersist()\n",
    "        df_traind.unpersist()\n",
    "        df_trainw.unpersist()\n",
    "        \n",
    "print(\"FINISHED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f948a898-cd63-4fc3-8b39-5fe3987b1eea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "canceldown-2-1-4-nhn_ohn_\n",
      "### LOADING TEST DATA s3a://udacity-dsnd/sparkify/output/07-test-4-1-2-canceldown-sparkify_event_data.json\n",
      "### LOADING TRAIND DATA s3a://udacity-dsnd/sparkify/output/07-traind-4-1-2-canceldown-sparkify_event_data.json\n",
      "### LOADING TRAINW DATA s3a://udacity-dsnd/sparkify/output/07-trainw-4-1-2-canceldown-sparkify_event_data.json\n",
      "\n",
      "------------------------------------\n",
      "TRAINING canceldown-2-1-4-nhn_ohn_ lr_100_0_0\n",
      "------------------------------------\n",
      "err: 16.461213930997054\n",
      "                  \n",
      " Confusion Matrix: \n",
      "                  \n",
      "     | prediction| \n",
      "     |   0 |  1  | \n",
      " ----+-----+-----+ \n",
      " l 0 | 3536| 1820| \n",
      " b --+-----+-----+ \n",
      " l 1 |  417|  647| \n",
      " ----+-----+-----+ \n",
      "                   \n",
      "  accuraccy: 0.6515576323987539\n",
      "  precision: 0.2622618565058776\n",
      "  recall:    0.6080827067669173\n",
      "  f1:        0.3664684225431889\n",
      "  mcc:       0.2050814435083378\n",
      "  lr_100_0_0: f1 0.3664684225431889\n",
      "new best f1\n",
      "\n",
      "------------------------------------\n",
      "TRAINING canceldown-2-1-4-nhn_ohn_ lr_200_0_0\n",
      "------------------------------------\n",
      "err: 16.461213930997157\n",
      "                  \n",
      " Confusion Matrix: \n",
      "                  \n",
      "     | prediction| \n",
      "     |   0 |  1  | \n",
      " ----+-----+-----+ \n",
      " l 0 | 3536| 1820| \n",
      " b --+-----+-----+ \n",
      " l 1 |  417|  647| \n",
      " ----+-----+-----+ \n",
      "                   \n",
      "  accuraccy: 0.6515576323987539\n",
      "  precision: 0.2622618565058776\n",
      "  recall:    0.6080827067669173\n",
      "  f1:        0.3664684225431889\n",
      "  mcc:       0.2050814435083378\n",
      "  lr_200_0_0: f1 0.3664684225431889\n",
      "\n",
      "------------------------------------\n",
      "TRAINING canceldown-2-1-4-nhn_ohn_ lr_400_0_0\n",
      "------------------------------------\n",
      "err: 16.461213930997108\n",
      "                  \n",
      " Confusion Matrix: \n",
      "                  \n",
      "     | prediction| \n",
      "     |   0 |  1  | \n",
      " ----+-----+-----+ \n",
      " l 0 | 3536| 1820| \n",
      " b --+-----+-----+ \n",
      " l 1 |  417|  647| \n",
      " ----+-----+-----+ \n",
      "                   \n",
      "  accuraccy: 0.6515576323987539\n",
      "  precision: 0.2622618565058776\n",
      "  recall:    0.6080827067669173\n",
      "  f1:        0.3664684225431889\n",
      "  mcc:       0.2050814435083378\n",
      "  lr_400_0_0: f1 0.3664684225431889\n",
      "\n",
      "------------------------------------\n",
      "TRAINING canceldown-2-1-4-nhn_ohn_ lr_1000_0_0\n",
      "------------------------------------\n",
      "err: 16.461213930997157\n",
      "                  \n",
      " Confusion Matrix: \n",
      "                  \n",
      "     | prediction| \n",
      "     |   0 |  1  | \n",
      " ----+-----+-----+ \n",
      " l 0 | 3536| 1820| \n",
      " b --+-----+-----+ \n",
      " l 1 |  417|  647| \n",
      " ----+-----+-----+ \n",
      "                   \n",
      "  accuraccy: 0.6515576323987539\n",
      "  precision: 0.2622618565058776\n",
      "  recall:    0.6080827067669173\n",
      "  f1:        0.3664684225431889\n",
      "  mcc:       0.2050814435083378\n",
      "  lr_1000_0_0: f1 0.3664684225431889\n",
      "best f1 0.3664684225431889 for lr_100_0_0\n",
      "FINISHED\n"
     ]
    }
   ],
   "source": [
    "train_configs = [\n",
    "    {\n",
    "        \"CHURN\":                   [\"canceldown\"],\n",
    "        \"FUTURE_LOOKAHEAD_WEEKS\":  [2],\n",
    "        \"PAST_NEAR_HISTORY_WEEKS\": [1],\n",
    "        \"PAST_OLD_HISTORY_WEEKS\":  [4],\n",
    "        \"FEATURE_COLS\":            [[\"nhn_\", \"ohn_\"]],\n",
    "        \"MODEL\":                   [\n",
    "#                                     {\"rf\": [[10,20,40], [4,5,6]]},\n",
    "#                                     {\"dt\": [[4,5,6], [16,32,64]]},\n",
    "#                                     {\"sv\": [[50,100,200], [0,0.01,0.1]]},\n",
    "                                     {\"lr\": [[50,100,200,1000], [0], [0]]},\n",
    "#                                     {\"rf\": [[20], [5]]},\n",
    "#                                     {\"dt\": [[5], [32]]},\n",
    "#                                     {\"sv\": [[100], [0]]},\n",
    "#                                     {\"lr\": [[100], [0], [0]]},\n",
    "                                   ]\n",
    "    }\n",
    "]\n",
    "\n",
    "CF = {}\n",
    "for train_config in train_configs:\n",
    " for CHURN in train_config[\"CHURN\"]:\n",
    "  for FUTURE_LOOKAHEAD_WEEKS in train_config[\"FUTURE_LOOKAHEAD_WEEKS\"]:\n",
    "   for PAST_NEAR_HISTORY_WEEKS in train_config[\"PAST_NEAR_HISTORY_WEEKS\"]:\n",
    "    for PAST_OLD_HISTORY_WEEKS in train_config[\"PAST_OLD_HISTORY_WEEKS\"]:\n",
    "     for FEATURE_COLS in train_config[\"FEATURE_COLS\"]:\n",
    "        CF[\"CHURN\"] = CHURN\n",
    "        CF[\"FUTURE_LOOKAHEAD_WEEKS\"] = FUTURE_LOOKAHEAD_WEEKS\n",
    "        CF[\"PAST_NEAR_HISTORY_WEEKS\"] = PAST_NEAR_HISTORY_WEEKS\n",
    "        CF[\"PAST_OLD_HISTORY_WEEKS\"] = PAST_OLD_HISTORY_WEEKS\n",
    "        CF[\"FEATURE_COLS\"] = FEATURE_COLS\n",
    "        CF[\"MODEL_URL\"] = EVENT_DATA_URL.replace(\"/sparkify/\", \"/sparkify/output/07-model-{PAST_OLD_HISTORY_WEEKS}-{PAST_NEAR_HISTORY_WEEKS}-{FUTURE_LOOKAHEAD_WEEKS}-{CHURN}\").replace(\".json\", \"\")\n",
    "        CF[\"TESTTRAIN_DATA_URL\"] = EVENT_DATA_URL.replace(\"/sparkify/\", f\"/sparkify/output/07-testtrain-{PAST_OLD_HISTORY_WEEKS}-{PAST_NEAR_HISTORY_WEEKS}-{FUTURE_LOOKAHEAD_WEEKS}-{CHURN}-\")\n",
    "        prefixes = str(FEATURE_COLS).replace(\"[\",\"\").replace(\"]\",\"\").replace(\"'\",\"\").replace(\",\",\"\").replace(\" \",\"\")\n",
    "        configstr = f\"{CHURN}-{FUTURE_LOOKAHEAD_WEEKS}-{PAST_NEAR_HISTORY_WEEKS}-{PAST_OLD_HISTORY_WEEKS}-{prefixes}\"\n",
    "        print(f\"{configstr}\")\n",
    "\n",
    "        test_url = CF[\"TESTTRAIN_DATA_URL\"].replace(\"-testtrain-\", \"-test-\")\n",
    "        trainw_url = CF[\"TESTTRAIN_DATA_URL\"].replace(\"-testtrain-\", \"-trainw-\")\n",
    "        traind_url = CF[\"TESTTRAIN_DATA_URL\"].replace(\"-testtrain-\", \"-traind-\")\n",
    "\n",
    "        print(f\"### LOADING TEST DATA {test_url}\")\n",
    "        df_test_feat = spark.read.json(test_url)\n",
    "\n",
    "        print(f\"### LOADING TRAIND DATA {traind_url}\")\n",
    "        df_traind_feat = spark.read.json(traind_url)\n",
    "\n",
    "        print(f\"### LOADING TRAINW DATA {trainw_url}\")\n",
    "        df_trainw_feat = spark.read.json(trainw_url)\n",
    "        \n",
    "        df_test = create_train_test_vector(CF, df_test_feat)\n",
    "        df_traind = create_train_test_vector(CF, df_traind_feat)\n",
    "        df_trainw = create_train_test_vector(CF, df_trainw_feat)\n",
    "\n",
    "        df_test = df_test.unpersist()\n",
    "        df_traind = df_traind.unpersist()\n",
    "        df_trainw = df_trainw.unpersist()\n",
    "\n",
    "        for MODEL in train_config[\"MODEL\"]:\n",
    "            model, f1, model_name = hyper_tune(df_trainw, df_traind, df_test, configstr, MODEL)\n",
    "        \n",
    "        df_test.unpersist()\n",
    "        df_traind.unpersist()\n",
    "        df_trainw.unpersist()\n",
    "        \n",
    "print(\"FINISHED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "884f1b52-f375-4cbe-a695-2583bca29272",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_configs_1 = [\n",
    "    {\n",
    "        \"CHURN\":                   [\"canceldown\"],\n",
    "        \"FUTURE_LOOKAHEAD_WEEKS\":  [1],\n",
    "        \"PAST_NEAR_HISTORY_WEEKS\": [1],\n",
    "        \"PAST_OLD_HISTORY_WEEKS\":  [1],\n",
    "        \"FEATURE_COLS\":            [[\"r_\", \"nh_\"]],\n",
    "        \"DOWNSAMPLE\":              [1],\n",
    "        \"MODEL\":                   [{\"rf\": [[10], [4]]}]\n",
    "    },\n",
    "    {\n",
    "        \"CHURN\":                   [\"canceldown\"],\n",
    "        \"FUTURE_LOOKAHEAD_WEEKS\":  [2, 1],\n",
    "        \"PAST_NEAR_HISTORY_WEEKS\": [2, 1],\n",
    "        \"PAST_OLD_HISTORY_WEEKS\":  [99],\n",
    "        \"FEATURE_COLS\":            [[\"r_\", \"nh_\"]],\n",
    "        \"DOWNSAMPLE\":              [1],\n",
    "        \"MODEL\":                   [{\"rf\": [[10], [4]]}]\n",
    "    }\n",
    "]\n",
    "\n",
    "train_configs_2 = [\n",
    "    {\n",
    "        \"CHURN\":                   [\"canceldown\"],\n",
    "        \"FUTURE_LOOKAHEAD_WEEKS\":  [2],\n",
    "        \"PAST_NEAR_HISTORY_WEEKS\": [1],\n",
    "        \"PAST_OLD_HISTORY_WEEKS\":  [99],\n",
    "        \"FEATURE_COLS\":            [[\"r_\", \"nh_\"]],\n",
    "        \"DOWNSAMPLE\":              [0.25, 0.5, 0.75],\n",
    "        \"MODEL\":                   [{\"rf\": [[10], [4]]}]\n",
    "    }\n",
    "]\n",
    "\n",
    "train_configs_3 = [\n",
    "    {\n",
    "        \"CHURN\":                   [\"canceldown\"],\n",
    "        \"FUTURE_LOOKAHEAD_WEEKS\":  [2],\n",
    "        \"PAST_NEAR_HISTORY_WEEKS\": [1],\n",
    "        \"PAST_OLD_HISTORY_WEEKS\":  [1,2,3,4,5],\n",
    "        \"FEATURE_COLS\":            [[\"r_\", \"nh_\"]],\n",
    "        \"DOWNSAMPLE\":              [1],\n",
    "        \"MODEL\":                   [{\"rf\": [[10], [4]]}]\n",
    "    }\n",
    "]\n",
    "\n",
    "train_configs_4 = [\n",
    "    {\n",
    "        \"CHURN\":                   [\"canceldown\", \"cancel\", \"down\"],\n",
    "        \"FUTURE_LOOKAHEAD_WEEKS\":  [2],\n",
    "        \"PAST_NEAR_HISTORY_WEEKS\": [1],\n",
    "        \"PAST_OLD_HISTORY_WEEKS\":  [1],\n",
    "        \"FEATURE_COLS\":            [[\"r_\", \"nh_\", \"oh_\", \"nhn_\", \"ohn_\", \"d_\"]],\n",
    "        \"DOWNSAMPLE\":              [1],\n",
    "        \"MODEL\":                   [{\"rf\": [[10], [4]]}]\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "train_configs_5 = [\n",
    "    {\n",
    "        \"CHURN\":                   [\"canceldown\"],\n",
    "        \"FUTURE_LOOKAHEAD_WEEKS\":  [2],\n",
    "        \"PAST_NEAR_HISTORY_WEEKS\": [1],\n",
    "        \"PAST_OLD_HISTORY_WEEKS\":  [1],\n",
    "        \"FEATURE_COLS\":            [[\"r_\", \"nh_\"],\n",
    "                                    [\"r_\", \"oh_\"], \n",
    "                                    [\"r_\", \"nhn_\"], \n",
    "                                    [\"r_\", \"ohn_\"], \n",
    "                                    [\"r_\", \"d_\"], \n",
    "                                    [\"nh_\", \"oh_\"], \n",
    "                                    [\"nh_\", \"nhn_\"], \n",
    "                                    [\"nh_\", \"ohn_\"], \n",
    "                                    [\"nh_\", \"d_\"], \n",
    "                                    [\"oh_\", \"nhn_\"], \n",
    "                                    [\"oh_\", \"ohn_\"], \n",
    "                                    [\"oh_\", \"d_\"], \n",
    "                                    [\"nhn_\", \"ohn_\"], \n",
    "                                    [\"nhn_\", \"d_\"], \n",
    "                                    [\"ohn_\", \"d_\"]\n",
    "                                   ],\n",
    "        \"DOWNSAMPLE\":              [1],\n",
    "        \"MODEL\":                   [{\"rf\": [[10], [4]]}]\n",
    "    }\n",
    "]\n",
    "\n",
    "train_configs_6 = [\n",
    "    {\n",
    "        \"CHURN\":                   [\"canceldown\"],\n",
    "        \"FUTURE_LOOKAHEAD_WEEKS\":  [2],\n",
    "        \"PAST_NEAR_HISTORY_WEEKS\": [1],\n",
    "        \"PAST_OLD_HISTORY_WEEKS\":  [1],\n",
    "        \"FEATURE_COLS\":            [[\"r_\"], [\"nh_\"], [\"oh_\"], [\"nhn_\"], [\"ohn_\"], [\"d_\"]],\n",
    "        \"DOWNSAMPLE\":              [1],\n",
    "        \"MODEL\":                   [{\"rf\": [[10], [4]]}]\n",
    "    }\n",
    "]\n",
    "\n",
    "train_configs_7 = [\n",
    "    {\n",
    "        \"CHURN\":                   [\"canceldown\"],\n",
    "        \"FUTURE_LOOKAHEAD_WEEKS\":  [1],\n",
    "        \"PAST_NEAR_HISTORY_WEEKS\": [1],\n",
    "        \"PAST_OLD_HISTORY_WEEKS\":  [4],\n",
    "        \"FEATURE_COLS\":            [[\"nhn_\", \"ohn_\"]],\n",
    "        \"MODEL\":                   [{\"rf\": [[10], [4]]},{\"rf\": [[100,20], [5]]}]\n",
    "    }\n",
    "]\n",
    "\n",
    "train_configs_8 = [\n",
    "    {\n",
    "        \"CHURN\":                   [\"canceldown\"],\n",
    "        \"FUTURE_LOOKAHEAD_WEEKS\":  [2],\n",
    "        \"PAST_NEAR_HISTORY_WEEKS\": [1],\n",
    "        \"PAST_OLD_HISTORY_WEEKS\":  [4],\n",
    "        \"FEATURE_COLS\":            [[\"nhn_\", \"ohn_\"]],\n",
    "        \"MODEL\":                   [{\"dt\": [[5], [32]]},{\"rf\": [[10], [4]]}]\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1087fe0-1964-4c85-a5de-83be14d5fc6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "87d4cb43-ef96-4630-9844-f463b7b03c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### STOP SPARK SESSION\n"
     ]
    }
   ],
   "source": [
    "print(\"### STOP SPARK SESSION\")\n",
    "spark.stop()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899ac3c4-e4ee-42f3-88c8-a46ae17737df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
