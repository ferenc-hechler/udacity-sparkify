{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15fbf17d-596c-419b-ac66-e8126275f46a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# \\[07\\] Auto-Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df5acf66-8ff9-4d6f-9d94-f36e3a90c38d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### SETUP SPARK SESSION \"Sparkify\"\n",
      "Spark version: 3.3.2\n"
     ]
    }
   ],
   "source": [
    "EVENT_DATA_URL = \"s3a://udacity-dsnd/sparkify/sparkify_event_data.json\"\n",
    "# EVENT_DATA_URL = \"s3a://udacity-dsnd/sparkify/mini_sparkify_event_data.json\"\n",
    "\n",
    "CLEAN_DATA_URL = EVENT_DATA_URL.replace(\"/sparkify/\", \"/sparkify/output/02-cleaned-\")\n",
    "WEEK_AGGREGATED_DATA_URL = EVENT_DATA_URL.replace(\"/sparkify/\", \"/sparkify/output/04-week-aggregated-\")\n",
    "\n",
    "\n",
    "EXECUTOR_INSTANCES = 2\n",
    "EXECUTOR_MEM = '6g'\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from cryptography.fernet import Fernet\n",
    "import base64\n",
    "import socket\n",
    "\n",
    "!./install-s3-jars.sh\n",
    "\n",
    "def decrypt(encrypted_text):\n",
    "    \"\"\"\n",
    "    decrypts an encrypted text. The seed (master-password) for decryption is read from the file \".seed.txt\"\n",
    "    \n",
    "    Input: encrypted_text\n",
    "    \n",
    "    Output: the decrypted text. If the text was not encrypted with the same seed, \n",
    "            an exception is raised.\n",
    "    \"\"\"\n",
    "    with open('.seed.txt') as f:\n",
    "        seed = f.read().strip()\n",
    "    return Fernet(base64.b64encode((seed*32)[:32].encode('ascii')).decode('ascii')).decrypt(encrypted_text.encode('ascii')).decode('ascii')\n",
    "\n",
    "AWS_ACCESS_KEY_ID='V6ge1JcQpvyYGJjb'\n",
    "AWS_SECRET_ACCESS_KEY = decrypt('gAAAAABkDFI6865LaVJVgtTYo0aMx9-JTPbTo6cwOUjg5eNNPsZhBDoHbRZ8xuXQT0ImNfvqcecZuoJd1VzYQEpBaxyCnKvosii8O1KeqoL2NwKdKtL_AUfT4eW4dvJVP--VjEvc0gB4')\n",
    "OWN_IP=socket.gethostbyname(socket.gethostname())\n",
    "APP_NAME = \"Sparkify\"\n",
    "SPARK_MASTER = \"spark://bit-spark-master-svc.spark.svc.cluster.local:7077\"\n",
    "S3_HOST = \"minio-api-service.minio.svc\"\n",
    "\n",
    "print(f'### SETUP SPARK SESSION \"{APP_NAME}\"')\n",
    "spark = SparkSession.builder \\\n",
    "    .master(SPARK_MASTER) \\\n",
    "    .config(\"spark.jars\",\"/home/jovyan/jars/aws-java-sdk-bundle-1.11.1026.jar,/home/jovyan/jars/hadoop-aws-3.3.2.jar\") \\\n",
    "    .config(\"spark.driver.host\", OWN_IP) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", S3_HOST) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", AWS_ACCESS_KEY_ID) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", AWS_SECRET_ACCESS_KEY) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.executor.instances\", EXECUTOR_INSTANCES) \\\n",
    "    .config(\"spark.executor.memory\", EXECUTOR_MEM) \\\n",
    "    .appName(APP_NAME).getOrCreate()\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"WARN\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1281929a-6fdb-40bb-927e-db177696289f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### LOAD DATA s3a://udacity-dsnd/sparkify/output/04-week-aggregated-sparkify_event_data.json\n",
      "### PERSIST\n"
     ]
    }
   ],
   "source": [
    "print(f\"### LOAD DATA {WEEK_AGGREGATED_DATA_URL}\")\n",
    "df_userweek = spark.read.json(WEEK_AGGREGATED_DATA_URL)\n",
    "print(f\"### PERSIST\")\n",
    "df_userweek_persist = df_userweek.persist()\n",
    "df_userweek = df_userweek_persist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "75e62365-bffe-4b21-827b-087d467adc6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier, RandomForestClassificationModel, DecisionTreeClassifier, DecisionTreeClassificationModel, LinearSVC\n",
    "from pyspark.ml.feature import RegexTokenizer, VectorAssembler, Normalizer, StandardScaler, MinMaxScaler, MaxAbsScaler\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.types import IntegerType\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import datetime\n",
    "\n",
    "# timestamp constants for ts in milliseconds\n",
    "one_hour =        60*60*1000  #     3.600.000\n",
    "one_day =      24*60*60*1000  #    86.400.000\n",
    "one_week =   7*24*60*60*1000  #   604.800.000\n",
    "one_month = 28*24*60*60*1000  # 2.419.200.000\n",
    "\n",
    "def logresult(text):\n",
    "    print(text)\n",
    "    with open(\"result.log\", \"a\") as logf:\n",
    "        logf.write(text+\"\\n\")\n",
    "\n",
    "def oversample(df_train):\n",
    "    df_lab0 = df_train.where(F.col(\"label\") == 0)\n",
    "    df_lab1 = df_train.where(F.col(\"label\") == 1)\n",
    "    train0cnt = df_lab0.count()\n",
    "    train1cnt = df_lab1.count()\n",
    "    oversampled_train = df_train\n",
    "    sum1cnt = train1cnt\n",
    "    while sum1cnt <= train0cnt:\n",
    "        sum1cnt = sum1cnt+train1cnt\n",
    "        print(f\"oversampling to: {sum1cnt}/{train0cnt}\")\n",
    "        oversampled_train = oversampled_train.union(df_lab1)\n",
    "    return oversampled_train\n",
    "\n",
    "def downsample(df_train, factor):\n",
    "    df_lab0 = df_train.where(F.col(\"label\") == 0)\n",
    "    df_lab1 = df_train.where(F.col(\"label\") == 1)\n",
    "    train0cnt = df_lab0.count()\n",
    "    print(f\"orig-label-0: {train0cnt}\")\n",
    "    train1cnt = df_lab1.count()\n",
    "    print(f\"orig-label-1: {train1cnt}\")\n",
    "    frac = train1cnt/(factor*train0cnt+1)\n",
    "    df_downsampled = df_lab0.sample(fraction = frac, seed=42)\n",
    "    df_downsampled = df_downsampled.union(df_lab1)\n",
    "    print(f\"downsampled label-1 = {train1cnt}, label-0 ~ {train0cnt*frac}\")\n",
    "    return df_downsampled\n",
    "\n",
    "def add_weight_col(df_train):\n",
    "    label_counts = df_train.agg(F.sum(F.col(\"label\")).alias(\"l1\"), F.sum(1-F.col(\"label\")).alias(\"l0\")).collect()[0]\n",
    "    \n",
    "    w1 = label_counts.l0 / (label_counts.l0+label_counts.l1)\n",
    "    w0 = label_counts.l1 / (label_counts.l0+label_counts.l1)\n",
    "    \n",
    "    print(f\"label 0: {label_counts.l0}, label 1: {label_counts.l1}\")\n",
    "    df_result = df_train.withColumn(\"weight\", F.when(F.col(\"label\")==1, F.lit(w1)).otherwise(F.lit(w0)))\n",
    "    return df_result\n",
    "    \n",
    "\n",
    "def prefix_columns(df_orig, prefix, do_not_change_cols):\n",
    "    newcols = [prefix+col if not col in do_not_change_cols else col for col in df_orig.columns]\n",
    "    return df_orig.toDF(*newcols)\n",
    "\n",
    "def aggregate_week_data(from_week, to_week):\n",
    "    \"\"\"\n",
    "    Input: from_week, to_week\n",
    "    Output: aggregated sum data for the weeks from_week..to_week (both including)\n",
    "    \"\"\"\n",
    "    dropcols = [\"paid\", \"usermale\", \"userregistration\", \"wid\"]\n",
    "    df_weeks = df_userweek.where((F.col(\"wid\")>=from_week)&(F.col(\"wid\")<=to_week))\n",
    "    if from_week == to_week:\n",
    "        # no aggregation necessary, if there is only one week\n",
    "        return df_weeks.drop(*dropcols)\n",
    "    aggs = [F.sum(F.col(col)).alias(col) for col in df_weeks.columns if not col in [\"userId\", *dropcols]]\n",
    "    df_weeks = df_weeks.groupBy(\"userId\").agg(*aggs)\n",
    "    return df_weeks    \n",
    "\n",
    "\n",
    "def create_test_data(CF, current_week):\n",
    "\n",
    "    label_week_min = current_week-CF[\"FUTURE_LOOKAHEAD_WEEKS\"]\n",
    "    label_week_max = current_week-1\n",
    "\n",
    "    newhistory_week_min = current_week\n",
    "    newhistory_week_max = newhistory_week_min+CF[\"PAST_NEAR_HISTORY_WEEKS\"]-1\n",
    "\n",
    "    oldhistory_week_min = newhistory_week_max+1\n",
    "    oldhistory_week_max = oldhistory_week_min+CF[\"PAST_OLD_HISTORY_WEEKS\"]-1\n",
    "    \n",
    "    df_user = df_userweek.where(F.col(\"wid\") == newhistory_week_min).select(\"userId\", \"wid\", \"paid\", \"usermale\", \"userregistration\")\n",
    "    df_user = df_user.withColumn(\"userregistration\", F.col(\"userregistration\")-7*newhistory_week_min)\n",
    "\n",
    "    df_label = aggregate_week_data(label_week_min, label_week_max)\n",
    "    df_newhistory = aggregate_week_data(newhistory_week_min, newhistory_week_max)\n",
    "    df_oldhistory = aggregate_week_data(oldhistory_week_min, oldhistory_week_max)\n",
    "\n",
    "    if CF[\"CHURN\"]==\"cancel\":\n",
    "        df_label = df_label.withColumn(\"label\", F.when(F.col(\"pg_cancellation_confirmation\")>0, F.lit(1)).otherwise(F.lit(0))).select(\"userid\", \"label\")\n",
    "    elif CF[\"CHURN\"]==\"down\":\n",
    "        df_label = df_label.withColumn(\"label\", F.when(F.col(\"pg_submit_downgrade\")>0, F.lit(1)).otherwise(F.lit(0))).select(\"userid\", \"label\")\n",
    "    elif CF[\"CHURN\"]==\"canceldown\":\n",
    "        df_label = df_label.withColumn(\"label\", F.when(F.col(\"pg_cancellation_confirmation\")+F.col(\"pg_submit_downgrade\")>0, F.lit(1)).otherwise(F.lit(0))).select(\"userid\", \"label\")\n",
    "    else: \n",
    "        raise Exception(f'invalid value for CHURN {CF[\"CHURN\"]}')\n",
    "    df_user = df_user.join(df_label, \"userId\")\n",
    "\n",
    "    df_user = df_user.join(prefix_columns(df_newhistory, \"nh_\", [\"userId\"]), \"userId\")\n",
    "    df_user = df_user.join(prefix_columns(df_oldhistory, \"oh_\", [\"userId\"]), \"userId\")\n",
    "\n",
    "    for c in df_oldhistory.columns:\n",
    "        if not c in [\"userId\", \"session_hours\", \"session_start\"]:\n",
    "            df_user = df_user.withColumn(\"ohn_\"+c, F.col(\"oh_\"+c)/F.greatest(F.col(\"oh_session_hours\"), F.lit(0.01)))\n",
    "    df_user = df_user.withColumn(\"ohn_session_hours\", F.col(\"oh_session_hours\"))\n",
    "    df_user = df_user.withColumn(\"ohn_session_start\", F.col(\"oh_session_start\"))\n",
    "    for c in df_newhistory.columns:\n",
    "        if not c in [\"userId\", \"session_hours\", \"session_start\"]:\n",
    "            df_user = df_user.withColumn(\"nhn_\"+c, F.col(\"nh_\"+c)/F.greatest(F.col(\"nh_session_hours\"), F.lit(0.01)))\n",
    "    df_user = df_user.withColumn(\"nhn_session_hours\", F.col(\"nh_session_hours\"))\n",
    "    df_user = df_user.withColumn(\"nhn_session_start\", F.col(\"nh_session_start\"))\n",
    "    for c in df_newhistory.columns:\n",
    "        if not c in [\"userId\"]:\n",
    "            df_user = df_user.withColumn(\"r_\"+c, F.col(\"nhn_\"+c)/F.greatest(F.lit(0.01), F.col(\"ohn_\"+c)))\n",
    "    for c in df_newhistory.columns:\n",
    "        if not c in [\"userId\"]:\n",
    "            df_user = df_user.withColumn(\"d_\"+c, F.col(\"nhn_\"+c)-F.col(\"ohn_\"+c))\n",
    "    \n",
    "    return df_user\n",
    "\n",
    "\n",
    "\n",
    "def confuse(df_test_pred):\n",
    "    n00 = df_test_pred.where((F.col(\"label\")==0)&(F.col(\"prediction\")==0)).count()\n",
    "    n01 = df_test_pred.where((F.col(\"label\")==0)&(F.col(\"prediction\")==1)).count()\n",
    "    n10 = df_test_pred.where((F.col(\"label\")==1)&(F.col(\"prediction\")==0)).count()\n",
    "    n11 = df_test_pred.where((F.col(\"label\")==1)&(F.col(\"prediction\")==1)).count()\n",
    "    s00 = \"{:5d}\".format(n00)\n",
    "    s01 = \"{:5d}\".format(n01)\n",
    "    s10 = \"{:5d}\".format(n10)\n",
    "    s11 = \"{:5d}\".format(n11)\n",
    "    logresult(f\"                  \")\n",
    "    logresult(f\" Confusion Matrix: \")\n",
    "    logresult(f\"                  \")\n",
    "    logresult(f\"     | prediction| \")\n",
    "    logresult(f\"     |   0 |  1  | \")\n",
    "    logresult(f\" ----+-----+-----+ \")\n",
    "    logresult(f\" l 0 |{s00}|{s01}| \")\n",
    "    logresult(f\" b --+-----+-----+ \")\n",
    "    logresult(f\" l 1 |{s10}|{s11}| \")\n",
    "    logresult(f\" ----+-----+-----+ \")\n",
    "    logresult(f\"                   \")\n",
    "    TP = n11\n",
    "    TN = n00\n",
    "    FP = n01\n",
    "    FN = n10\n",
    "    accuracy = 0\n",
    "    if TP+TN+FP+FN!=0:\n",
    "        accuracy = (TP+TN)/(TP+TN+FP+FN)\n",
    "    precision = 0\n",
    "    if TP+FP!=0:\n",
    "        precision = TP/(TP+FP)\n",
    "    recall = 0\n",
    "    if TP+FN!=0:\n",
    "        recall = TP/(TP+FN)\n",
    "    f1 = 0\n",
    "    if precision+recall!=0:\n",
    "        f1 = 2*precision*recall/(precision+recall)\n",
    "    logresult(f\"  accuraccy: {accuracy}\")\n",
    "    logresult(f\"  precision: {precision}\")\n",
    "    logresult(f\"  recall:    {recall}\")\n",
    "    logresult(f\"  f1:        {f1}\")\n",
    "    # https://towardsdatascience.com/matthews-correlation-coefficient-when-to-use-it-and-when-to-avoid-it-310b3c923f7e\n",
    "    mcc = -9\n",
    "    nenn = (TN+FN)*(FP+TP)*(TN+FP)*(FN+TP)\n",
    "    if nenn!=0:   \n",
    "        mcc = (TN*TP-FP*FN)/math.sqrt(nenn)\n",
    "    logresult(f\"  mcc:       {mcc}\")\n",
    "    return (accuracy, precision, recall, f1)\n",
    "    \n",
    "    \n",
    "def hyper_tune_rf(config, num_tree_values, max_depth_values):   # 20,5\n",
    "    best_f1 = -1\n",
    "    best_model = None\n",
    "    best_model_name = \"?\"\n",
    "    for num_trees in num_tree_values:\n",
    "        for max_depth in max_depth_values:\n",
    "            model_name = f\"rf_{num_trees}_{max_depth}\"\n",
    "            rf = RandomForestClassifier(featuresCol=\"features\", numTrees=num_trees, maxDepth=max_depth, weightCol=\"weight\", seed=42)\n",
    "            rf_model = rf.fit(df_train)\n",
    "            predict_test  = rf_model.transform(df_test)\n",
    "            logresult(f\"\")\n",
    "            logresult(f\"------------------------------------\")\n",
    "            logresult(f\"TRAINING {configstr} {model_name}\")\n",
    "            logresult(f\"------------------------------------\")\n",
    "            accuracy, precision, recall, f1 = confuse(predict_test)\n",
    "            print(f\"  {model_name}: f1 {f1}\")\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                best_model = rf_model\n",
    "                best_model_name = model_name\n",
    "    print(f\"best f1 {f1} for {best_model_name}\")\n",
    "    return (best_model, best_f1, best_model_name)\n",
    "\n",
    "\n",
    "def hyper_tune_lr(max_iters, reg_params, elastic_net_params): # 100, 0, 0\n",
    "    # https://towardsdatascience.com/beginners-guide-to-linear-regression-with-pyspark-bfc39b45a9e9\n",
    "    evaluator = RegressionEvaluator(predictionCol=\"prediction_orig\", labelCol=\"label\", metricName=\"rmse\") \n",
    "    \n",
    "    best_err = 9999\n",
    "    best_model = None\n",
    "    best_model_name = \"?\"\n",
    "    for  max_iter in  max_iters:\n",
    "        for reg_param in reg_params:\n",
    "            for elastic_net_param in elastic_net_params:\n",
    "                model_name = f\"lr_{max_iter}_{reg_param}_{elastic_net_param}\"\n",
    "                lr = LinearRegression(featuresCol=\"features\", maxIter= max_iter, regParam=reg_param, elasticNetParam=elastic_net_param)\n",
    "                model = lr.fit(df_train)\n",
    "                predict_test  = model.transform(df_test)\n",
    "                predict_test = predict_test.withColumnRenamed(\"prediction\", \"prediction_orig\")\n",
    "                err = evaluator.evaluate(predict_test)\n",
    "                print(f\"err: {err}\")\n",
    "                thr = 0.15\n",
    "                predict_test = predict_test.withColumn(\"prediction\", F.when(F.col(\"prediction_orig\")>=thr,1).otherwise(0))\n",
    "                accuracy, precision, recall, f1 = confuse(predict_test)\n",
    "                print(f\"  {model_name}: f1 {f1}\")\n",
    "                if err < best_err:\n",
    "                    best_err = err\n",
    "                    best_model = model\n",
    "                    best_model_name = model_name\n",
    "    print(f\"best f1 {f1} for {best_model_name}\")\n",
    "    return (best_model, best_err, best_model_name)\n",
    "\n",
    "\n",
    "def hyper_tune_dt(max_depths, max_bins_list):\n",
    "    best_f1 = -1\n",
    "    best_model = None\n",
    "    best_model_name = \"?\"\n",
    "    for  max_depth in max_depths:\n",
    "        for max_bins in max_bins_list:\n",
    "            model_name = f\"dt_{max_depth}_{max_bins}\"\n",
    "            dt = DecisionTreeClassifier(featuresCol=\"features\", maxDepth=max_depth, maxBins=max_bins)\n",
    "            model = dt.fit(df_train)\n",
    "            predict_test  = model.transform(df_test)\n",
    "            accuracy, precision, recall, f1 = confuse(predict_test)\n",
    "            print(f\"  {model_name}: f1 {f1}\")\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                best_model = model\n",
    "                best_model_name = model_name\n",
    "    print(f\"best f1 {f1} for {best_model_name}\")\n",
    "    return (best_model, best_f1, best_model_name)\n",
    "\n",
    "\n",
    "def hyper_tune_sv(max_iters, reg_params):\n",
    "    best_f1 = -1\n",
    "    best_model = None\n",
    "    best_model_name = \"?\"\n",
    "    for  max_iter in max_iters:\n",
    "        for reg_param in reg_params:\n",
    "            model_name = f\"svm_{max_iter}_{reg_param}\"\n",
    "            lsvc = LinearSVC(featuresCol=\"features\", maxIter=max_iter, regParam=reg_param)\n",
    "            model = lsvc.fit(df_train)\n",
    "            predict_test  = model.transform(df_test)\n",
    "            accuracy, precision, recall, f1 = confuse(predict_test)\n",
    "            print(f\"  {model_name}: f1 {f1}\")\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                best_model = model\n",
    "                best_model_name = model_name\n",
    "    print(f\"best f1 {f1} for {best_model_name}\")\n",
    "    return (best_model, best_f1, best_model_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "94d70f4e-a592-441a-ad87-32c3f8ba0fa5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_train_test_data(CF):\n",
    "    df_testtrain = create_test_data(CF, 1)\n",
    "    df_testtrain = df_testtrain.union(create_test_data(CF, 2))\n",
    "    df_testtrain = df_testtrain.union(create_test_data(CF, 3))\n",
    "    df_testtrain = df_testtrain.union(create_test_data(CF, 4))\n",
    "    featureCols = [\"paid\", \"usermale\", \"userregistration\"]\n",
    "    for prefix in CF[\"FEATURE_COLS\"]:\n",
    "        featureCols = [*featureCols, *[col for col in df_testtrain.columns if col.startswith(prefix)]]\n",
    "    assembler = VectorAssembler(inputCols=featureCols, outputCol=\"features\")\n",
    "    df_testtrain_vec=assembler.transform(df_testtrain).select(\"userId\", \"wid\", \"label\",\"features\")\n",
    "    return df_testtrain_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "884f1b52-f375-4cbe-a695-2583bca29272",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_configs_1 = [\n",
    "    {\n",
    "        \"CHURN\":                   [\"canceldown\"],\n",
    "        \"FUTURE_LOOKAHEAD_WEEKS\":  [1],\n",
    "        \"PAST_NEAR_HISTORY_WEEKS\": [1],\n",
    "        \"PAST_OLD_HISTORY_WEEKS\":  [1],\n",
    "        \"FEATURE_COLS\":            [[\"r_\", \"nh_\"]],\n",
    "        \"DOWNSAMPLE\":              [1],\n",
    "        \"MODEL\":                   [{\"rf\": [[10], [4]]}]\n",
    "    },\n",
    "    {\n",
    "        \"CHURN\":                   [\"canceldown\"],\n",
    "        \"FUTURE_LOOKAHEAD_WEEKS\":  [2, 1],\n",
    "        \"PAST_NEAR_HISTORY_WEEKS\": [2, 1],\n",
    "        \"PAST_OLD_HISTORY_WEEKS\":  [99],\n",
    "        \"FEATURE_COLS\":            [[\"r_\", \"nh_\"]],\n",
    "        \"DOWNSAMPLE\":              [1],\n",
    "        \"MODEL\":                   [{\"rf\": [[10], [4]]}]\n",
    "    }\n",
    "]\n",
    "\n",
    "train_configs_2 = [\n",
    "    {\n",
    "        \"CHURN\":                   [\"canceldown\"],\n",
    "        \"FUTURE_LOOKAHEAD_WEEKS\":  [2],\n",
    "        \"PAST_NEAR_HISTORY_WEEKS\": [1],\n",
    "        \"PAST_OLD_HISTORY_WEEKS\":  [99],\n",
    "        \"FEATURE_COLS\":            [[\"r_\", \"nh_\"]],\n",
    "        \"DOWNSAMPLE\":              [0.25, 0.5, 0.75],\n",
    "        \"MODEL\":                   [{\"rf\": [[10], [4]]}]\n",
    "    }\n",
    "]\n",
    "\n",
    "train_configs_3 = [\n",
    "    {\n",
    "        \"CHURN\":                   [\"canceldown\"],\n",
    "        \"FUTURE_LOOKAHEAD_WEEKS\":  [2],\n",
    "        \"PAST_NEAR_HISTORY_WEEKS\": [1],\n",
    "        \"PAST_OLD_HISTORY_WEEKS\":  [1,2,3,4,5],\n",
    "        \"FEATURE_COLS\":            [[\"r_\", \"nh_\"]],\n",
    "        \"DOWNSAMPLE\":              [1],\n",
    "        \"MODEL\":                   [{\"rf\": [[10], [4]]}]\n",
    "    }\n",
    "]\n",
    "\n",
    "train_configs_4 = [\n",
    "    {\n",
    "        \"CHURN\":                   [\"canceldown\", \"cancel\", \"down\"],\n",
    "        \"FUTURE_LOOKAHEAD_WEEKS\":  [2],\n",
    "        \"PAST_NEAR_HISTORY_WEEKS\": [1],\n",
    "        \"PAST_OLD_HISTORY_WEEKS\":  [1],\n",
    "        \"FEATURE_COLS\":            [[\"r_\", \"nh_\", \"oh_\", \"nhn_\", \"ohn_\", \"d_\"]],\n",
    "        \"DOWNSAMPLE\":              [1],\n",
    "        \"MODEL\":                   [{\"rf\": [[10], [4]]}]\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "train_configs_5 = [\n",
    "    {\n",
    "        \"CHURN\":                   [\"canceldown\"],\n",
    "        \"FUTURE_LOOKAHEAD_WEEKS\":  [2],\n",
    "        \"PAST_NEAR_HISTORY_WEEKS\": [1],\n",
    "        \"PAST_OLD_HISTORY_WEEKS\":  [1],\n",
    "        \"FEATURE_COLS\":            [[\"r_\", \"nh_\"],\n",
    "                                    [\"r_\", \"oh_\"], \n",
    "                                    [\"r_\", \"nhn_\"], \n",
    "                                    [\"r_\", \"ohn_\"], \n",
    "                                    [\"r_\", \"d_\"], \n",
    "                                    [\"nh_\", \"oh_\"], \n",
    "                                    [\"nh_\", \"nhn_\"], \n",
    "                                    [\"nh_\", \"ohn_\"], \n",
    "                                    [\"nh_\", \"d_\"], \n",
    "                                    [\"oh_\", \"nhn_\"], \n",
    "                                    [\"oh_\", \"ohn_\"], \n",
    "                                    [\"oh_\", \"d_\"], \n",
    "                                    [\"nhn_\", \"ohn_\"], \n",
    "                                    [\"nhn_\", \"d_\"], \n",
    "                                    [\"ohn_\", \"d_\"]\n",
    "                                   ],\n",
    "        \"DOWNSAMPLE\":              [1],\n",
    "        \"MODEL\":                   [{\"rf\": [[10], [4]]}]\n",
    "    }\n",
    "]\n",
    "\n",
    "train_configs_6 = [\n",
    "    {\n",
    "        \"CHURN\":                   [\"canceldown\"],\n",
    "        \"FUTURE_LOOKAHEAD_WEEKS\":  [2],\n",
    "        \"PAST_NEAR_HISTORY_WEEKS\": [1],\n",
    "        \"PAST_OLD_HISTORY_WEEKS\":  [1],\n",
    "        \"FEATURE_COLS\":            [[\"r_\"], [\"nh_\"], [\"oh_\"], [\"nhn_\"], [\"ohn_\"], [\"d_\"]],\n",
    "        \"DOWNSAMPLE\":              [1],\n",
    "        \"MODEL\":                   [{\"rf\": [[10], [4]]}]\n",
    "    }\n",
    "]\n",
    "\n",
    "train_configs = [\n",
    "    {\n",
    "        \"CHURN\":                   [\"canceldown\"],\n",
    "        \"FUTURE_LOOKAHEAD_WEEKS\":  [2],\n",
    "        \"PAST_NEAR_HISTORY_WEEKS\": [1],\n",
    "        \"PAST_OLD_HISTORY_WEEKS\":  [4],\n",
    "        \"FEATURE_COLS\":            [[\"nhn_\", \"ohn_\"]],\n",
    "        \"DOWNSAMPLE\":              [1],\n",
    "        \"MODEL\":                   [{\"rf\": [[10], [4]]}]\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "704d88a9-f996-467e-b77e-98b933c42048",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CF = {\n",
    "        \"CHURN\":                   \"canceldown\",\n",
    "        \"FUTURE_LOOKAHEAD_WEEKS\":  2,\n",
    "        \"PAST_NEAR_HISTORY_WEEKS\": 1,\n",
    "        \"PAST_OLD_HISTORY_WEEKS\":  4,\n",
    "        \"FEATURE_COLS\":            [\"nhn_\", \"ohn_\"],\n",
    "        \"DOWNSAMPLE\":              1,\n",
    "        \"MODEL\":                   {\"rf\": [[10], [4]]}\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f58e8b4-df25-420d-90b4-38d1d7fc9a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_testtrain_vec = create_train_test_data(CF)\n",
    "df_train, df_test = df_testtrain_vec.randomSplit([0.7, 0.3], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "abc9daf0-398c-42ea-8346-93b1e0cb3880",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "canceldown-2-1-4-nhn_ohn_-1-wc\n"
     ]
    }
   ],
   "source": [
    "prefixes = str(CF[\"FEATURE_COLS\"]).replace(\"[\",\"\").replace(\"]\",\"\").replace(\"'\",\"\").replace(\",\",\"\").replace(\" \",\"\")\n",
    "configstr = f'{CF[\"CHURN\"]}-{CF[\"FUTURE_LOOKAHEAD_WEEKS\"]}-{CF[\"PAST_NEAR_HISTORY_WEEKS\"]}-{CF[\"PAST_OLD_HISTORY_WEEKS\"]}-{prefixes}-{CF[\"DOWNSAMPLE\"]}-wc'\n",
    "print(f\"{configstr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bda491a1-d6de-46c9-a6a8-678afa9c6e4b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label 0: 25132, label 1: 4431\n"
     ]
    }
   ],
   "source": [
    "df_train = add_weight_col(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4a91dbdd-50e4-47e4-8747-f9bd35290930",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train = df_train.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a8e16bcb-3ced-461e-80cf-9f39524beadd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CF[\"MODEL\"] = {\"rf\": [[100], [5]]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cf2ac6fd-d93d-4ddd-976b-f7fdd571e52a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------\n",
      "TRAINING canceldown-2-1-4-nhn_ohn_-1-wc rf_100_5\n",
      "------------------------------------\n",
      "                  \n",
      " Confusion Matrix: \n",
      "                  \n",
      "     | prediction| \n",
      "     |   0 |  1  | \n",
      " ----+-----+-----+ \n",
      " l 0 | 7179| 3755| \n",
      " b --+-----+-----+ \n",
      " l 1 |  673| 1197| \n",
      " ----+-----+-----+ \n",
      "                   \n",
      "  accuraccy: 0.6541705716963448\n",
      "  precision: 0.2417205169628433\n",
      "  recall:    0.6401069518716578\n",
      "  f1:        0.35092348284960423\n",
      "  mcc:       0.21514051325250577\n",
      "  rf_100_5: f1 0.35092348284960423\n",
      "best f1 0.35092348284960423 for rf_100_5\n"
     ]
    }
   ],
   "source": [
    "model, f1, model_name = hyper_tune_rf(configstr, *CF[\"MODEL\"][\"rf\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f43d3574-6856-4370-b6a5-c5987cdf2bfb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+-----+--------------------+------------------+\n",
      "| userId|wid|label|            features|            weight|\n",
      "+-------+---+-----+--------------------+------------------+\n",
      "|1030587|  1|    0|(57,[0,2,3,5,6,9,...|0.8501166999289652|\n",
      "|1033297|  1|    0|(57,[1,2,3,5,6,11...|0.8501166999289652|\n",
      "|1069552|  1|    0|(57,[2,3,15,17,24...|0.8501166999289652|\n",
      "|1083324|  1|    0|(57,[1,2,3,5,6,12...|0.8501166999289652|\n",
      "|1102913|  1|    0|(57,[0,1,2,3,5,6,...|0.8501166999289652|\n",
      "+-------+---+-----+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01eb2bf7-397e-4f8e-a2d2-5024cbe21d31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152a5c3c-df07-4618-a6af-755da70fb8c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3c4010-4a33-44c3-b37b-04320034674a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b99a4ccb-9e2a-429e-97d7-e419a2581f8e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "canceldown-2-1-1-r_-1\n",
      "### TRAIN / TEST SPLIT\n",
      "orig-label-0: 20127\n",
      "orig-label-1: 3993\n",
      "downsampled label-1 = 3993, label-0 ~ 3992.80161963434\n",
      "\n",
      "------------------------------------\n",
      "TRAINING canceldown-2-1-1-r_-1 rf_10_4\n",
      "------------------------------------\n",
      "                  \n",
      " Confusion Matrix: \n",
      "                  \n",
      "     | prediction| \n",
      "     |   0 |  1  | \n",
      " ----+-----+-----+ \n",
      " l 0 | 5210| 3656| \n",
      " b --+-----+-----+ \n",
      " l 1 |  607| 1075| \n",
      " ----+-----+-----+ \n",
      "                   \n",
      "  accuraccy: 0.5958475540386803\n",
      "  precision: 0.22722468822659056\n",
      "  recall:    0.6391200951248514\n",
      "  f1:        0.33525651021362857\n",
      "  mcc:       0.16692215220863454\n",
      "  rf_10_4: f1 0.33525651021362857\n",
      "best f1 0.33525651021362857 for rf_10_4\n",
      "canceldown-2-1-1-nh_-1\n",
      "### TRAIN / TEST SPLIT\n",
      "orig-label-0: 20127\n",
      "orig-label-1: 3993\n",
      "downsampled label-1 = 3993, label-0 ~ 3992.80161963434\n",
      "\n",
      "------------------------------------\n",
      "TRAINING canceldown-2-1-1-nh_-1 rf_10_4\n",
      "------------------------------------\n",
      "                  \n",
      " Confusion Matrix: \n",
      "                  \n",
      "     | prediction| \n",
      "     |   0 |  1  | \n",
      " ----+-----+-----+ \n",
      " l 0 | 5725| 3141| \n",
      " b --+-----+-----+ \n",
      " l 1 |  710|  972| \n",
      " ----+-----+-----+ \n",
      "                   \n",
      "  accuraccy: 0.6349070913917331\n",
      "  precision: 0.2363238512035011\n",
      "  recall:    0.5778834720570749\n",
      "  f1:        0.3354616048317515\n",
      "  mcc:       0.16784637024109988\n",
      "  rf_10_4: f1 0.3354616048317515\n",
      "best f1 0.3354616048317515 for rf_10_4\n",
      "canceldown-2-1-1-oh_-1\n",
      "### TRAIN / TEST SPLIT\n",
      "orig-label-0: 20327\n",
      "orig-label-1: 3993\n",
      "downsampled label-1 = 3993, label-0 ~ 3992.8035714285716\n",
      "\n",
      "------------------------------------\n",
      "TRAINING canceldown-2-1-1-oh_-1 rf_10_4\n",
      "------------------------------------\n",
      "                  \n",
      " Confusion Matrix: \n",
      "                  \n",
      "     | prediction| \n",
      "     |   0 |  1  | \n",
      " ----+-----+-----+ \n",
      " l 0 | 5757| 3109| \n",
      " b --+-----+-----+ \n",
      " l 1 |  761|  921| \n",
      " ----+-----+-----+ \n",
      "                   \n",
      "  accuraccy: 0.6331058020477816\n",
      "  precision: 0.22853598014888338\n",
      "  recall:    0.5475624256837098\n",
      "  f1:        0.3224789915966387\n",
      "  mcc:       0.14835636152849438\n",
      "  rf_10_4: f1 0.3224789915966387\n",
      "best f1 0.3224789915966387 for rf_10_4\n",
      "canceldown-2-1-1-nhn_-1\n",
      "### TRAIN / TEST SPLIT\n",
      "orig-label-0: 20127\n",
      "orig-label-1: 3993\n",
      "downsampled label-1 = 3993, label-0 ~ 3992.80161963434\n",
      "\n",
      "------------------------------------\n",
      "TRAINING canceldown-2-1-1-nhn_-1 rf_10_4\n",
      "------------------------------------\n",
      "                  \n",
      " Confusion Matrix: \n",
      "                  \n",
      "     | prediction| \n",
      "     |   0 |  1  | \n",
      " ----+-----+-----+ \n",
      " l 0 | 5363| 3503| \n",
      " b --+-----+-----+ \n",
      " l 1 |  630| 1052| \n",
      " ----+-----+-----+ \n",
      "                   \n",
      "  accuraccy: 0.6081721653394009\n",
      "  precision: 0.2309549945115258\n",
      "  recall:    0.6254458977407847\n",
      "  f1:        0.33734167067500404\n",
      "  mcc:       0.17024790745245777\n",
      "  rf_10_4: f1 0.33734167067500404\n",
      "best f1 0.33734167067500404 for rf_10_4\n",
      "canceldown-2-1-1-ohn_-1\n",
      "### TRAIN / TEST SPLIT\n",
      "orig-label-0: 20127\n",
      "orig-label-1: 3993\n",
      "downsampled label-1 = 3993, label-0 ~ 3992.80161963434\n",
      "\n",
      "------------------------------------\n",
      "TRAINING canceldown-2-1-1-ohn_-1 rf_10_4\n",
      "------------------------------------\n",
      "                  \n",
      " Confusion Matrix: \n",
      "                  \n",
      "     | prediction| \n",
      "     |   0 |  1  | \n",
      " ----+-----+-----+ \n",
      " l 0 | 5587| 3279| \n",
      " b --+-----+-----+ \n",
      " l 1 |  712|  970| \n",
      " ----+-----+-----+ \n",
      "                   \n",
      "  accuraccy: 0.6216344330678801\n",
      "  precision: 0.22828900917863026\n",
      "  recall:    0.5766944114149821\n",
      "  f1:        0.327094924970494\n",
      "  mcc:       0.1544055183482094\n",
      "  rf_10_4: f1 0.327094924970494\n",
      "best f1 0.327094924970494 for rf_10_4\n",
      "canceldown-2-1-1-d_-1\n",
      "### TRAIN / TEST SPLIT\n",
      "orig-label-0: 20127\n",
      "orig-label-1: 3993\n",
      "downsampled label-1 = 3993, label-0 ~ 3992.80161963434\n",
      "\n",
      "------------------------------------\n",
      "TRAINING canceldown-2-1-1-d_-1 rf_10_4\n",
      "------------------------------------\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o90947.count.\n: java.lang.OutOfMemoryError: Not enough memory to build and broadcast the table to all worker nodes. As a workaround, you can either disable broadcast by setting spark.sql.autoBroadcastJoinThreshold to -1 or increase the spark driver memory by setting spark.driver.memory to a higher value.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.notEnoughMemoryToBuildAndBroadcastTableError(QueryExecutionErrors.scala:1838)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.$anonfun$relationFuture$1(BroadcastExchangeExec.scala:183)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$1(SQLExecution.scala:191)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m         df_train \u001b[38;5;241m=\u001b[39m downsample(df_train, DOWNSAMPLE)\n\u001b[1;32m     29\u001b[0m         df_train \u001b[38;5;241m=\u001b[39m df_train\u001b[38;5;241m.\u001b[39mpersist()\n\u001b[0;32m---> 30\u001b[0m         model, f1, model_name \u001b[38;5;241m=\u001b[39m \u001b[43mhyper_tune_rf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfigstr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mCF\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMODEL\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m         df_train\u001b[38;5;241m.\u001b[39munpersist()\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFINISHED\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 186\u001b[0m, in \u001b[0;36mhyper_tune_rf\u001b[0;34m(config, num_tree_values, max_depth_values)\u001b[0m\n\u001b[1;32m    184\u001b[0m logresult(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTRAINING \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfigstr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    185\u001b[0m logresult(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m------------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 186\u001b[0m accuracy, precision, recall, f1 \u001b[38;5;241m=\u001b[39m \u001b[43mconfuse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredict_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: f1 \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m f1 \u001b[38;5;241m>\u001b[39m best_f1:\n",
      "Cell \u001b[0;32mIn[4], line 126\u001b[0m, in \u001b[0;36mconfuse\u001b[0;34m(df_test_pred)\u001b[0m\n\u001b[1;32m    124\u001b[0m n00 \u001b[38;5;241m=\u001b[39m df_test_pred\u001b[38;5;241m.\u001b[39mwhere((F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m&\u001b[39m(F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m))\u001b[38;5;241m.\u001b[39mcount()\n\u001b[1;32m    125\u001b[0m n01 \u001b[38;5;241m=\u001b[39m df_test_pred\u001b[38;5;241m.\u001b[39mwhere((F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m&\u001b[39m(F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mcount()\n\u001b[0;32m--> 126\u001b[0m n10 \u001b[38;5;241m=\u001b[39m \u001b[43mdf_test_pred\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m&\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m n11 \u001b[38;5;241m=\u001b[39m df_test_pred\u001b[38;5;241m.\u001b[39mwhere((F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m&\u001b[39m(F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mcount()\n\u001b[1;32m    128\u001b[0m s00 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{:5d}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n00)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py:804\u001b[0m, in \u001b[0;36mDataFrame.count\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    794\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m    795\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the number of rows in this :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m    796\u001b[0m \n\u001b[1;32m    797\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;124;03m    2\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 804\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o90947.count.\n: java.lang.OutOfMemoryError: Not enough memory to build and broadcast the table to all worker nodes. As a workaround, you can either disable broadcast by setting spark.sql.autoBroadcastJoinThreshold to -1 or increase the spark driver memory by setting spark.driver.memory to a higher value.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.notEnoughMemoryToBuildAndBroadcastTableError(QueryExecutionErrors.scala:1838)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.$anonfun$relationFuture$1(BroadcastExchangeExec.scala:183)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$1(SQLExecution.scala:191)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n"
     ]
    }
   ],
   "source": [
    "CF = {}\n",
    "for train_config in train_configs:\n",
    " for CHURN in train_config[\"CHURN\"]:\n",
    "  for FUTURE_LOOKAHEAD_WEEKS in train_config[\"FUTURE_LOOKAHEAD_WEEKS\"]:\n",
    "   for PAST_NEAR_HISTORY_WEEKS in train_config[\"PAST_NEAR_HISTORY_WEEKS\"]:\n",
    "    for PAST_OLD_HISTORY_WEEKS in train_config[\"PAST_OLD_HISTORY_WEEKS\"]:\n",
    "     for FEATURE_COLS in train_config[\"FEATURE_COLS\"]:\n",
    "      for DOWNSAMPLE in train_config[\"DOWNSAMPLE\"]:\n",
    "       for MODEL in train_config[\"MODEL\"]:\n",
    "        CF[\"CHURN\"] = CHURN\n",
    "        CF[\"FUTURE_LOOKAHEAD_WEEKS\"] = FUTURE_LOOKAHEAD_WEEKS\n",
    "        CF[\"PAST_NEAR_HISTORY_WEEKS\"] = PAST_NEAR_HISTORY_WEEKS\n",
    "        CF[\"PAST_OLD_HISTORY_WEEKS\"] = PAST_OLD_HISTORY_WEEKS\n",
    "        CF[\"FEATURE_COLS\"] = FEATURE_COLS\n",
    "        CF[\"DOWNSAMPLE\"] = DOWNSAMPLE\n",
    "        CF[\"MODEL\"] = MODEL\n",
    "        CF[\"MODEL_URL\"] = EVENT_DATA_URL.replace(\"/sparkify/\", \"/sparkify/output/07-model-{PAST_OLD_HISTORY_WEEKS}-{PAST_NEAR_HISTORY_WEEKS}-{FUTURE_LOOKAHEAD_WEEKS}-{CHURN}\").replace(\".json\", \"\")\n",
    "        CF[\"TESTTRAIN_DATA_URL\"] = EVENT_DATA_URL.replace(\"/sparkify/\", f\"/sparkify/output/07-testtrain-{PAST_OLD_HISTORY_WEEKS}-{PAST_NEAR_HISTORY_WEEKS}-{FUTURE_LOOKAHEAD_WEEKS}-{CHURN}\")\n",
    "        prefixes = str(FEATURE_COLS).replace(\"[\",\"\").replace(\"]\",\"\").replace(\"'\",\"\").replace(\",\",\"\").replace(\" \",\"\")\n",
    "        configstr = f\"{CHURN}-{FUTURE_LOOKAHEAD_WEEKS}-{PAST_NEAR_HISTORY_WEEKS}-{PAST_OLD_HISTORY_WEEKS}-{prefixes}-{DOWNSAMPLE}\"\n",
    "        print(f\"{configstr}\")\n",
    "        df_testtrain_vec = create_train_test_data(CF)\n",
    "        \n",
    "        print(f\"### TRAIN / TEST SPLIT\")\n",
    "        df_train, df_test = df_testtrain_vec.randomSplit([0.7, 0.3], seed=42)\n",
    "        \n",
    "        df_train = downsample(df_train, DOWNSAMPLE)\n",
    "\n",
    "        df_train = df_train.persist()\n",
    "        model, f1, model_name = hyper_tune_rf(configstr, *CF[\"MODEL\"][\"rf\"])\n",
    "        df_train.unpersist()\n",
    "        \n",
    "print(\"FINISHED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b2c23e48-4b7f-4af6-b41d-73c3f403b896",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### SAVING TESTTRAIN DATA s3a://udacity-dsnd/sparkify/output/06-testtrain-sparkify_event_data-churncanceldown.json\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "# -----------------\n",
    "\n",
    "ttd_url = TESTTRAIN_DATA_URL.replace(\".json\",f\"-churn{CHURN}.json\")\n",
    "print(f\"### SAVING TESTTRAIN DATA {ttd_url}\")\n",
    "df_testtrain.write.format('json').mode('overwrite').save(ttd_url)\n",
    "print(f\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c26b914a-60cf-46c0-ac62-2203143bed51",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### LOAD TESTTRAIN DATA s3a://udacity-dsnd/sparkify/output/06-testtrain-sparkify_event_data-churncanceldown.json\n",
      "### PERSIST\n"
     ]
    }
   ],
   "source": [
    "# -----------------\n",
    "\n",
    "ttd_url = TESTTRAIN_DATA_URL.replace(\".json\",f\"-churn{CHURN}.json\")\n",
    "print(f\"### LOAD TESTTRAIN DATA {ttd_url}\")\n",
    "df_testtrain = spark.read.json(ttd_url)\n",
    "print(f\"### PERSIST\")\n",
    "df_testtrain_persist = df_testtrain.persist()\n",
    "df_testtrain = df_testtrain_persist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b5eb1a69-82fa-4190-b5b0-ad959dce6619",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_testtrain_orig = df_testtrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f4cb27d2-abea-4795-be45-0578bba434e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df_testtrain = oversample(df_testtrain)\n",
    "#df_testtrain = downsample(df_testtrain, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f3d4de6c-9882-44d0-9076-11f96fcaf2d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### CREATE FEATURE COLUMN\n",
      "### TRAIN / TEST SPLIT\n",
      "orig-label-0: 23793\n",
      "orig-label-1: 2644\n",
      "downsampled label-1 = 2644, label-0 ~ 5287.555536877495\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(f\"### CREATE FEATURE COLUMN\")\n",
    "\n",
    "featureCols = [\"paid\", \"usermale\", \"userregistration\"]    \n",
    "\n",
    "featureCols = [*featureCols, *[col for col in df_testtrain.columns if col.startswith(\"r_\")]]\n",
    "featureCols = [*featureCols, *[col for col in df_testtrain.columns if col.startswith(\"nh_\")]]\n",
    "\n",
    "#featureCols = [col for col in df_testtrain.columns if not col in [\"userId\", \"wid\", \"label\"]]\n",
    "assembler = VectorAssembler(inputCols=featureCols, outputCol=\"features\")\n",
    "df_testtrain_vec=assembler.transform(df_testtrain).select(\"userId\", \"wid\", \"label\",\"features\")\n",
    "\n",
    "\n",
    "df_testtrain_vec_persist = df_testtrain_vec.persist()\n",
    "df_testtrain_vec = df_testtrain_vec_persist\n",
    "\n",
    "# -----------------\n",
    "\n",
    "print(f\"### TRAIN / TEST SPLIT\")\n",
    "df_train, df_test = df_testtrain_vec.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "#print(f\"train: {df_train.count()}\")\n",
    "#print(f\"  l1: {df_train.where(df_train.label==1).count()}\")\n",
    "#print(f\"  l0: {df_train.where(df_train.label==0).count()}\")\n",
    "#print(f\"test: {df_test.count()}\")\n",
    "#print(f\"  l1: {df_test.where(df_test.label==1).count()}\")\n",
    "#print(f\"  l0: {df_test.where(df_test.label==0).count()}\")\n",
    "\n",
    "## Fit scaler to train dataset\n",
    "#scaler = MaxAbsScaler().setInputCol('features').setOutputCol('scaled_features')\n",
    "#df_train = df_train.drop(\"scaled_features\")\n",
    "#scaler_model = scaler.fit(df_train)\n",
    "## Scale train and test features\n",
    "#df_train = scaler_model.transform(df_train)\n",
    "#df_test = df_test.drop(\"scaled_features\")\n",
    "#df_test = scaler_model.transform(df_test)\n",
    "\n",
    "# -----------------\n",
    "df_test_orig = df_test\n",
    "df_train_orig = df_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "21448c99-cb4f-4d10-96ef-4631baa13554",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#df_train = oversample(df_train, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c523e445-7fe4-4f7f-b091-b0f496edc929",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 5173\n",
      "  l1: 2585\n",
      "  l0: 2588\n",
      "test: 2274\n",
      "  l1: 1176\n",
      "  l0: 1098\n"
     ]
    }
   ],
   "source": [
    "print(f\"train: {df_train.count()}\")\n",
    "print(f\"  l1: {df_train.where(df_train.label==1).count()}\")\n",
    "print(f\"  l0: {df_train.where(df_train.label==0).count()}\")\n",
    "print(f\"test: {df_test.count()}\")\n",
    "print(f\"  l1: {df_test.where(df_test.label==1).count()}\")\n",
    "print(f\"  l0: {df_test.where(df_test.label==0).count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6ed1fc9a-ef90-482b-983b-67361ce2e193",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oversampling to: 1400/10233\n",
      "oversampling to: 2100/10233\n",
      "oversampling to: 2800/10233\n",
      "oversampling to: 3500/10233\n",
      "oversampling to: 4200/10233\n",
      "oversampling to: 4900/10233\n",
      "oversampling to: 5600/10233\n",
      "oversampling to: 6300/10233\n",
      "oversampling to: 7000/10233\n",
      "oversampling to: 7700/10233\n",
      "oversampling to: 8400/10233\n",
      "oversampling to: 9100/10233\n",
      "oversampling to: 9800/10233\n",
      "oversampling to: 10500/10233\n"
     ]
    }
   ],
   "source": [
    "df_test = oversample(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be7e432a-4c0a-48fc-ab2f-c7a932cf4b80",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oversampling to: 2230/25009\n",
      "oversampling to: 3345/25009\n",
      "oversampling to: 4460/25009\n",
      "oversampling to: 5575/25009\n",
      "oversampling to: 6690/25009\n",
      "oversampling to: 7805/25009\n",
      "oversampling to: 8920/25009\n",
      "oversampling to: 10035/25009\n",
      "oversampling to: 11150/25009\n",
      "oversampling to: 12265/25009\n",
      "oversampling to: 13380/25009\n",
      "oversampling to: 14495/25009\n",
      "oversampling to: 15610/25009\n",
      "oversampling to: 16725/25009\n",
      "oversampling to: 17840/25009\n",
      "oversampling to: 18955/25009\n",
      "oversampling to: 20070/25009\n",
      "oversampling to: 21185/25009\n",
      "oversampling to: 22300/25009\n",
      "oversampling to: 23415/25009\n",
      "oversampling to: 24530/25009\n",
      "oversampling to: 25645/25009\n"
     ]
    }
   ],
   "source": [
    "df_train_orig = df_train\n",
    "df_train = oversample(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98acd19d-53e1-4c78-a732-d8984c479011",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model, f1, model_name = hyper_tune_rf([20], [5])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43218ff7-bbfc-4f7b-9aa5-e48fa90e7f77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model, f1, model_name = hyper_tune_lr([100], [0], [0])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "19ed8fda-31d2-473f-8359-b11544e5a18d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "| userId|count|\n",
      "+-------+-----+\n",
      "|1554956|    4|\n",
      "|1339528|    4|\n",
      "|1538485|    3|\n",
      "|1125943|    3|\n",
      "|1831733|    3|\n",
      "|1586895|    3|\n",
      "|1392770|    3|\n",
      "|1373602|    3|\n",
      "|1178026|    3|\n",
      "|1602181|    3|\n",
      "|1390064|    3|\n",
      "|1591353|    3|\n",
      "|1812177|    3|\n",
      "|1141231|    3|\n",
      "|1888253|    3|\n",
      "|1116029|    3|\n",
      "|1386578|    3|\n",
      "|1558736|    3|\n",
      "|1996408|    3|\n",
      "|1037209|    3|\n",
      "+-------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train.groupBy(\"userId\").count().sort(F.desc(F.col(\"count\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "242cdc8f-9cac-410f-a091-034052a32078",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.where(F.col(\"userId\")==\"1655208\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "63cd2a28-73fb-4545-8a47-154a6fa60e23",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### PREDICT TRAIN\n",
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       1.0|\n",
      "|    0|       1.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "### PREDICT TEST\n",
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "|    0|       1.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       1.0|\n",
      "|    0|       1.0|\n",
      "|    0|       1.0|\n",
      "|    0|       1.0|\n",
      "|    0|       0.0|\n",
      "|    0|       1.0|\n",
      "|    0|       0.0|\n",
      "+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "### EVALUATE PREDICTION\n",
      "+-----+--------------------+----------+--------------------+\n",
      "|label|       rawPrediction|prediction|         probability|\n",
      "+-----+--------------------+----------+--------------------+\n",
      "|    0|[8.59056034958328...|       1.0|[0.42952801747916...|\n",
      "|    0|[12.1124409849263...|       0.0|[0.60562204924631...|\n",
      "|    0|[13.7858852326191...|       0.0|[0.68929426163095...|\n",
      "|    0|[8.29733438713462...|       1.0|[0.41486671935673...|\n",
      "|    0|[9.09423883661818...|       1.0|[0.45471194183090...|\n",
      "+-----+--------------------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "The area under ROC for train set is 0.7060216921425774\n",
      "The area under ROC for test set is 0.6850268112709564\n",
      "### EVAL TRAIN:\n",
      "                  \n",
      " Confusion Matrix: \n",
      "                  \n",
      "     | prediction| \n",
      "     |   0 |  1  | \n",
      " ----+-----+-----+ \n",
      " l 0 | 1594|  994| \n",
      " b --+-----+-----+ \n",
      " l 1 |  865| 1720| \n",
      " ----+-----+-----+ \n",
      "                   \n",
      "CALC\n",
      "  accuraccy: 0.640634061473033\n",
      "  precision: 0.6337509211495947\n",
      "  recall:    0.6653771760154739\n",
      "  f1:        0.6491790903944141\n",
      "  mcc:       0.2816391488947921\n",
      "### EVAL TEST:\n",
      "                  \n",
      " Confusion Matrix: \n",
      "                  \n",
      "     | prediction| \n",
      "     |   0 |  1  | \n",
      " ----+-----+-----+ \n",
      " l 0 |  667|  431| \n",
      " b --+-----+-----+ \n",
      " l 1 |  413|  763| \n",
      " ----+-----+-----+ \n",
      "                   \n",
      "CALC\n",
      "  accuraccy: 0.6288478452066842\n",
      "  precision: 0.6390284757118928\n",
      "  recall:    0.6488095238095238\n",
      "  f1:        0.6438818565400843\n",
      "  mcc:       0.2564493005019037\n"
     ]
    }
   ],
   "source": [
    "# -----------------\n",
    "\n",
    "print(f\"### PREDICT TRAIN\")\n",
    "predict_train = model.transform(df_train)\n",
    "predict_train.select(\"label\", \"prediction\").show(10)\n",
    "print(f\"### PREDICT TEST\")\n",
    "predict_test  = model.transform(df_test)\n",
    "predict_test.select(\"label\", \"prediction\").show(10)\n",
    "\n",
    "# -----------------\n",
    "\n",
    "print(f\"### EVALUATE PREDICTION\")\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol ='rawPrediction', labelCol ='label')\n",
    "predict_test.select(\"label\", \"rawPrediction\", \"prediction\", \"probability\").show(5)\n",
    "print(\"The area under ROC for train set is {}\".format(evaluator.evaluate(predict_train)))\n",
    "print(\"The area under ROC for test set is {}\".format(evaluator.evaluate(predict_test)))\n",
    "\n",
    "print(f\"### EVAL TRAIN:\")\n",
    "confuse(predict_train)\n",
    "print(f\"### EVAL TEST:\")\n",
    "acc, prec, rec, f1 = confuse(predict_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "54651be2-0cae-437d-b15c-05bd63d6bb27",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### SAVE MODEL rf_20_5 22.481265611990008\n",
      "model saved to s3a://udacity-dsnd/sparkify/output/05-model-sparkify_event_data_rf_20_5_f1val0.225\n"
     ]
    }
   ],
   "source": [
    "# -----------------\n",
    "\n",
    "print(f\"### SAVE MODEL {model_name} {f1*100}\")\n",
    "model_url = f'{MODEL_URL}_{model_name}_f1val{round(f1,3)}'\n",
    "model.write().overwrite().save(model_url)\n",
    "print(f\"model saved to {model_url}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "b008cf24-a1ba-47a0-9358-030e895948e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('paid', 0.09554455155152178),\n",
       " ('oh_session_hours', 0.09302707160037064),\n",
       " ('oh_pg_thumbs_up', 0.07582709757031283),\n",
       " ('nh_pg_home', 0.07340830594860079),\n",
       " ('oh_pg_home', 0.0725284688157008),\n",
       " ('oh_session_start', 0.06713461595629495),\n",
       " ('oh_pg_settings', 0.050296622196977205),\n",
       " ('oh_pg_nextsong', 0.03470888922561419),\n",
       " ('nh_pg_logout', 0.031418303669647894),\n",
       " ('oh_pg_logout', 0.029338603181943555),\n",
       " ('oh_pg_thumbs_down', 0.02669508164427074),\n",
       " ('oh_pg_downgrade', 0.0263905768134274),\n",
       " ('oh_pg_add_to_playlist', 0.02189388493915634),\n",
       " ('nh_pg_thumbs_up', 0.021718938628132543),\n",
       " ('nh_pg_downgrade', 0.021610587051586076),\n",
       " ('oh_pg_login', 0.021310639336320675),\n",
       " ('nh_pg_thumbs_down', 0.020975045098593534),\n",
       " ('nh_pg_nextsong', 0.01929290425786421),\n",
       " ('nh_pg_add_to_playlist', 0.01864976547102134),\n",
       " ('nh_status_307', 0.016846133774795467),\n",
       " ('nh_session_hours', 0.016745889155579934),\n",
       " ('nh_pg_add_friend', 0.014856832748766466),\n",
       " ('oh_pg_roll_advert', 0.014159788446922988),\n",
       " ('nh_pg_settings', 0.011455073384124942),\n",
       " ('oh_status_307', 0.011261520204120721),\n",
       " ('oh_pg_about', 0.01081888688907817),\n",
       " ('oh_pg_add_friend', 0.010449979278293572),\n",
       " ('userregistration', 0.010331715693484279),\n",
       " ('nh_session_start', 0.009619201226052369),\n",
       " ('oh_pg_help', 0.00593734543223226),\n",
       " ('nh_pg_upgrade', 0.005483083691995258),\n",
       " ('nh_pg_help', 0.0054691825812952415),\n",
       " ('nh_pg_login', 0.005171923839646339),\n",
       " ('oh_status_404', 0.004981750333382591),\n",
       " ('nh_pg_roll_advert', 0.004403692638058564),\n",
       " ('nh_pg_submit_downgrade', 0.004249893599462164),\n",
       " ('oh_pg_save_settings', 0.0034216769552920688),\n",
       " ('nh_pg_error', 0.0024428874818185698),\n",
       " ('oh_pg_submit_upgrade', 0.0019265281149218618),\n",
       " ('oh_pg_error', 0.0019210946597746476),\n",
       " ('oh_pg_upgrade', 0.0014654235781669683),\n",
       " ('nh_pg_about', 0.0013718261158678826),\n",
       " ('oh_pg_submit_downgrade', 0.0011572777978853636),\n",
       " ('nh_status_404', 0.0010957622431648402),\n",
       " ('usermale', 0.0007544194723368592),\n",
       " ('nh_pg_submit_upgrade', 0.00043125770612211737),\n",
       " ('nh_pg_cancel', 0.0),\n",
       " ('nh_pg_cancellation_confirmation', 0.0),\n",
       " ('nh_pg_register', 0.0),\n",
       " ('nh_pg_save_settings', 0.0),\n",
       " ('nh_pg_submit_registration', 0.0),\n",
       " ('oh_pg_cancel', 0.0),\n",
       " ('oh_pg_cancellation_confirmation', 0.0),\n",
       " ('oh_pg_register', 0.0),\n",
       " ('oh_pg_submit_registration', 0.0)]"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featimp = model.featureImportances\n",
    "nameimp = {}\n",
    "for i in range(len(featimp)):\n",
    "    nameimp[featureCols[i]] = featimp[i]\n",
    "sorted(nameimp.items(), key=lambda x:-x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2614fba5-caf1-4f40-a7f9-7481366b5d03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fcc868-f8e5-4498-87c0-7fd090587025",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1087fe0-1964-4c85-a5de-83be14d5fc6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "87d4cb43-ef96-4630-9844-f463b7b03c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### STOP SPARK SESSION\n"
     ]
    }
   ],
   "source": [
    "print(\"### STOP SPARK SESSION\")\n",
    "spark.stop()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899ac3c4-e4ee-42f3-88c8-a46ae17737df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
