{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15fbf17d-596c-419b-ac66-e8126275f46a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# \\[07\\] Auto-Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df5acf66-8ff9-4d6f-9d94-f36e3a90c38d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### SETUP SPARK SESSION \"Sparkify\"\n",
      "Spark version: 3.3.2\n"
     ]
    }
   ],
   "source": [
    "EVENT_DATA_URL = \"s3a://udacity-dsnd/sparkify/sparkify_event_data.json\"\n",
    "# EVENT_DATA_URL = \"s3a://udacity-dsnd/sparkify/mini_sparkify_event_data.json\"\n",
    "\n",
    "CLEAN_DATA_URL = EVENT_DATA_URL.replace(\"/sparkify/\", \"/sparkify/output/02-cleaned-\")\n",
    "WEEK_AGGREGATED_DATA_URL = EVENT_DATA_URL.replace(\"/sparkify/\", \"/sparkify/output/04-week-aggregated-\")\n",
    "\n",
    "\n",
    "EXECUTOR_INSTANCES = 2\n",
    "EXECUTOR_MEM = '6g'\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from cryptography.fernet import Fernet\n",
    "import base64\n",
    "import socket\n",
    "\n",
    "!./install-s3-jars.sh\n",
    "\n",
    "def decrypt(encrypted_text):\n",
    "    \"\"\"\n",
    "    decrypts an encrypted text. The seed (master-password) for decryption is read from the file \".seed.txt\"\n",
    "    \n",
    "    Input: encrypted_text\n",
    "    \n",
    "    Output: the decrypted text. If the text was not encrypted with the same seed, \n",
    "            an exception is raised.\n",
    "    \"\"\"\n",
    "    with open('.seed.txt') as f:\n",
    "        seed = f.read().strip()\n",
    "    return Fernet(base64.b64encode((seed*32)[:32].encode('ascii')).decode('ascii')).decrypt(encrypted_text.encode('ascii')).decode('ascii')\n",
    "\n",
    "AWS_ACCESS_KEY_ID='V6ge1JcQpvyYGJjb'\n",
    "AWS_SECRET_ACCESS_KEY = decrypt('gAAAAABkDFI6865LaVJVgtTYo0aMx9-JTPbTo6cwOUjg5eNNPsZhBDoHbRZ8xuXQT0ImNfvqcecZuoJd1VzYQEpBaxyCnKvosii8O1KeqoL2NwKdKtL_AUfT4eW4dvJVP--VjEvc0gB4')\n",
    "OWN_IP=socket.gethostbyname(socket.gethostname())\n",
    "APP_NAME = \"Sparkify\"\n",
    "SPARK_MASTER = \"spark://bit-spark-master-svc.spark.svc.cluster.local:7077\"\n",
    "S3_HOST = \"minio-api-service.minio.svc\"\n",
    "\n",
    "print(f'### SETUP SPARK SESSION \"{APP_NAME}\"')\n",
    "spark = SparkSession.builder \\\n",
    "    .master(SPARK_MASTER) \\\n",
    "    .config(\"spark.jars\",\"/home/jovyan/jars/aws-java-sdk-bundle-1.11.1026.jar,/home/jovyan/jars/hadoop-aws-3.3.2.jar\") \\\n",
    "    .config(\"spark.driver.host\", OWN_IP) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", S3_HOST) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", AWS_ACCESS_KEY_ID) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", AWS_SECRET_ACCESS_KEY) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.executor.instances\", EXECUTOR_INSTANCES) \\\n",
    "    .config(\"spark.executor.memory\", EXECUTOR_MEM) \\\n",
    "    .appName(APP_NAME).getOrCreate()\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"WARN\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1281929a-6fdb-40bb-927e-db177696289f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### LOAD DATA s3a://udacity-dsnd/sparkify/output/04-week-aggregated-sparkify_event_data.json\n",
      "### PERSIST\n"
     ]
    }
   ],
   "source": [
    "print(f\"### LOAD DATA {WEEK_AGGREGATED_DATA_URL}\")\n",
    "df_userweek = spark.read.json(WEEK_AGGREGATED_DATA_URL)\n",
    "print(f\"### PERSIST\")\n",
    "df_userweek_persist = df_userweek.persist()\n",
    "df_userweek = df_userweek_persist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "75e62365-bffe-4b21-827b-087d467adc6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier, RandomForestClassificationModel, DecisionTreeClassifier, DecisionTreeClassificationModel, LinearSVC\n",
    "from pyspark.ml.feature import RegexTokenizer, VectorAssembler, Normalizer, StandardScaler, MinMaxScaler, MaxAbsScaler\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.types import IntegerType\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import datetime\n",
    "\n",
    "# timestamp constants for ts in milliseconds\n",
    "one_hour =        60*60*1000  #     3.600.000\n",
    "one_day =      24*60*60*1000  #    86.400.000\n",
    "one_week =   7*24*60*60*1000  #   604.800.000\n",
    "\n",
    "def logresult(text):\n",
    "    \"\"\"\n",
    "    Input: text\n",
    "    Print given text to console and also write it at the end of the file \"result.log\".\n",
    "    This allows persisting the output of longer /multiple train runs.\n",
    "    \"\"\"\n",
    "    print(text)\n",
    "    with open(\"result.log\", \"a\") as logf:\n",
    "        logf.write(text+\"\\n\")\n",
    "\n",
    "def oversample(df_train):\n",
    "    df_lab0 = df_train.where(F.col(\"label\") == 0)\n",
    "    df_lab1 = df_train.where(F.col(\"label\") == 1)\n",
    "    train0cnt = df_lab0.count()\n",
    "    train1cnt = df_lab1.count()\n",
    "    oversampled_train = df_train\n",
    "    sum1cnt = train1cnt\n",
    "    while sum1cnt <= train0cnt:\n",
    "        sum1cnt = sum1cnt+train1cnt\n",
    "        print(f\"oversampling to: {sum1cnt}/{train0cnt}\")\n",
    "        oversampled_train = oversampled_train.union(df_lab1)\n",
    "    return oversampled_train\n",
    "\n",
    "def downsample(df_train, factor):\n",
    "    df_lab0 = df_train.where(F.col(\"label\") == 0)\n",
    "    df_lab1 = df_train.where(F.col(\"label\") == 1)\n",
    "    train0cnt = df_lab0.count()\n",
    "    print(f\"orig-label-0: {train0cnt}\")\n",
    "    train1cnt = df_lab1.count()\n",
    "    print(f\"orig-label-1: {train1cnt}\")\n",
    "    frac = train1cnt/(factor*train0cnt+1)\n",
    "    df_downsampled = df_lab0.sample(fraction = frac, seed=42)\n",
    "    df_downsampled = df_downsampled.union(df_lab1)\n",
    "    print(f\"downsampled label-1 = {train1cnt}, label-0 ~ {train0cnt*frac}\")\n",
    "    return df_downsampled\n",
    "\n",
    "def add_weight_col(df_train):\n",
    "    \"\"\"\n",
    "    Input:  Training DataFrame with a column \"label\" containing a binary classification (1 or 0)\n",
    "    Output: Newly created DataFrame, which contains an additional \"weight\" column, which compensates \n",
    "            the different frequency of both partitions. The sum of weights for label \"1\" is equal to the \n",
    "            sum of weights for the label \"0\".\n",
    "    \"\"\"\n",
    "    label_counts = df_train.agg(F.sum(F.col(\"label\")).alias(\"l1\"), F.sum(1-F.col(\"label\")).alias(\"l0\")).collect()[0]\n",
    "    l0 = label_counts.l0\n",
    "    l1 = label_counts.l1\n",
    "    w1 = l0 / (l0+l1)\n",
    "    w0 = l1 / (l0+l1)\n",
    "    print(f\"label 0: {l0}, label 1: {l1}\")\n",
    "    df_result = df_train.withColumn(\"weight\", F.when(F.col(\"label\")==1, F.lit(w1)).otherwise(F.lit(w0)))\n",
    "    return df_result\n",
    "    \n",
    "def prefix_columns(df_orig, prefix, do_not_change_cols):\n",
    "    \"\"\"\n",
    "    Input:  df_orig - original DataFrame\n",
    "            prefix - string to be added to all column names\n",
    "            do_not_change_cols - columns which should be excluded from beeing prefixed\n",
    "    Output: new DataFrame with renamed columns. All columns now start with the given prefix,\n",
    "            with the exception of the columns named in do_not_change_cols\n",
    "    \"\"\"\n",
    "    newcols = [prefix+col if not col in do_not_change_cols else col for col in df_orig.columns]\n",
    "    return df_orig.toDF(*newcols)\n",
    "\n",
    "def aggregate_week_data(from_week, to_week):\n",
    "    \"\"\"\n",
    "    Input: from_week, to_week\n",
    "    Output: aggregated sum data for the weeks from_week..to_week (both including)\n",
    "    \"\"\"\n",
    "    dropcols = [\"paid\", \"usermale\", \"userregistration\", \"wid\"]\n",
    "    df_weeks = df_userweek.where((F.col(\"wid\")>=from_week)&(F.col(\"wid\")<=to_week))\n",
    "    if from_week == to_week:\n",
    "        # no aggregation necessary, if there is only one week\n",
    "        return df_weeks.drop(*dropcols)\n",
    "    aggs = [F.sum(F.col(col)).alias(col) for col in df_weeks.columns if not col in [\"userId\", *dropcols]]\n",
    "    df_weeks = df_weeks.groupBy(\"userId\").agg(*aggs)\n",
    "    return df_weeks    \n",
    "\n",
    "\n",
    "def create_test_data(CF, current_week):\n",
    "    \"\"\"\n",
    "    Input:  CF - configuration to be used for aggregations\n",
    "            current_week - the latest week of history data\n",
    "    split data into three timeslots (future/new-history/old-histor)\n",
    "    and then use ######\n",
    "    \"\"\"\n",
    "\n",
    "    label_week_min = current_week-CF[\"FUTURE_LOOKAHEAD_WEEKS\"]\n",
    "    label_week_max = current_week-1\n",
    "\n",
    "    newhistory_week_min = current_week\n",
    "    newhistory_week_max = newhistory_week_min+CF[\"PAST_NEAR_HISTORY_WEEKS\"]-1\n",
    "\n",
    "    oldhistory_week_min = newhistory_week_max+1\n",
    "    oldhistory_week_max = oldhistory_week_min+CF[\"PAST_OLD_HISTORY_WEEKS\"]-1\n",
    "    \n",
    "    df_user = df_userweek.where(F.col(\"wid\") == newhistory_week_min).select(\"userId\", \"wid\", \"paid\", \"usermale\", \"userregistration\")\n",
    "    df_user = df_user.withColumn(\"userregistration\", F.col(\"userregistration\")-7*newhistory_week_min)\n",
    "\n",
    "    df_label = aggregate_week_data(label_week_min, label_week_max)\n",
    "    df_newhistory = aggregate_week_data(newhistory_week_min, newhistory_week_max)\n",
    "    df_oldhistory = aggregate_week_data(oldhistory_week_min, oldhistory_week_max)\n",
    "\n",
    "    if CF[\"CHURN\"]==\"canceldown\":\n",
    "        df_label = df_label.withColumn(\"label\", F.when(F.col(\"pg_cancellation_confirmation\")+F.col(\"pg_submit_downgrade\")>0, F.lit(1)).otherwise(F.lit(0))).select(\"userid\", \"label\")\n",
    "    elif CF[\"CHURN\"]==\"cancel\":\n",
    "        df_label = df_label.withColumn(\"label\", F.when(F.col(\"pg_cancellation_confirmation\")>0, F.lit(1)).otherwise(F.lit(0))).select(\"userid\", \"label\")\n",
    "    elif CF[\"CHURN\"]==\"down\":\n",
    "        df_label = df_label.withColumn(\"label\", F.when(F.col(\"pg_submit_downgrade\")>0, F.lit(1)).otherwise(F.lit(0))).select(\"userid\", \"label\")\n",
    "    else: \n",
    "        raise Exception(f'invalid value for CHURN {CF[\"CHURN\"]}')\n",
    "    df_user = df_user.join(df_label, \"userId\")\n",
    "\n",
    "    df_user = df_user.join(prefix_columns(df_newhistory, \"nh_\", [\"userId\"]), \"userId\")\n",
    "    df_user = df_user.join(prefix_columns(df_oldhistory, \"oh_\", [\"userId\"]), \"userId\")\n",
    "\n",
    "    for c in df_newhistory.columns:\n",
    "        if not c in [\"userId\", \"session_hours\", \"session_start\"]:\n",
    "            df_user = df_user.withColumn(\"nhn_\"+c, F.col(\"nh_\"+c)/F.greatest(F.col(\"nh_session_hours\"), F.lit(0.01)))\n",
    "    df_user = df_user.withColumn(\"nhn_session_hours\", F.col(\"nh_session_hours\")/CF[\"PAST_NEAR_HISTORY_WEEKS\"])\n",
    "    df_user = df_user.withColumn(\"nhn_session_start\", F.col(\"nh_session_start\")/CF[\"PAST_NEAR_HISTORY_WEEKS\"])\n",
    "\n",
    "    for c in df_oldhistory.columns:\n",
    "        if not c in [\"userId\", \"session_hours\", \"session_start\"]:\n",
    "            df_user = df_user.withColumn(\"ohn_\"+c, F.col(\"oh_\"+c)/F.greatest(F.col(\"oh_session_hours\"), F.lit(0.01)))\n",
    "    df_user = df_user.withColumn(\"ohn_session_hours\", F.col(\"oh_session_hours\")/CF[\"PAST_OLD_HISTORY_WEEKS\"])\n",
    "    df_user = df_user.withColumn(\"ohn_session_start\", F.col(\"oh_session_start\")/CF[\"PAST_OLD_HISTORY_WEEKS\"])\n",
    "    \n",
    "    for c in df_newhistory.columns:\n",
    "        if not c in [\"userId\"]:\n",
    "            df_user = df_user.withColumn(\"r_\"+c, F.col(\"nhn_\"+c)/F.greatest(F.lit(0.01), F.col(\"ohn_\"+c)))\n",
    "    for c in df_newhistory.columns:\n",
    "        if not c in [\"userId\"]:\n",
    "            df_user = df_user.withColumn(\"d_\"+c, F.col(\"nhn_\"+c)-F.col(\"ohn_\"+c))\n",
    "    \n",
    "    return df_user\n",
    "\n",
    "\n",
    "\n",
    "def confuse(df_test_pred):\n",
    "    n00 = df_test_pred.where((F.col(\"label\")==0)&(F.col(\"prediction\")==0)).count()\n",
    "    n01 = df_test_pred.where((F.col(\"label\")==0)&(F.col(\"prediction\")==1)).count()\n",
    "    n10 = df_test_pred.where((F.col(\"label\")==1)&(F.col(\"prediction\")==0)).count()\n",
    "    n11 = df_test_pred.where((F.col(\"label\")==1)&(F.col(\"prediction\")==1)).count()\n",
    "    s00 = \"{:5d}\".format(n00)\n",
    "    s01 = \"{:5d}\".format(n01)\n",
    "    s10 = \"{:5d}\".format(n10)\n",
    "    s11 = \"{:5d}\".format(n11)\n",
    "    logresult(f\"                  \")\n",
    "    logresult(f\" Confusion Matrix: \")\n",
    "    logresult(f\"                  \")\n",
    "    logresult(f\"     | prediction| \")\n",
    "    logresult(f\"     |   0 |  1  | \")\n",
    "    logresult(f\" ----+-----+-----+ \")\n",
    "    logresult(f\" l 0 |{s00}|{s01}| \")\n",
    "    logresult(f\" b --+-----+-----+ \")\n",
    "    logresult(f\" l 1 |{s10}|{s11}| \")\n",
    "    logresult(f\" ----+-----+-----+ \")\n",
    "    logresult(f\"                   \")\n",
    "    TP = n11\n",
    "    TN = n00\n",
    "    FP = n01\n",
    "    FN = n10\n",
    "    accuracy = 0\n",
    "    if TP+TN+FP+FN!=0:\n",
    "        accuracy = (TP+TN)/(TP+TN+FP+FN)\n",
    "    precision = 0\n",
    "    if TP+FP!=0:\n",
    "        precision = TP/(TP+FP)\n",
    "    recall = 0\n",
    "    if TP+FN!=0:\n",
    "        recall = TP/(TP+FN)\n",
    "    f1 = 0\n",
    "    if precision+recall!=0:\n",
    "        f1 = 2*precision*recall/(precision+recall)\n",
    "    logresult(f\"  accuraccy: {accuracy}\")\n",
    "    logresult(f\"  precision: {precision}\")\n",
    "    logresult(f\"  recall:    {recall}\")\n",
    "    logresult(f\"  f1:        {f1}\")\n",
    "    # https://towardsdatascience.com/matthews-correlation-coefficient-when-to-use-it-and-when-to-avoid-it-310b3c923f7e\n",
    "    mcc = -9\n",
    "    nenn = (TN+FN)*(FP+TP)*(TN+FP)*(FN+TP)\n",
    "    if nenn!=0:   \n",
    "        mcc = (TN*TP-FP*FN)/math.sqrt(nenn)\n",
    "    logresult(f\"  mcc:       {mcc}\")\n",
    "    return (accuracy, precision, recall, f1)\n",
    "    \n",
    "    \n",
    "def hyper_tune_rf(df_train, df_test, configstr, num_tree_values, max_depth_values):   # 20, 5\n",
    "    best_f1 = -1\n",
    "    best_model = None\n",
    "    best_model_name = \"?\"\n",
    "    for num_trees in num_tree_values:\n",
    "        for max_depth in max_depth_values:\n",
    "            model_name = f\"rf_{num_trees}_{max_depth}\"\n",
    "            logresult(f\"\")\n",
    "            logresult(f\"------------------------------------\")\n",
    "            logresult(f\"TRAINING {configstr} {model_name}\")\n",
    "            logresult(f\"------------------------------------\")\n",
    "            rf = RandomForestClassifier(featuresCol=\"features\", numTrees=num_trees, maxDepth=max_depth, weightCol=\"weight\", seed=42)\n",
    "            model = rf.fit(df_train)\n",
    "            predict_test  = model.transform(df_test)\n",
    "            accuracy, precision, recall, f1 = confuse(predict_test)\n",
    "            print(f\"  {model_name}: f1 {f1}\")\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                best_model = model\n",
    "                best_model_name = model_name\n",
    "    print(f\"best f1 {f1} for {best_model_name}\")\n",
    "    return (best_model, best_f1, best_model_name)\n",
    "\n",
    "\n",
    "def hyper_tune_dt(df_train, df_test, configstr, max_depths, max_bins_list):  # 5, 32\n",
    "    best_f1 = -1\n",
    "    best_model = None\n",
    "    best_model_name = \"?\"\n",
    "    for  max_depth in max_depths:\n",
    "        for max_bins in max_bins_list:\n",
    "            model_name = f\"dt_{max_depth}_{max_bins}\"\n",
    "            logresult(f\"\")\n",
    "            logresult(f\"------------------------------------\")\n",
    "            logresult(f\"TRAINING {configstr} {model_name}\")\n",
    "            logresult(f\"------------------------------------\")\n",
    "            dt = DecisionTreeClassifier(featuresCol=\"features\", maxDepth=max_depth, maxBins=max_bins, weightCol=\"weight\", seed=42)\n",
    "            model = dt.fit(df_train)\n",
    "            predict_test  = model.transform(df_test)\n",
    "            accuracy, precision, recall, f1 = confuse(predict_test)\n",
    "            print(f\"  {model_name}: f1 {f1}\")\n",
    "            if f1 > best_f1:\n",
    "                print(f\"new best f1\")\n",
    "                best_f1 = f1\n",
    "                best_model = model\n",
    "                best_model_name = model_name\n",
    "    print(f\"best f1 {f1} for {best_model_name}\")\n",
    "    return (best_model, best_f1, best_model_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def hyper_tune_lr(df_train, df_test, max_iters, reg_params, elastic_net_params): # 100, 0, 0\n",
    "    # https://towardsdatascience.com/beginners-guide-to-linear-regression-with-pyspark-bfc39b45a9e9\n",
    "    evaluator = RegressionEvaluator(predictionCol=\"prediction_orig\", labelCol=\"label\", metricName=\"rmse\") \n",
    "    \n",
    "    best_err = 9999\n",
    "    best_model = None\n",
    "    best_model_name = \"?\"\n",
    "    for  max_iter in  max_iters:\n",
    "        for reg_param in reg_params:\n",
    "            for elastic_net_param in elastic_net_params:\n",
    "                model_name = f\"lr_{max_iter}_{reg_param}_{elastic_net_param}\"\n",
    "                lr = LinearRegression(featuresCol=\"features\", maxIter= max_iter, regParam=reg_param, elasticNetParam=elastic_net_param)\n",
    "                model = lr.fit(df_train)\n",
    "                predict_test  = model.transform(df_test)\n",
    "                predict_test = predict_test.withColumnRenamed(\"prediction\", \"prediction_orig\")\n",
    "                err = evaluator.evaluate(predict_test)\n",
    "                print(f\"err: {err}\")\n",
    "                thr = 0.5\n",
    "                predict_test = predict_test.withColumn(\"prediction\", F.when(F.col(\"prediction_orig\")>=thr,1).otherwise(0))\n",
    "                accuracy, precision, recall, f1 = confuse(predict_test)\n",
    "                print(f\"  {model_name}: f1 {f1}\")\n",
    "                if err < best_err:\n",
    "                    best_err = err\n",
    "                    best_model = model\n",
    "                    best_model_name = model_name\n",
    "    print(f\"best f1 {f1} for {best_model_name}\")\n",
    "    return (best_model, best_err, best_model_name)\n",
    "\n",
    "\n",
    "def hyper_tune_sv(df_train, df_test, max_iters, reg_params):   # 100, 0\n",
    "    best_f1 = -1\n",
    "    best_model = None\n",
    "    best_model_name = \"?\"\n",
    "    for  max_iter in max_iters:\n",
    "        for reg_param in reg_params:\n",
    "            model_name = f\"svm_{max_iter}_{reg_param}\"\n",
    "            lsvc = LinearSVC(featuresCol=\"features\", maxIter=max_iter, regParam=reg_param)\n",
    "            model = lsvc.fit(df_train)\n",
    "            predict_test  = model.transform(df_test)\n",
    "            accuracy, precision, recall, f1 = confuse(predict_test)\n",
    "            print(f\"  {model_name}: f1 {f1}\")\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                best_model = model\n",
    "                best_model_name = model_name\n",
    "    print(f\"best f1 {f1} for {best_model_name}\")\n",
    "    return (best_model, best_f1, best_model_name)\n",
    "    \n",
    "\n",
    "def hyper_tune(df_trainw, df_traind, df_test, configstr, model_config):\n",
    "    if \"rf\" in model_config:\n",
    "          model, f1, model_name = hyper_tune_rf(df_trainw, df_test, configstr, *(model_config[\"rf\"]))\n",
    "    if \"dt\" in model_config:\n",
    "          model, f1, model_name = hyper_tune_dt(df_trainw, df_test, configstr, *(model_config[\"dt\"]))\n",
    "    if \"lr\" in model_config:\n",
    "          model, f1, model_name = hyper_tune_lr(df_traind, df_test, configstr, *(model_config[\"lr\"]))\n",
    "    if \"sv\" in model_config:\n",
    "          model, f1, model_name = hyper_tune_sv(df_traind, df_test, configstr, *(model_config[\"sv\"]))\n",
    "    return (model, f1, model_name)\n",
    "\n",
    "MAX_WID = 8\n",
    "\n",
    "def create_train_test_data(CF):\n",
    "    current_week = CF[\"FUTURE_LOOKAHEAD_WEEKS\"]\n",
    "    history_weeks = CF[\"PAST_NEAR_HISTORY_WEEKS\"]+CF[\"PAST_OLD_HISTORY_WEEKS\"]\n",
    "    df_testtrain = create_test_data(CF, current_week)\n",
    "    while current_week+history_weeks < MAX_WID:\n",
    "        current_week = current_week+1\n",
    "        df_testtrain = df_testtrain.union(create_test_data(CF, current_week))\n",
    "    return df_testtrain\n",
    "\n",
    "def create_train_test_vector(CF, df_testtrain):\n",
    "    featureCols = [\"paid\", \"usermale\", \"userregistration\"]\n",
    "    for prefix in CF[\"FEATURE_COLS\"]:\n",
    "        featureCols = [*featureCols, *[col for col in df_testtrain.columns if col.startswith(prefix)]]\n",
    "    assembler = VectorAssembler(inputCols=featureCols, outputCol=\"features\")\n",
    "    select_cols = [\"userId\", \"wid\", \"label\",\"features\"]\n",
    "    if \"weight\" in df_testtrain.columns:\n",
    "        select_cols = [*select_cols, \"weight\"]\n",
    "    df_testtrain_vec=assembler.transform(df_testtrain).select(*select_cols)\n",
    "    return df_testtrain_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "884f1b52-f375-4cbe-a695-2583bca29272",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_configs_1 = [\n",
    "    {\n",
    "        \"CHURN\":                   [\"canceldown\"],\n",
    "        \"FUTURE_LOOKAHEAD_WEEKS\":  [1],\n",
    "        \"PAST_NEAR_HISTORY_WEEKS\": [1],\n",
    "        \"PAST_OLD_HISTORY_WEEKS\":  [1],\n",
    "        \"FEATURE_COLS\":            [[\"r_\", \"nh_\"]],\n",
    "        \"DOWNSAMPLE\":              [1],\n",
    "        \"MODEL\":                   [{\"rf\": [[10], [4]]}]\n",
    "    },\n",
    "    {\n",
    "        \"CHURN\":                   [\"canceldown\"],\n",
    "        \"FUTURE_LOOKAHEAD_WEEKS\":  [2, 1],\n",
    "        \"PAST_NEAR_HISTORY_WEEKS\": [2, 1],\n",
    "        \"PAST_OLD_HISTORY_WEEKS\":  [99],\n",
    "        \"FEATURE_COLS\":            [[\"r_\", \"nh_\"]],\n",
    "        \"DOWNSAMPLE\":              [1],\n",
    "        \"MODEL\":                   [{\"rf\": [[10], [4]]}]\n",
    "    }\n",
    "]\n",
    "\n",
    "train_configs_2 = [\n",
    "    {\n",
    "        \"CHURN\":                   [\"canceldown\"],\n",
    "        \"FUTURE_LOOKAHEAD_WEEKS\":  [2],\n",
    "        \"PAST_NEAR_HISTORY_WEEKS\": [1],\n",
    "        \"PAST_OLD_HISTORY_WEEKS\":  [99],\n",
    "        \"FEATURE_COLS\":            [[\"r_\", \"nh_\"]],\n",
    "        \"DOWNSAMPLE\":              [0.25, 0.5, 0.75],\n",
    "        \"MODEL\":                   [{\"rf\": [[10], [4]]}]\n",
    "    }\n",
    "]\n",
    "\n",
    "train_configs_3 = [\n",
    "    {\n",
    "        \"CHURN\":                   [\"canceldown\"],\n",
    "        \"FUTURE_LOOKAHEAD_WEEKS\":  [2],\n",
    "        \"PAST_NEAR_HISTORY_WEEKS\": [1],\n",
    "        \"PAST_OLD_HISTORY_WEEKS\":  [1,2,3,4,5],\n",
    "        \"FEATURE_COLS\":            [[\"r_\", \"nh_\"]],\n",
    "        \"DOWNSAMPLE\":              [1],\n",
    "        \"MODEL\":                   [{\"rf\": [[10], [4]]}]\n",
    "    }\n",
    "]\n",
    "\n",
    "train_configs_4 = [\n",
    "    {\n",
    "        \"CHURN\":                   [\"canceldown\", \"cancel\", \"down\"],\n",
    "        \"FUTURE_LOOKAHEAD_WEEKS\":  [2],\n",
    "        \"PAST_NEAR_HISTORY_WEEKS\": [1],\n",
    "        \"PAST_OLD_HISTORY_WEEKS\":  [1],\n",
    "        \"FEATURE_COLS\":            [[\"r_\", \"nh_\", \"oh_\", \"nhn_\", \"ohn_\", \"d_\"]],\n",
    "        \"DOWNSAMPLE\":              [1],\n",
    "        \"MODEL\":                   [{\"rf\": [[10], [4]]}]\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "train_configs_5 = [\n",
    "    {\n",
    "        \"CHURN\":                   [\"canceldown\"],\n",
    "        \"FUTURE_LOOKAHEAD_WEEKS\":  [2],\n",
    "        \"PAST_NEAR_HISTORY_WEEKS\": [1],\n",
    "        \"PAST_OLD_HISTORY_WEEKS\":  [1],\n",
    "        \"FEATURE_COLS\":            [[\"r_\", \"nh_\"],\n",
    "                                    [\"r_\", \"oh_\"], \n",
    "                                    [\"r_\", \"nhn_\"], \n",
    "                                    [\"r_\", \"ohn_\"], \n",
    "                                    [\"r_\", \"d_\"], \n",
    "                                    [\"nh_\", \"oh_\"], \n",
    "                                    [\"nh_\", \"nhn_\"], \n",
    "                                    [\"nh_\", \"ohn_\"], \n",
    "                                    [\"nh_\", \"d_\"], \n",
    "                                    [\"oh_\", \"nhn_\"], \n",
    "                                    [\"oh_\", \"ohn_\"], \n",
    "                                    [\"oh_\", \"d_\"], \n",
    "                                    [\"nhn_\", \"ohn_\"], \n",
    "                                    [\"nhn_\", \"d_\"], \n",
    "                                    [\"ohn_\", \"d_\"]\n",
    "                                   ],\n",
    "        \"DOWNSAMPLE\":              [1],\n",
    "        \"MODEL\":                   [{\"rf\": [[10], [4]]}]\n",
    "    }\n",
    "]\n",
    "\n",
    "train_configs_6 = [\n",
    "    {\n",
    "        \"CHURN\":                   [\"canceldown\"],\n",
    "        \"FUTURE_LOOKAHEAD_WEEKS\":  [2],\n",
    "        \"PAST_NEAR_HISTORY_WEEKS\": [1],\n",
    "        \"PAST_OLD_HISTORY_WEEKS\":  [1],\n",
    "        \"FEATURE_COLS\":            [[\"r_\"], [\"nh_\"], [\"oh_\"], [\"nhn_\"], [\"ohn_\"], [\"d_\"]],\n",
    "        \"DOWNSAMPLE\":              [1],\n",
    "        \"MODEL\":                   [{\"rf\": [[10], [4]]}]\n",
    "    }\n",
    "]\n",
    "\n",
    "train_configs_7 = [\n",
    "    {\n",
    "        \"CHURN\":                   [\"canceldown\"],\n",
    "        \"FUTURE_LOOKAHEAD_WEEKS\":  [1],\n",
    "        \"PAST_NEAR_HISTORY_WEEKS\": [1],\n",
    "        \"PAST_OLD_HISTORY_WEEKS\":  [4],\n",
    "        \"FEATURE_COLS\":            [[\"nhn_\", \"ohn_\"]],\n",
    "        \"MODEL\":                   [{\"rf\": [[10], [4]]},{\"rf\": [[100,20], [5]]}]\n",
    "    }\n",
    "]\n",
    "\n",
    "train_configs = [\n",
    "    {\n",
    "        \"CHURN\":                   [\"canceldown\"],\n",
    "        \"FUTURE_LOOKAHEAD_WEEKS\":  [2],\n",
    "        \"PAST_NEAR_HISTORY_WEEKS\": [1],\n",
    "        \"PAST_OLD_HISTORY_WEEKS\":  [4],\n",
    "        \"FEATURE_COLS\":            [[\"nhn_\", \"ohn_\"]],\n",
    "        \"MODEL\":                   [{\"dt\": [[5], [32]]},{\"rf\": [[10], [4]]}]\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55ffae24-b569-4e7a-80ea-e10d8304c58e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "canceldown-2-1-4-nhn_ohn_\n",
      "### TRAIN / TEST SPLIT\n",
      "### SAVING TEST DATA s3a://udacity-dsnd/sparkify/output/07-test-4-1-2-canceldown-sparkify_event_data.json\n",
      "orig-label-0: 12768\n",
      "orig-label-1: 2523\n",
      "downsampled label-1 = 2523, label-0 ~ 2522.802412091785\n",
      "### SAVING TRAIND DATA s3a://udacity-dsnd/sparkify/output/07-traind-4-1-2-canceldown-sparkify_event_data.json\n",
      "label 0: 12768, label 1: 2523\n",
      "### SAVING TRAINW DATA s3a://udacity-dsnd/sparkify/output/07-trainw-4-1-2-canceldown-sparkify_event_data.json\n",
      "\n",
      "------------------------------------\n",
      "TRAINING canceldown-2-1-4-nhn_ohn_ dt_5_32\n",
      "------------------------------------\n"
     ]
    },
    {
     "ename": "IllegalArgumentException",
     "evalue": "weight does not exist. Available: userId, wid, label, features",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 47\u001b[0m\n\u001b[1;32m     44\u001b[0m df_trainw \u001b[38;5;241m=\u001b[39m df_trainw\u001b[38;5;241m.\u001b[39munpersist()\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m MODEL \u001b[38;5;129;01min\u001b[39;00m train_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMODEL\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m---> 47\u001b[0m     model, f1, model_name \u001b[38;5;241m=\u001b[39m \u001b[43mhyper_tune\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_trainw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_traind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfigstr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMODEL\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m df_test\u001b[38;5;241m.\u001b[39munpersist()\n\u001b[1;32m     50\u001b[0m df_traind\u001b[38;5;241m.\u001b[39munpersist()\n",
      "Cell \u001b[0;32mIn[8], line 310\u001b[0m, in \u001b[0;36mhyper_tune\u001b[0;34m(df_trainw, df_traind, df_test, configstr, model_config)\u001b[0m\n\u001b[1;32m    308\u001b[0m       model, f1, model_name \u001b[38;5;241m=\u001b[39m hyper_tune_rf(df_trainw, df_test, configstr, \u001b[38;5;241m*\u001b[39m(model_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrf\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdt\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m model_config:\n\u001b[0;32m--> 310\u001b[0m       model, f1, model_name \u001b[38;5;241m=\u001b[39m \u001b[43mhyper_tune_dt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_trainw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfigstr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m model_config:\n\u001b[1;32m    312\u001b[0m       model, f1, model_name \u001b[38;5;241m=\u001b[39m hyper_tune_lr(df_traind, df_test, configstr, \u001b[38;5;241m*\u001b[39m(model_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n",
      "Cell \u001b[0;32mIn[8], line 242\u001b[0m, in \u001b[0;36mhyper_tune_dt\u001b[0;34m(df_train, df_test, configstr, max_depths, max_bins_list)\u001b[0m\n\u001b[1;32m    240\u001b[0m logresult(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m------------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    241\u001b[0m dt \u001b[38;5;241m=\u001b[39m DecisionTreeClassifier(featuresCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m, maxDepth\u001b[38;5;241m=\u001b[39mmax_depth, maxBins\u001b[38;5;241m=\u001b[39mmax_bins, weightCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m\"\u001b[39m, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m--> 242\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mdt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    243\u001b[0m predict_test  \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtransform(df_test)\n\u001b[1;32m    244\u001b[0m accuracy, precision, recall, f1 \u001b[38;5;241m=\u001b[39m confuse(predict_test)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py:383\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[0;32m--> 383\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py:380\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 380\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: weight does not exist. Available: userId, wid, label, features"
     ]
    }
   ],
   "source": [
    "CF = {}\n",
    "for train_config in train_configs:\n",
    " for CHURN in train_config[\"CHURN\"]:\n",
    "  for FUTURE_LOOKAHEAD_WEEKS in train_config[\"FUTURE_LOOKAHEAD_WEEKS\"]:\n",
    "   for PAST_NEAR_HISTORY_WEEKS in train_config[\"PAST_NEAR_HISTORY_WEEKS\"]:\n",
    "    for PAST_OLD_HISTORY_WEEKS in train_config[\"PAST_OLD_HISTORY_WEEKS\"]:\n",
    "     for FEATURE_COLS in train_config[\"FEATURE_COLS\"]:\n",
    "        CF[\"CHURN\"] = CHURN\n",
    "        CF[\"FUTURE_LOOKAHEAD_WEEKS\"] = FUTURE_LOOKAHEAD_WEEKS\n",
    "        CF[\"PAST_NEAR_HISTORY_WEEKS\"] = PAST_NEAR_HISTORY_WEEKS\n",
    "        CF[\"PAST_OLD_HISTORY_WEEKS\"] = PAST_OLD_HISTORY_WEEKS\n",
    "        CF[\"FEATURE_COLS\"] = FEATURE_COLS\n",
    "        CF[\"MODEL_URL\"] = EVENT_DATA_URL.replace(\"/sparkify/\", \"/sparkify/output/07-model-{PAST_OLD_HISTORY_WEEKS}-{PAST_NEAR_HISTORY_WEEKS}-{FUTURE_LOOKAHEAD_WEEKS}-{CHURN}\").replace(\".json\", \"\")\n",
    "        CF[\"TESTTRAIN_DATA_URL\"] = EVENT_DATA_URL.replace(\"/sparkify/\", f\"/sparkify/output/07-testtrain-{PAST_OLD_HISTORY_WEEKS}-{PAST_NEAR_HISTORY_WEEKS}-{FUTURE_LOOKAHEAD_WEEKS}-{CHURN}-\")\n",
    "        prefixes = str(FEATURE_COLS).replace(\"[\",\"\").replace(\"]\",\"\").replace(\"'\",\"\").replace(\",\",\"\").replace(\" \",\"\")\n",
    "        configstr = f\"{CHURN}-{FUTURE_LOOKAHEAD_WEEKS}-{PAST_NEAR_HISTORY_WEEKS}-{PAST_OLD_HISTORY_WEEKS}-{prefixes}\"\n",
    "        print(f\"{configstr}\")\n",
    "        df_testtrain = create_train_test_data(CF)\n",
    "\n",
    "        print(f\"### TRAIN / TEST SPLIT\")\n",
    "        df_train_feat, df_test_feat = df_testtrain.randomSplit([0.7, 0.3], seed=42)\n",
    "        \n",
    "        test_url = CF[\"TESTTRAIN_DATA_URL\"].replace(\"-testtrain-\", \"-test-\")\n",
    "        traind_url = CF[\"TESTTRAIN_DATA_URL\"].replace(\"-testtrain-\", \"-traind-\")\n",
    "        trainw_url = CF[\"TESTTRAIN_DATA_URL\"].replace(\"-testtrain-\", \"-trainw-\")\n",
    "\n",
    "        print(f\"### SAVING TEST DATA {test_url}\")\n",
    "        df_test_feat.write.format('json').mode('overwrite').save(test_url)\n",
    "\n",
    "        df_traind_feat = downsample(df_train_feat, 1)\n",
    "        print(f\"### SAVING TRAIND DATA {traind_url}\")\n",
    "        df_traind_feat.write.format('json').mode('overwrite').save(traind_url)\n",
    "\n",
    "        df_trainw_feat = add_weight_col(df_train_feat)\n",
    "        print(f\"### SAVING TRAINW DATA {trainw_url}\")\n",
    "        df_trainw_feat.write.format('json').mode('overwrite').save(trainw_url)\n",
    "\n",
    "        df_test = create_train_test_vector(CF, df_test_feat)\n",
    "        df_traind = create_train_test_vector(CF, df_traind_feat)\n",
    "        df_trainw = create_train_test_vector(CF, df_trainw_feat)\n",
    "\n",
    "        df_test = df_test.unpersist()\n",
    "        df_traind = df_traind.unpersist()\n",
    "        df_trainw = df_trainw.unpersist()\n",
    "        \n",
    "        for MODEL in train_config[\"MODEL\"]:\n",
    "            model, f1, model_name = hyper_tune(df_trainw, df_traind, df_test, configstr, MODEL)\n",
    "        \n",
    "        df_test.unpersist()\n",
    "        df_traind.unpersist()\n",
    "        df_trainw.unpersist()\n",
    "        \n",
    "print(\"FINISHED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f948a898-cd63-4fc3-8b39-5fe3987b1eea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "canceldown-2-1-4-nhn_ohn_\n",
      "### LOADING TEST DATA s3a://udacity-dsnd/sparkify/output/07-test-4-1-2-canceldown-sparkify_event_data.json\n",
      "### LOADING TRAIND DATA s3a://udacity-dsnd/sparkify/output/07-traind-4-1-2-canceldown-sparkify_event_data.json\n",
      "### LOADING TRAINW DATA s3a://udacity-dsnd/sparkify/output/07-trainw-4-1-2-canceldown-sparkify_event_data.json\n",
      "\n",
      "------------------------------------\n",
      "TRAINING canceldown-2-1-4-nhn_ohn_ dt_13_1024\n",
      "------------------------------------\n",
      "                  \n",
      " Confusion Matrix: \n",
      "                  \n",
      "     | prediction| \n",
      "     |   0 |  1  | \n",
      " ----+-----+-----+ \n",
      " l 0 | 3869| 1614| \n",
      " b --+-----+-----+ \n",
      " l 1 |  319|  679| \n",
      " ----+-----+-----+ \n",
      "                   \n",
      "  accuraccy: 0.7017435580928869\n",
      "  precision: 0.296118621892717\n",
      "  recall:    0.6803607214428857\n",
      "  f1:        0.4126405347918566\n",
      "  mcc:       0.2913749396892743\n",
      "  dt_13_1024: f1 0.4126405347918566\n",
      "new best f1\n",
      "\n",
      "------------------------------------\n",
      "TRAINING canceldown-2-1-4-nhn_ohn_ dt_13_2048\n",
      "------------------------------------\n",
      "                  \n",
      " Confusion Matrix: \n",
      "                  \n",
      "     | prediction| \n",
      "     |   0 |  1  | \n",
      " ----+-----+-----+ \n",
      " l 0 | 3740| 1743| \n",
      " b --+-----+-----+ \n",
      " l 1 |  303|  695| \n",
      " ----+-----+-----+ \n",
      "                   \n",
      "  accuraccy: 0.6843079771640179\n",
      "  precision: 0.28506972928630026\n",
      "  recall:    0.6963927855711423\n",
      "  f1:        0.40454016298020956\n",
      "  mcc:       0.28201541119141693\n",
      "  dt_13_2048: f1 0.40454016298020956\n",
      "\n",
      "------------------------------------\n",
      "TRAINING canceldown-2-1-4-nhn_ohn_ dt_14_1024\n",
      "------------------------------------\n",
      "                  \n",
      " Confusion Matrix: \n",
      "                  \n",
      "     | prediction| \n",
      "     |   0 |  1  | \n",
      " ----+-----+-----+ \n",
      " l 0 | 4014| 1469| \n",
      " b --+-----+-----+ \n",
      " l 1 |  331|  667| \n",
      " ----+-----+-----+ \n",
      "                   \n",
      "  accuraccy: 0.7222650825489894\n",
      "  precision: 0.31226591760299627\n",
      "  recall:    0.6683366733466933\n",
      "  f1:        0.42565411614550097\n",
      "  mcc:       0.30746245635511305\n",
      "  dt_14_1024: f1 0.42565411614550097\n",
      "new best f1\n",
      "\n",
      "------------------------------------\n",
      "TRAINING canceldown-2-1-4-nhn_ohn_ dt_14_2048\n",
      "------------------------------------\n",
      "                  \n",
      " Confusion Matrix: \n",
      "                  \n",
      "     | prediction| \n",
      "     |   0 |  1  | \n",
      " ----+-----+-----+ \n",
      " l 0 | 4046| 1437| \n",
      " b --+-----+-----+ \n",
      " l 1 |  338|  660| \n",
      " ----+-----+-----+ \n",
      "                   \n",
      "  accuraccy: 0.7261225119580311\n",
      "  precision: 0.3147353361945637\n",
      "  recall:    0.6613226452905812\n",
      "  f1:        0.42649434571890144\n",
      "  mcc:       0.30801635891685175\n",
      "  dt_14_2048: f1 0.42649434571890144\n",
      "new best f1\n",
      "\n",
      "------------------------------------\n",
      "TRAINING canceldown-2-1-4-nhn_ohn_ dt_15_1024\n",
      "------------------------------------\n",
      "                  \n",
      " Confusion Matrix: \n",
      "                  \n",
      "     | prediction| \n",
      "     |   0 |  1  | \n",
      " ----+-----+-----+ \n",
      " l 0 | 4139| 1344| \n",
      " b --+-----+-----+ \n",
      " l 1 |  344|  654| \n",
      " ----+-----+-----+ \n",
      "                   \n",
      "  accuraccy: 0.7395463663014967\n",
      "  precision: 0.32732732732732733\n",
      "  recall:    0.655310621242485\n",
      "  f1:        0.43658210947930576\n",
      "  mcc:       0.3206096935938597\n",
      "  dt_15_1024: f1 0.43658210947930576\n",
      "new best f1\n",
      "\n",
      "------------------------------------\n",
      "TRAINING canceldown-2-1-4-nhn_ohn_ dt_15_2048\n",
      "------------------------------------\n",
      "                  \n",
      " Confusion Matrix: \n",
      "                  \n",
      "     | prediction| \n",
      "     |   0 |  1  | \n",
      " ----+-----+-----+ \n",
      " l 0 | 4154| 1329| \n",
      " b --+-----+-----+ \n",
      " l 1 |  357|  641| \n",
      " ----+-----+-----+ \n",
      "                   \n",
      "  accuraccy: 0.7398549606542201\n",
      "  precision: 0.3253807106598985\n",
      "  recall:    0.6422845691382766\n",
      "  f1:        0.43194070080862534\n",
      "  mcc:       0.3138016335482743\n",
      "  dt_15_2048: f1 0.43194070080862534\n",
      "best f1 0.43194070080862534 for dt_15_1024\n",
      "FINISHED\n"
     ]
    }
   ],
   "source": [
    "train_configs = [\n",
    "    {\n",
    "        \"CHURN\":                   [\"canceldown\"],\n",
    "        \"FUTURE_LOOKAHEAD_WEEKS\":  [2],\n",
    "        \"PAST_NEAR_HISTORY_WEEKS\": [1],\n",
    "        \"PAST_OLD_HISTORY_WEEKS\":  [4],\n",
    "        \"FEATURE_COLS\":            [[\"nhn_\", \"ohn_\"]],\n",
    "        \"MODEL\":                   [{\"dt\": [[13,14,15], [1024,2048]]}]\n",
    "    }\n",
    "]\n",
    "\n",
    "CF = {}\n",
    "for train_config in train_configs:\n",
    " for CHURN in train_config[\"CHURN\"]:\n",
    "  for FUTURE_LOOKAHEAD_WEEKS in train_config[\"FUTURE_LOOKAHEAD_WEEKS\"]:\n",
    "   for PAST_NEAR_HISTORY_WEEKS in train_config[\"PAST_NEAR_HISTORY_WEEKS\"]:\n",
    "    for PAST_OLD_HISTORY_WEEKS in train_config[\"PAST_OLD_HISTORY_WEEKS\"]:\n",
    "     for FEATURE_COLS in train_config[\"FEATURE_COLS\"]:\n",
    "        CF[\"CHURN\"] = CHURN\n",
    "        CF[\"FUTURE_LOOKAHEAD_WEEKS\"] = FUTURE_LOOKAHEAD_WEEKS\n",
    "        CF[\"PAST_NEAR_HISTORY_WEEKS\"] = PAST_NEAR_HISTORY_WEEKS\n",
    "        CF[\"PAST_OLD_HISTORY_WEEKS\"] = PAST_OLD_HISTORY_WEEKS\n",
    "        CF[\"FEATURE_COLS\"] = FEATURE_COLS\n",
    "        CF[\"MODEL_URL\"] = EVENT_DATA_URL.replace(\"/sparkify/\", \"/sparkify/output/07-model-{PAST_OLD_HISTORY_WEEKS}-{PAST_NEAR_HISTORY_WEEKS}-{FUTURE_LOOKAHEAD_WEEKS}-{CHURN}\").replace(\".json\", \"\")\n",
    "        CF[\"TESTTRAIN_DATA_URL\"] = EVENT_DATA_URL.replace(\"/sparkify/\", f\"/sparkify/output/07-testtrain-{PAST_OLD_HISTORY_WEEKS}-{PAST_NEAR_HISTORY_WEEKS}-{FUTURE_LOOKAHEAD_WEEKS}-{CHURN}-\")\n",
    "        prefixes = str(FEATURE_COLS).replace(\"[\",\"\").replace(\"]\",\"\").replace(\"'\",\"\").replace(\",\",\"\").replace(\" \",\"\")\n",
    "        configstr = f\"{CHURN}-{FUTURE_LOOKAHEAD_WEEKS}-{PAST_NEAR_HISTORY_WEEKS}-{PAST_OLD_HISTORY_WEEKS}-{prefixes}\"\n",
    "        print(f\"{configstr}\")\n",
    "\n",
    "        test_url = CF[\"TESTTRAIN_DATA_URL\"].replace(\"-testtrain-\", \"-test-\")\n",
    "        trainw_url = CF[\"TESTTRAIN_DATA_URL\"].replace(\"-testtrain-\", \"-trainw-\")\n",
    "        traind_url = CF[\"TESTTRAIN_DATA_URL\"].replace(\"-testtrain-\", \"-traind-\")\n",
    "\n",
    "        print(f\"### LOADING TEST DATA {test_url}\")\n",
    "        df_test_feat = spark.read.json(test_url)\n",
    "\n",
    "        print(f\"### LOADING TRAIND DATA {traind_url}\")\n",
    "        df_traind_feat = spark.read.json(traind_url)\n",
    "\n",
    "        print(f\"### LOADING TRAINW DATA {trainw_url}\")\n",
    "        df_trainw_feat = spark.read.json(trainw_url)\n",
    "        \n",
    "        df_test = create_train_test_vector(CF, df_test_feat)\n",
    "        df_traind = create_train_test_vector(CF, df_traind_feat)\n",
    "        df_trainw = create_train_test_vector(CF, df_trainw_feat)\n",
    "\n",
    "        df_test = df_test.unpersist()\n",
    "        df_traind = df_traind.unpersist()\n",
    "        df_trainw = df_trainw.unpersist()\n",
    "\n",
    "        for MODEL in train_config[\"MODEL\"]:\n",
    "            model, f1, model_name = hyper_tune(df_trainw, df_traind, df_test, configstr, MODEL)\n",
    "        \n",
    "        df_test.unpersist()\n",
    "        df_traind.unpersist()\n",
    "        df_trainw.unpersist()\n",
    "        \n",
    "print(\"FINISHED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "55c49705-266a-4ec3-a5b1-5291da11ac42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "url = \"s3a://udacity-dsnd/sparkify/output/07-test-4-1-2-canceldown-sparkify_event_data.json\"\n",
    "df_x = spark.read.json(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "60624053-b527-4ec6-a149-0b903ac3e9bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- d_num_events: double (nullable = true)\n",
      " |-- d_pg_about: double (nullable = true)\n",
      " |-- d_pg_add_friend: double (nullable = true)\n",
      " |-- d_pg_add_to_playlist: double (nullable = true)\n",
      " |-- d_pg_cancel: double (nullable = true)\n",
      " |-- d_pg_cancellation_confirmation: double (nullable = true)\n",
      " |-- d_pg_downgrade: double (nullable = true)\n",
      " |-- d_pg_error: double (nullable = true)\n",
      " |-- d_pg_help: double (nullable = true)\n",
      " |-- d_pg_home: double (nullable = true)\n",
      " |-- d_pg_login: double (nullable = true)\n",
      " |-- d_pg_logout: double (nullable = true)\n",
      " |-- d_pg_nextsong: double (nullable = true)\n",
      " |-- d_pg_register: double (nullable = true)\n",
      " |-- d_pg_roll_advert: double (nullable = true)\n",
      " |-- d_pg_save_settings: double (nullable = true)\n",
      " |-- d_pg_settings: double (nullable = true)\n",
      " |-- d_pg_submit_downgrade: double (nullable = true)\n",
      " |-- d_pg_submit_registration: double (nullable = true)\n",
      " |-- d_pg_submit_upgrade: double (nullable = true)\n",
      " |-- d_pg_thumbs_down: double (nullable = true)\n",
      " |-- d_pg_thumbs_up: double (nullable = true)\n",
      " |-- d_pg_upgrade: double (nullable = true)\n",
      " |-- d_session_hours: double (nullable = true)\n",
      " |-- d_session_start: double (nullable = true)\n",
      " |-- d_status_307: double (nullable = true)\n",
      " |-- d_status_404: double (nullable = true)\n",
      " |-- label: long (nullable = true)\n",
      " |-- nh_num_events: long (nullable = true)\n",
      " |-- nh_pg_about: long (nullable = true)\n",
      " |-- nh_pg_add_friend: long (nullable = true)\n",
      " |-- nh_pg_add_to_playlist: long (nullable = true)\n",
      " |-- nh_pg_cancel: long (nullable = true)\n",
      " |-- nh_pg_cancellation_confirmation: long (nullable = true)\n",
      " |-- nh_pg_downgrade: long (nullable = true)\n",
      " |-- nh_pg_error: long (nullable = true)\n",
      " |-- nh_pg_help: long (nullable = true)\n",
      " |-- nh_pg_home: long (nullable = true)\n",
      " |-- nh_pg_login: long (nullable = true)\n",
      " |-- nh_pg_logout: long (nullable = true)\n",
      " |-- nh_pg_nextsong: long (nullable = true)\n",
      " |-- nh_pg_register: long (nullable = true)\n",
      " |-- nh_pg_roll_advert: long (nullable = true)\n",
      " |-- nh_pg_save_settings: long (nullable = true)\n",
      " |-- nh_pg_settings: long (nullable = true)\n",
      " |-- nh_pg_submit_downgrade: long (nullable = true)\n",
      " |-- nh_pg_submit_registration: long (nullable = true)\n",
      " |-- nh_pg_submit_upgrade: long (nullable = true)\n",
      " |-- nh_pg_thumbs_down: long (nullable = true)\n",
      " |-- nh_pg_thumbs_up: long (nullable = true)\n",
      " |-- nh_pg_upgrade: long (nullable = true)\n",
      " |-- nh_session_hours: double (nullable = true)\n",
      " |-- nh_session_start: long (nullable = true)\n",
      " |-- nh_status_307: long (nullable = true)\n",
      " |-- nh_status_404: long (nullable = true)\n",
      " |-- nhn_num_events: double (nullable = true)\n",
      " |-- nhn_pg_about: double (nullable = true)\n",
      " |-- nhn_pg_add_friend: double (nullable = true)\n",
      " |-- nhn_pg_add_to_playlist: double (nullable = true)\n",
      " |-- nhn_pg_cancel: double (nullable = true)\n",
      " |-- nhn_pg_cancellation_confirmation: double (nullable = true)\n",
      " |-- nhn_pg_downgrade: double (nullable = true)\n",
      " |-- nhn_pg_error: double (nullable = true)\n",
      " |-- nhn_pg_help: double (nullable = true)\n",
      " |-- nhn_pg_home: double (nullable = true)\n",
      " |-- nhn_pg_login: double (nullable = true)\n",
      " |-- nhn_pg_logout: double (nullable = true)\n",
      " |-- nhn_pg_nextsong: double (nullable = true)\n",
      " |-- nhn_pg_register: double (nullable = true)\n",
      " |-- nhn_pg_roll_advert: double (nullable = true)\n",
      " |-- nhn_pg_save_settings: double (nullable = true)\n",
      " |-- nhn_pg_settings: double (nullable = true)\n",
      " |-- nhn_pg_submit_downgrade: double (nullable = true)\n",
      " |-- nhn_pg_submit_registration: double (nullable = true)\n",
      " |-- nhn_pg_submit_upgrade: double (nullable = true)\n",
      " |-- nhn_pg_thumbs_down: double (nullable = true)\n",
      " |-- nhn_pg_thumbs_up: double (nullable = true)\n",
      " |-- nhn_pg_upgrade: double (nullable = true)\n",
      " |-- nhn_session_hours: double (nullable = true)\n",
      " |-- nhn_session_start: double (nullable = true)\n",
      " |-- nhn_status_307: double (nullable = true)\n",
      " |-- nhn_status_404: double (nullable = true)\n",
      " |-- oh_num_events: long (nullable = true)\n",
      " |-- oh_pg_about: long (nullable = true)\n",
      " |-- oh_pg_add_friend: long (nullable = true)\n",
      " |-- oh_pg_add_to_playlist: long (nullable = true)\n",
      " |-- oh_pg_cancel: long (nullable = true)\n",
      " |-- oh_pg_cancellation_confirmation: long (nullable = true)\n",
      " |-- oh_pg_downgrade: long (nullable = true)\n",
      " |-- oh_pg_error: long (nullable = true)\n",
      " |-- oh_pg_help: long (nullable = true)\n",
      " |-- oh_pg_home: long (nullable = true)\n",
      " |-- oh_pg_login: long (nullable = true)\n",
      " |-- oh_pg_logout: long (nullable = true)\n",
      " |-- oh_pg_nextsong: long (nullable = true)\n",
      " |-- oh_pg_register: long (nullable = true)\n",
      " |-- oh_pg_roll_advert: long (nullable = true)\n",
      " |-- oh_pg_save_settings: long (nullable = true)\n",
      " |-- oh_pg_settings: long (nullable = true)\n",
      " |-- oh_pg_submit_downgrade: long (nullable = true)\n",
      " |-- oh_pg_submit_registration: long (nullable = true)\n",
      " |-- oh_pg_submit_upgrade: long (nullable = true)\n",
      " |-- oh_pg_thumbs_down: long (nullable = true)\n",
      " |-- oh_pg_thumbs_up: long (nullable = true)\n",
      " |-- oh_pg_upgrade: long (nullable = true)\n",
      " |-- oh_session_hours: double (nullable = true)\n",
      " |-- oh_session_start: long (nullable = true)\n",
      " |-- oh_status_307: long (nullable = true)\n",
      " |-- oh_status_404: long (nullable = true)\n",
      " |-- ohn_num_events: double (nullable = true)\n",
      " |-- ohn_pg_about: double (nullable = true)\n",
      " |-- ohn_pg_add_friend: double (nullable = true)\n",
      " |-- ohn_pg_add_to_playlist: double (nullable = true)\n",
      " |-- ohn_pg_cancel: double (nullable = true)\n",
      " |-- ohn_pg_cancellation_confirmation: double (nullable = true)\n",
      " |-- ohn_pg_downgrade: double (nullable = true)\n",
      " |-- ohn_pg_error: double (nullable = true)\n",
      " |-- ohn_pg_help: double (nullable = true)\n",
      " |-- ohn_pg_home: double (nullable = true)\n",
      " |-- ohn_pg_login: double (nullable = true)\n",
      " |-- ohn_pg_logout: double (nullable = true)\n",
      " |-- ohn_pg_nextsong: double (nullable = true)\n",
      " |-- ohn_pg_register: double (nullable = true)\n",
      " |-- ohn_pg_roll_advert: double (nullable = true)\n",
      " |-- ohn_pg_save_settings: double (nullable = true)\n",
      " |-- ohn_pg_settings: double (nullable = true)\n",
      " |-- ohn_pg_submit_downgrade: double (nullable = true)\n",
      " |-- ohn_pg_submit_registration: double (nullable = true)\n",
      " |-- ohn_pg_submit_upgrade: double (nullable = true)\n",
      " |-- ohn_pg_thumbs_down: double (nullable = true)\n",
      " |-- ohn_pg_thumbs_up: double (nullable = true)\n",
      " |-- ohn_pg_upgrade: double (nullable = true)\n",
      " |-- ohn_session_hours: double (nullable = true)\n",
      " |-- ohn_session_start: double (nullable = true)\n",
      " |-- ohn_status_307: double (nullable = true)\n",
      " |-- ohn_status_404: double (nullable = true)\n",
      " |-- paid: long (nullable = true)\n",
      " |-- r_num_events: double (nullable = true)\n",
      " |-- r_pg_about: double (nullable = true)\n",
      " |-- r_pg_add_friend: double (nullable = true)\n",
      " |-- r_pg_add_to_playlist: double (nullable = true)\n",
      " |-- r_pg_cancel: double (nullable = true)\n",
      " |-- r_pg_cancellation_confirmation: double (nullable = true)\n",
      " |-- r_pg_downgrade: double (nullable = true)\n",
      " |-- r_pg_error: double (nullable = true)\n",
      " |-- r_pg_help: double (nullable = true)\n",
      " |-- r_pg_home: double (nullable = true)\n",
      " |-- r_pg_login: double (nullable = true)\n",
      " |-- r_pg_logout: double (nullable = true)\n",
      " |-- r_pg_nextsong: double (nullable = true)\n",
      " |-- r_pg_register: double (nullable = true)\n",
      " |-- r_pg_roll_advert: double (nullable = true)\n",
      " |-- r_pg_save_settings: double (nullable = true)\n",
      " |-- r_pg_settings: double (nullable = true)\n",
      " |-- r_pg_submit_downgrade: double (nullable = true)\n",
      " |-- r_pg_submit_registration: double (nullable = true)\n",
      " |-- r_pg_submit_upgrade: double (nullable = true)\n",
      " |-- r_pg_thumbs_down: double (nullable = true)\n",
      " |-- r_pg_thumbs_up: double (nullable = true)\n",
      " |-- r_pg_upgrade: double (nullable = true)\n",
      " |-- r_session_hours: double (nullable = true)\n",
      " |-- r_session_start: double (nullable = true)\n",
      " |-- r_status_307: double (nullable = true)\n",
      " |-- r_status_404: double (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      " |-- usermale: long (nullable = true)\n",
      " |-- userregistration: double (nullable = true)\n",
      " |-- wid: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_x.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "704d88a9-f996-467e-b77e-98b933c42048",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CF = {\n",
    "        \"CHURN\":                   \"canceldown\",\n",
    "        \"FUTURE_LOOKAHEAD_WEEKS\":  2,\n",
    "        \"PAST_NEAR_HISTORY_WEEKS\": 1,\n",
    "        \"PAST_OLD_HISTORY_WEEKS\":  4,\n",
    "        \"FEATURE_COLS\":            [\"nhn_\", \"ohn_\"],\n",
    "        \"DOWNSAMPLE\":              1,\n",
    "        \"MODEL\":                   {\"rf\": [[10], [4]]}\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f58e8b4-df25-420d-90b4-38d1d7fc9a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_testtrain_vec = create_train_test_data(CF)\n",
    "df_train, df_test = df_testtrain_vec.randomSplit([0.7, 0.3], seed=42)\n",
    "prefixes = str(CF[\"FEATURE_COLS\"]).replace(\"[\",\"\").replace(\"]\",\"\").replace(\"'\",\"\").replace(\",\",\"\").replace(\" \",\"\")\n",
    "configstr = f'{CF[\"CHURN\"]}-{CF[\"FUTURE_LOOKAHEAD_WEEKS\"]}-{CF[\"PAST_NEAR_HISTORY_WEEKS\"]}-{CF[\"PAST_OLD_HISTORY_WEEKS\"]}-{prefixes}-{CF[\"DOWNSAMPLE\"]}-wc'\n",
    "print(f\"{configstr}\")\n",
    "df_train = add_weight_col(df_train)\n",
    "df_train = df_train.persist()\n",
    "CF[\"MODEL\"] = {\"rf\": [[100], [5]]}\n",
    "model, f1, model_name = hyper_tune_rf(configstr, *CF[\"MODEL\"][\"rf\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "abc9daf0-398c-42ea-8346-93b1e0cb3880",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "canceldown-2-1-4-nhn_ohn_-1-wc\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bda491a1-d6de-46c9-a6a8-678afa9c6e4b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label 0: 25132, label 1: 4431\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a8e16bcb-3ced-461e-80cf-9f39524beadd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cf2ac6fd-d93d-4ddd-976b-f7fdd571e52a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------\n",
      "TRAINING canceldown-2-1-4-nhn_ohn_-1-wc rf_100_5\n",
      "------------------------------------\n",
      "                  \n",
      " Confusion Matrix: \n",
      "                  \n",
      "     | prediction| \n",
      "     |   0 |  1  | \n",
      " ----+-----+-----+ \n",
      " l 0 | 7179| 3755| \n",
      " b --+-----+-----+ \n",
      " l 1 |  673| 1197| \n",
      " ----+-----+-----+ \n",
      "                   \n",
      "  accuraccy: 0.6541705716963448\n",
      "  precision: 0.2417205169628433\n",
      "  recall:    0.6401069518716578\n",
      "  f1:        0.35092348284960423\n",
      "  mcc:       0.21514051325250577\n",
      "  rf_100_5: f1 0.35092348284960423\n",
      "best f1 0.35092348284960423 for rf_100_5\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f43d3574-6856-4370-b6a5-c5987cdf2bfb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+-----+--------------------+------------------+\n",
      "| userId|wid|label|            features|            weight|\n",
      "+-------+---+-----+--------------------+------------------+\n",
      "|1030587|  1|    0|(57,[0,2,3,5,6,9,...|0.8501166999289652|\n",
      "|1033297|  1|    0|(57,[1,2,3,5,6,11...|0.8501166999289652|\n",
      "|1069552|  1|    0|(57,[2,3,15,17,24...|0.8501166999289652|\n",
      "|1083324|  1|    0|(57,[1,2,3,5,6,12...|0.8501166999289652|\n",
      "|1102913|  1|    0|(57,[0,1,2,3,5,6,...|0.8501166999289652|\n",
      "+-------+---+-----+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01eb2bf7-397e-4f8e-a2d2-5024cbe21d31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152a5c3c-df07-4618-a6af-755da70fb8c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3c4010-4a33-44c3-b37b-04320034674a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b2c23e48-4b7f-4af6-b41d-73c3f403b896",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### SAVING TESTTRAIN DATA s3a://udacity-dsnd/sparkify/output/06-testtrain-sparkify_event_data-churncanceldown.json\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "# -----------------\n",
    "\n",
    "ttd_url = TESTTRAIN_DATA_URL.replace(\".json\",f\"-churn{CHURN}.json\")\n",
    "print(f\"### SAVING TESTTRAIN DATA {ttd_url}\")\n",
    "df_testtrain.write.format('json').mode('overwrite').save(ttd_url)\n",
    "print(f\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c26b914a-60cf-46c0-ac62-2203143bed51",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### LOAD TESTTRAIN DATA s3a://udacity-dsnd/sparkify/output/06-testtrain-sparkify_event_data-churncanceldown.json\n",
      "### PERSIST\n"
     ]
    }
   ],
   "source": [
    "# -----------------\n",
    "\n",
    "ttd_url = TESTTRAIN_DATA_URL.replace(\".json\",f\"-churn{CHURN}.json\")\n",
    "print(f\"### LOAD TESTTRAIN DATA {ttd_url}\")\n",
    "df_testtrain = spark.read.json(ttd_url)\n",
    "print(f\"### PERSIST\")\n",
    "df_testtrain_persist = df_testtrain.persist()\n",
    "df_testtrain = df_testtrain_persist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b5eb1a69-82fa-4190-b5b0-ad959dce6619",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_testtrain_orig = df_testtrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f4cb27d2-abea-4795-be45-0578bba434e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df_testtrain = oversample(df_testtrain)\n",
    "#df_testtrain = downsample(df_testtrain, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f3d4de6c-9882-44d0-9076-11f96fcaf2d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### CREATE FEATURE COLUMN\n",
      "### TRAIN / TEST SPLIT\n",
      "orig-label-0: 23793\n",
      "orig-label-1: 2644\n",
      "downsampled label-1 = 2644, label-0 ~ 5287.555536877495\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(f\"### CREATE FEATURE COLUMN\")\n",
    "\n",
    "featureCols = [\"paid\", \"usermale\", \"userregistration\"]    \n",
    "\n",
    "featureCols = [*featureCols, *[col for col in df_testtrain.columns if col.startswith(\"r_\")]]\n",
    "featureCols = [*featureCols, *[col for col in df_testtrain.columns if col.startswith(\"nh_\")]]\n",
    "\n",
    "#featureCols = [col for col in df_testtrain.columns if not col in [\"userId\", \"wid\", \"label\"]]\n",
    "assembler = VectorAssembler(inputCols=featureCols, outputCol=\"features\")\n",
    "df_testtrain_vec=assembler.transform(df_testtrain).select(\"userId\", \"wid\", \"label\",\"features\")\n",
    "\n",
    "\n",
    "df_testtrain_vec_persist = df_testtrain_vec.persist()\n",
    "df_testtrain_vec = df_testtrain_vec_persist\n",
    "\n",
    "# -----------------\n",
    "\n",
    "print(f\"### TRAIN / TEST SPLIT\")\n",
    "df_train, df_test = df_testtrain_vec.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "#print(f\"train: {df_train.count()}\")\n",
    "#print(f\"  l1: {df_train.where(df_train.label==1).count()}\")\n",
    "#print(f\"  l0: {df_train.where(df_train.label==0).count()}\")\n",
    "#print(f\"test: {df_test.count()}\")\n",
    "#print(f\"  l1: {df_test.where(df_test.label==1).count()}\")\n",
    "#print(f\"  l0: {df_test.where(df_test.label==0).count()}\")\n",
    "\n",
    "## Fit scaler to train dataset\n",
    "#scaler = MaxAbsScaler().setInputCol('features').setOutputCol('scaled_features')\n",
    "#df_train = df_train.drop(\"scaled_features\")\n",
    "#scaler_model = scaler.fit(df_train)\n",
    "## Scale train and test features\n",
    "#df_train = scaler_model.transform(df_train)\n",
    "#df_test = df_test.drop(\"scaled_features\")\n",
    "#df_test = scaler_model.transform(df_test)\n",
    "\n",
    "# -----------------\n",
    "df_test_orig = df_test\n",
    "df_train_orig = df_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "21448c99-cb4f-4d10-96ef-4631baa13554",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#df_train = oversample(df_train, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c523e445-7fe4-4f7f-b091-b0f496edc929",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 5173\n",
      "  l1: 2585\n",
      "  l0: 2588\n",
      "test: 2274\n",
      "  l1: 1176\n",
      "  l0: 1098\n"
     ]
    }
   ],
   "source": [
    "print(f\"train: {df_train.count()}\")\n",
    "print(f\"  l1: {df_train.where(df_train.label==1).count()}\")\n",
    "print(f\"  l0: {df_train.where(df_train.label==0).count()}\")\n",
    "print(f\"test: {df_test.count()}\")\n",
    "print(f\"  l1: {df_test.where(df_test.label==1).count()}\")\n",
    "print(f\"  l0: {df_test.where(df_test.label==0).count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6ed1fc9a-ef90-482b-983b-67361ce2e193",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oversampling to: 1400/10233\n",
      "oversampling to: 2100/10233\n",
      "oversampling to: 2800/10233\n",
      "oversampling to: 3500/10233\n",
      "oversampling to: 4200/10233\n",
      "oversampling to: 4900/10233\n",
      "oversampling to: 5600/10233\n",
      "oversampling to: 6300/10233\n",
      "oversampling to: 7000/10233\n",
      "oversampling to: 7700/10233\n",
      "oversampling to: 8400/10233\n",
      "oversampling to: 9100/10233\n",
      "oversampling to: 9800/10233\n",
      "oversampling to: 10500/10233\n"
     ]
    }
   ],
   "source": [
    "df_test = oversample(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be7e432a-4c0a-48fc-ab2f-c7a932cf4b80",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oversampling to: 2230/25009\n",
      "oversampling to: 3345/25009\n",
      "oversampling to: 4460/25009\n",
      "oversampling to: 5575/25009\n",
      "oversampling to: 6690/25009\n",
      "oversampling to: 7805/25009\n",
      "oversampling to: 8920/25009\n",
      "oversampling to: 10035/25009\n",
      "oversampling to: 11150/25009\n",
      "oversampling to: 12265/25009\n",
      "oversampling to: 13380/25009\n",
      "oversampling to: 14495/25009\n",
      "oversampling to: 15610/25009\n",
      "oversampling to: 16725/25009\n",
      "oversampling to: 17840/25009\n",
      "oversampling to: 18955/25009\n",
      "oversampling to: 20070/25009\n",
      "oversampling to: 21185/25009\n",
      "oversampling to: 22300/25009\n",
      "oversampling to: 23415/25009\n",
      "oversampling to: 24530/25009\n",
      "oversampling to: 25645/25009\n"
     ]
    }
   ],
   "source": [
    "df_train_orig = df_train\n",
    "df_train = oversample(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98acd19d-53e1-4c78-a732-d8984c479011",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model, f1, model_name = hyper_tune_rf([20], [5])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43218ff7-bbfc-4f7b-9aa5-e48fa90e7f77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model, f1, model_name = hyper_tune_lr([100], [0], [0])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "19ed8fda-31d2-473f-8359-b11544e5a18d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "| userId|count|\n",
      "+-------+-----+\n",
      "|1554956|    4|\n",
      "|1339528|    4|\n",
      "|1538485|    3|\n",
      "|1125943|    3|\n",
      "|1831733|    3|\n",
      "|1586895|    3|\n",
      "|1392770|    3|\n",
      "|1373602|    3|\n",
      "|1178026|    3|\n",
      "|1602181|    3|\n",
      "|1390064|    3|\n",
      "|1591353|    3|\n",
      "|1812177|    3|\n",
      "|1141231|    3|\n",
      "|1888253|    3|\n",
      "|1116029|    3|\n",
      "|1386578|    3|\n",
      "|1558736|    3|\n",
      "|1996408|    3|\n",
      "|1037209|    3|\n",
      "+-------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train.groupBy(\"userId\").count().sort(F.desc(F.col(\"count\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "242cdc8f-9cac-410f-a091-034052a32078",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.where(F.col(\"userId\")==\"1655208\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "63cd2a28-73fb-4545-8a47-154a6fa60e23",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### PREDICT TRAIN\n",
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       1.0|\n",
      "|    0|       1.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "### PREDICT TEST\n",
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "|    0|       1.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       1.0|\n",
      "|    0|       1.0|\n",
      "|    0|       1.0|\n",
      "|    0|       1.0|\n",
      "|    0|       0.0|\n",
      "|    0|       1.0|\n",
      "|    0|       0.0|\n",
      "+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "### EVALUATE PREDICTION\n",
      "+-----+--------------------+----------+--------------------+\n",
      "|label|       rawPrediction|prediction|         probability|\n",
      "+-----+--------------------+----------+--------------------+\n",
      "|    0|[8.59056034958328...|       1.0|[0.42952801747916...|\n",
      "|    0|[12.1124409849263...|       0.0|[0.60562204924631...|\n",
      "|    0|[13.7858852326191...|       0.0|[0.68929426163095...|\n",
      "|    0|[8.29733438713462...|       1.0|[0.41486671935673...|\n",
      "|    0|[9.09423883661818...|       1.0|[0.45471194183090...|\n",
      "+-----+--------------------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "The area under ROC for train set is 0.7060216921425774\n",
      "The area under ROC for test set is 0.6850268112709564\n",
      "### EVAL TRAIN:\n",
      "                  \n",
      " Confusion Matrix: \n",
      "                  \n",
      "     | prediction| \n",
      "     |   0 |  1  | \n",
      " ----+-----+-----+ \n",
      " l 0 | 1594|  994| \n",
      " b --+-----+-----+ \n",
      " l 1 |  865| 1720| \n",
      " ----+-----+-----+ \n",
      "                   \n",
      "CALC\n",
      "  accuraccy: 0.640634061473033\n",
      "  precision: 0.6337509211495947\n",
      "  recall:    0.6653771760154739\n",
      "  f1:        0.6491790903944141\n",
      "  mcc:       0.2816391488947921\n",
      "### EVAL TEST:\n",
      "                  \n",
      " Confusion Matrix: \n",
      "                  \n",
      "     | prediction| \n",
      "     |   0 |  1  | \n",
      " ----+-----+-----+ \n",
      " l 0 |  667|  431| \n",
      " b --+-----+-----+ \n",
      " l 1 |  413|  763| \n",
      " ----+-----+-----+ \n",
      "                   \n",
      "CALC\n",
      "  accuraccy: 0.6288478452066842\n",
      "  precision: 0.6390284757118928\n",
      "  recall:    0.6488095238095238\n",
      "  f1:        0.6438818565400843\n",
      "  mcc:       0.2564493005019037\n"
     ]
    }
   ],
   "source": [
    "# -----------------\n",
    "\n",
    "print(f\"### PREDICT TRAIN\")\n",
    "predict_train = model.transform(df_train)\n",
    "predict_train.select(\"label\", \"prediction\").show(10)\n",
    "print(f\"### PREDICT TEST\")\n",
    "predict_test  = model.transform(df_test)\n",
    "predict_test.select(\"label\", \"prediction\").show(10)\n",
    "\n",
    "# -----------------\n",
    "\n",
    "print(f\"### EVALUATE PREDICTION\")\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol ='rawPrediction', labelCol ='label')\n",
    "predict_test.select(\"label\", \"rawPrediction\", \"prediction\", \"probability\").show(5)\n",
    "print(\"The area under ROC for train set is {}\".format(evaluator.evaluate(predict_train)))\n",
    "print(\"The area under ROC for test set is {}\".format(evaluator.evaluate(predict_test)))\n",
    "\n",
    "print(f\"### EVAL TRAIN:\")\n",
    "confuse(predict_train)\n",
    "print(f\"### EVAL TEST:\")\n",
    "acc, prec, rec, f1 = confuse(predict_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "54651be2-0cae-437d-b15c-05bd63d6bb27",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### SAVE MODEL rf_20_5 22.481265611990008\n",
      "model saved to s3a://udacity-dsnd/sparkify/output/05-model-sparkify_event_data_rf_20_5_f1val0.225\n"
     ]
    }
   ],
   "source": [
    "# -----------------\n",
    "\n",
    "print(f\"### SAVE MODEL {model_name} {f1*100}\")\n",
    "model_url = f'{MODEL_URL}_{model_name}_f1val{round(f1,3)}'\n",
    "model.write().overwrite().save(model_url)\n",
    "print(f\"model saved to {model_url}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "b008cf24-a1ba-47a0-9358-030e895948e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('paid', 0.09554455155152178),\n",
       " ('oh_session_hours', 0.09302707160037064),\n",
       " ('oh_pg_thumbs_up', 0.07582709757031283),\n",
       " ('nh_pg_home', 0.07340830594860079),\n",
       " ('oh_pg_home', 0.0725284688157008),\n",
       " ('oh_session_start', 0.06713461595629495),\n",
       " ('oh_pg_settings', 0.050296622196977205),\n",
       " ('oh_pg_nextsong', 0.03470888922561419),\n",
       " ('nh_pg_logout', 0.031418303669647894),\n",
       " ('oh_pg_logout', 0.029338603181943555),\n",
       " ('oh_pg_thumbs_down', 0.02669508164427074),\n",
       " ('oh_pg_downgrade', 0.0263905768134274),\n",
       " ('oh_pg_add_to_playlist', 0.02189388493915634),\n",
       " ('nh_pg_thumbs_up', 0.021718938628132543),\n",
       " ('nh_pg_downgrade', 0.021610587051586076),\n",
       " ('oh_pg_login', 0.021310639336320675),\n",
       " ('nh_pg_thumbs_down', 0.020975045098593534),\n",
       " ('nh_pg_nextsong', 0.01929290425786421),\n",
       " ('nh_pg_add_to_playlist', 0.01864976547102134),\n",
       " ('nh_status_307', 0.016846133774795467),\n",
       " ('nh_session_hours', 0.016745889155579934),\n",
       " ('nh_pg_add_friend', 0.014856832748766466),\n",
       " ('oh_pg_roll_advert', 0.014159788446922988),\n",
       " ('nh_pg_settings', 0.011455073384124942),\n",
       " ('oh_status_307', 0.011261520204120721),\n",
       " ('oh_pg_about', 0.01081888688907817),\n",
       " ('oh_pg_add_friend', 0.010449979278293572),\n",
       " ('userregistration', 0.010331715693484279),\n",
       " ('nh_session_start', 0.009619201226052369),\n",
       " ('oh_pg_help', 0.00593734543223226),\n",
       " ('nh_pg_upgrade', 0.005483083691995258),\n",
       " ('nh_pg_help', 0.0054691825812952415),\n",
       " ('nh_pg_login', 0.005171923839646339),\n",
       " ('oh_status_404', 0.004981750333382591),\n",
       " ('nh_pg_roll_advert', 0.004403692638058564),\n",
       " ('nh_pg_submit_downgrade', 0.004249893599462164),\n",
       " ('oh_pg_save_settings', 0.0034216769552920688),\n",
       " ('nh_pg_error', 0.0024428874818185698),\n",
       " ('oh_pg_submit_upgrade', 0.0019265281149218618),\n",
       " ('oh_pg_error', 0.0019210946597746476),\n",
       " ('oh_pg_upgrade', 0.0014654235781669683),\n",
       " ('nh_pg_about', 0.0013718261158678826),\n",
       " ('oh_pg_submit_downgrade', 0.0011572777978853636),\n",
       " ('nh_status_404', 0.0010957622431648402),\n",
       " ('usermale', 0.0007544194723368592),\n",
       " ('nh_pg_submit_upgrade', 0.00043125770612211737),\n",
       " ('nh_pg_cancel', 0.0),\n",
       " ('nh_pg_cancellation_confirmation', 0.0),\n",
       " ('nh_pg_register', 0.0),\n",
       " ('nh_pg_save_settings', 0.0),\n",
       " ('nh_pg_submit_registration', 0.0),\n",
       " ('oh_pg_cancel', 0.0),\n",
       " ('oh_pg_cancellation_confirmation', 0.0),\n",
       " ('oh_pg_register', 0.0),\n",
       " ('oh_pg_submit_registration', 0.0)]"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featimp = model.featureImportances\n",
    "nameimp = {}\n",
    "for i in range(len(featimp)):\n",
    "    nameimp[featureCols[i]] = featimp[i]\n",
    "sorted(nameimp.items(), key=lambda x:-x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2614fba5-caf1-4f40-a7f9-7481366b5d03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fcc868-f8e5-4498-87c0-7fd090587025",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1087fe0-1964-4c85-a5de-83be14d5fc6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "87d4cb43-ef96-4630-9844-f463b7b03c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### STOP SPARK SESSION\n"
     ]
    }
   ],
   "source": [
    "print(\"### STOP SPARK SESSION\")\n",
    "spark.stop()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899ac3c4-e4ee-42f3-88c8-a46ae17737df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
