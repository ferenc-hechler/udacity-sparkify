{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15fbf17d-596c-419b-ac66-e8126275f46a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# \\[05\\] Training Models\n",
    "\n",
    "## First thoughts how to train\n",
    "\n",
    "We have 8 and a little bit more weeks of event data aggregated per week and user.\n",
    "\n",
    "For training our model we have to make some decisions:\n",
    "\n",
    "### How far in the future is a churn counted as predictable?  \n",
    "\n",
    "A churn that will happen in 2 months can not be predicted from the current data.  \n",
    "My feeling is that one or two weeks in the future should be fine. \n",
    "\n",
    "### How many data from the past do we need to make a prediction?  \n",
    "\n",
    "Very old history data will not have an impact on the current decision of the user.  \n",
    "But we only have two months of data. So, we can not go too far back anyway.  \n",
    "\n",
    "### Which times to make comparisons?\n",
    "\n",
    "What might be important is to detect changes from behaviour in the past to the current behaviour.  \n",
    "So maybe comparing the events from the last week with the events from the last month might give new insights.\n",
    "\n",
    "If we limit the history to 4 weeks, we can create multiple trainingdata sets for  \n",
    "predicting the results of week 0 with data from week 1,2,3,4.\n",
    "Predicting the results of week 1 with data from week 2,3,4,5.\n",
    "...\n",
    "Predicting the results of week 3 with data from week 4,5,6,7.\n",
    "\n",
    "So we get 4 times more training data than we have users.\n",
    "Maybe we can also make a prediction for week 4, but then we have only partial (5/7) data of week 8.\n",
    "\n",
    "What might be problematic, if a user already churned (downgraded) already in the history.  \n",
    "Then the comparison between old and latest history might not show differences.  \n",
    "\n",
    "\n",
    "## Starter Model\n",
    "\n",
    "For a first start I will make the following splits:\n",
    "\n",
    "* Label = 1: In the next week the user will have at least one churn\n",
    "* history data: one month of history data will be used to make a prediction\n",
    "* comparison: the last week of the history data will be put into relation with the previous three weeks\n",
    "\n",
    "\n",
    "## Setup Spark Session\n",
    "\n",
    "for a detailed description what is done here see [01-setup-spark-session.ipynb](01-setup-spark-session.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "df5acf66-8ff9-4d6f-9d94-f36e3a90c38d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### SETUP SPARK SESSION \"Sparkify\"\n",
      "Spark version: 3.3.2\n"
     ]
    }
   ],
   "source": [
    "EVENT_DATA_URL = \"s3a://udacity-dsnd/sparkify/sparkify_event_data.json\"\n",
    "# EVENT_DATA_URL = \"s3a://udacity-dsnd/sparkify/mini_sparkify_event_data.json\"\n",
    "\n",
    "CLEAN_DATA_URL = EVENT_DATA_URL.replace(\"/sparkify/\", \"/sparkify/output/02-cleaned-\")\n",
    "WEEK_AGGREGATED_DATA_URL = EVENT_DATA_URL.replace(\"/sparkify/\", \"/sparkify/output/04-week-aggregated-\")\n",
    "MODEL_URL = EVENT_DATA_URL.replace(\"/sparkify/\", \"/sparkify/output/05-model-\").replace(\".json\", \"\")\n",
    "\n",
    "EXECUTOR_INSTANCES = 2\n",
    "EXECUTOR_MEM = '6g'\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from cryptography.fernet import Fernet\n",
    "import base64\n",
    "import socket\n",
    "\n",
    "!./install-s3-jars.sh\n",
    "\n",
    "def decrypt(encrypted_text):\n",
    "    \"\"\"\n",
    "    decrypts an encrypted text. The seed (master-password) for decryption is read from the file \".seed.txt\"\n",
    "    \n",
    "    Input: encrypted_text\n",
    "    \n",
    "    Output: the decrypted text. If the text was not encrypted with the same seed, \n",
    "            an exception is raised.\n",
    "    \"\"\"\n",
    "    with open('.seed.txt') as f:\n",
    "        seed = f.read().strip()\n",
    "    return Fernet(base64.b64encode((seed*32)[:32].encode('ascii')).decode('ascii')).decrypt(encrypted_text.encode('ascii')).decode('ascii')\n",
    "\n",
    "AWS_ACCESS_KEY_ID='V6ge1JcQpvyYGJjb'\n",
    "AWS_SECRET_ACCESS_KEY = decrypt('gAAAAABkDFI6865LaVJVgtTYo0aMx9-JTPbTo6cwOUjg5eNNPsZhBDoHbRZ8xuXQT0ImNfvqcecZuoJd1VzYQEpBaxyCnKvosii8O1KeqoL2NwKdKtL_AUfT4eW4dvJVP--VjEvc0gB4')\n",
    "OWN_IP=socket.gethostbyname(socket.gethostname())\n",
    "APP_NAME = \"Sparkify\"\n",
    "SPARK_MASTER = \"spark://bit-spark-master-svc.spark.svc.cluster.local:7077\"\n",
    "S3_HOST = \"minio-api-service.minio.svc\"\n",
    "\n",
    "print(f'### SETUP SPARK SESSION \"{APP_NAME}\"')\n",
    "spark = SparkSession.builder \\\n",
    "    .master(SPARK_MASTER) \\\n",
    "    .config(\"spark.jars\",\"/home/jovyan/jars/aws-java-sdk-bundle-1.11.1026.jar,/home/jovyan/jars/hadoop-aws-3.3.2.jar\") \\\n",
    "    .config(\"spark.driver.host\", OWN_IP) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", S3_HOST) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", AWS_ACCESS_KEY_ID) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", AWS_SECRET_ACCESS_KEY) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.executor.instances\", EXECUTOR_INSTANCES) \\\n",
    "    .config(\"spark.executor.memory\", EXECUTOR_MEM) \\\n",
    "    .appName(APP_NAME).getOrCreate()\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"WARN\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2e1f5c-f14f-45bc-9a55-ced8665a1168",
   "metadata": {},
   "source": [
    "## Get Aggregated Data\n",
    "\n",
    "There are two possibilities, how to get the data aggregated per week.  \n",
    "Load the saved data saved in step 04 from S3 or reapply the transformations to the original dataset.  \n",
    "\n",
    "**Only apply one of both possibilities**\n",
    "\n",
    "### Possibility 1 - Load aggregated dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "ff4fe75b-57c5-411b-9266-1ab208c6e8b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### LOAD DATA s3a://udacity-dsnd/sparkify/output/04-week-aggregated-sparkify_event_data.json\n",
      "### PERSIST\n"
     ]
    }
   ],
   "source": [
    "print(f\"### LOAD DATA {WEEK_AGGREGATED_DATA_URL}\")\n",
    "df_userweek = spark.read.json(WEEK_AGGREGATED_DATA_URL)\n",
    "print(f\"### PERSIST\")\n",
    "df_userweek_persist = df_userweek.persist()\n",
    "df_userweek = df_userweek_persist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f25cdd-3151-4f61-8c8d-b4064a61b467",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Possibility 2 - Load original dataset and Apply Transformations\n",
    "\n",
    "For a detailed description what is done here see [02-data-introspection.ipynb](02-data-introspection.ipynb) and [04-aggregate-data.ipynb](04-aggregate-data.ipynb) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "87704cd4-40ba-48a1-aad4-f2933bf10fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### LOAD DATA s3a://udacity-dsnd/sparkify/mini_sparkify_event_data.json\n",
      "### DROP UNUSED COLUMNS\n",
      "### REMOVE EMPTY USERID\n",
      "### ADD ID\n",
      "### VECTORIZE PAGE FEATURES\n",
      "### VECTORIZE LEVEL FEATURE\n",
      "### VECTORIZE GENDER FEATURE\n",
      "### VECTORIZE STATUS FEATURES\n",
      "### ADD SID\n",
      "### PERSIST\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "print(f\"### LOAD DATA {EVENT_DATA_URL}\")\n",
    "df = spark.read.json(EVENT_DATA_URL)\n",
    "\n",
    "# --- Step 02 Cleanup\n",
    "\n",
    "def norm_colname(name):\n",
    "    \"\"\"\n",
    "    Input: name which can contain spaces with upper and lowercase letters.\n",
    "    Output: all spaces replaced with an underscore and all letters converted to lowercase\n",
    "    \"\"\"\n",
    "    return name.replace(' ', '_').lower()\n",
    "\n",
    "print(f\"### DROP UNUSED COLUMNS\")\n",
    "df = df.drop(\"artist\", \"auth\", \"firstName\", \"lastName\", \"length\", \"location\", \"method\", \"song\", \"userAgent\")\n",
    "print(f\"### REMOVE EMPTY USERID\")\n",
    "df = df.filter(df.userId != '')\n",
    "print(f\"### ADD ID\")\n",
    "w = Window().orderBy(\"ts\")\n",
    "df = df.withColumn(\"id\", F.row_number().over(w))\n",
    "print(f\"### VECTORIZE PAGE FEATURES\")\n",
    "page_features = df.groupBy(\"id\").pivot(\"page\").agg(F.lit(1)).na.fill(0)\n",
    "page_features = page_features.toDF(*((\"pg_\"+norm_colname(col)) if col!=\"id\" else \"id\" for col in page_features.columns))\n",
    "df = df.join(page_features, \"id\")\n",
    "print(f\"### VECTORIZE LEVEL FEATURE\")\n",
    "df = df.withColumn(\"paid\", (df.level == 'paid').cast('int'))\n",
    "df = df.drop(\"level\")\n",
    "print(f\"### VECTORIZE GENDER FEATURE\")\n",
    "df = df.withColumn(\"male\", (df.gender == 'M').cast('int'))\n",
    "df = df.drop(\"gender\")\n",
    "print(f\"### VECTORIZE STATUS FEATURES\")\n",
    "status_features = df.groupBy(\"id\").pivot(\"status\").agg(F.lit(1)).na.fill(0)\n",
    "status_features = status_features.toDF(*((\"status_\"+col) if col != \"id\" else \"id\" for col in status_features.columns)).drop(\"status_200\")\n",
    "df = df.join(status_features, \"id\")\n",
    "df = df.drop(\"status\")\n",
    "print(f\"### ADD SID\")\n",
    "df_sess_user = df.select(\"sessionId\", \"userId\").dropDuplicates()\n",
    "w = Window().orderBy(\"sessionId\", \"userId\")\n",
    "df_sess_user = df_sess_user.withColumn(\"sid\", F.row_number().over(w))\n",
    "df = df.join(df_sess_user, [\"sessionId\", \"userId\"])\n",
    "df_session_start = df.groupBy(\"sid\").agg(F.min(\"id\").alias(\"id\")).drop(\"sid\").withColumn(\"session_start\", F.lit(1).cast(\"int\"))\n",
    "df = df.join(df_session_start, \"id\", how=\"outer\").fillna(0)\n",
    "df = df.drop(\"sessionId\", \"itemInSession\")\n",
    "print(f\"### PERSIST\")\n",
    "df_persist = df.persist()\n",
    "df = df_persist\n",
    "\n",
    "# --- Step 04 Aggregate Week\n",
    "\n",
    "print(f\"### TODO - MOVE TO CLEANUP / MAKE GENERIC\")\n",
    "ts_last = df.agg(F.max(df.ts).alias(\"ts_last\")).collect()[0].ts_last\n",
    "df = df.where(F.col(\"ts\")!=ts_last)\n",
    "print(f\"### GET LAST TS\")\n",
    "ts_first = df.agg(F.min(df.ts).alias(\"ts_first\")).collect()[0].ts_first\n",
    "ts_last = df.agg(F.max(df.ts).alias(\"ts_last\")).collect()[0].ts_last\n",
    "days = (ts_last - ts_first)/one_day\n",
    "print(f\"first timestamp: {datetime.datetime.fromtimestamp(ts_first/1000.0)}\")\n",
    "print(f\"last timestamp: {datetime.datetime.fromtimestamp(ts_last/1000.0)}\")\n",
    "print(f\"days: {days}\")\n",
    "print(f\"### ADD WEEK ID\")\n",
    "df = df.withColumn(\"wid\", F.floor((ts_last-F.col(\"ts\"))/one_week))\n",
    "df.groupBy(\"wid\").count().sort(\"wid\").show()\n",
    "print(f\"### AGG MALE\")\n",
    "df_user = df.groupBy(\"userId\").agg(F.max(F.col(\"male\")).alias(\"usermale\"), F.max((ts_last-F.col(\"registration\"))/one_day).alias(\"userregistration\"))\n",
    "print(f\"### AGG PG/STATUS\")\n",
    "sum_cols = [col for col in df.columns if col.startswith(\"pg_\") or col.startswith(\"status_\")]\n",
    "aggs = [F.sum(F.col(col)).alias(col) for col in sum_cols]\n",
    "df_pg_status_agg = df.groupBy(\"userId\", \"wid\").agg(*aggs)\n",
    "print(f\"### AGG SESSIONSTART\")\n",
    "aggs = [F.sum(F.col(\"session_start\")).alias(\"session_start\"), F.max(F.col(\"id\")).alias(\"max_id\")]\n",
    "df_sessionstart_agg = df.groupBy(\"userId\", \"wid\").agg(*aggs)\n",
    "print(f\"### AGG SESSIONHOURS\")\n",
    "df_sessionhours_agg = df.groupBy(\"userId\", \"wid\", \"sid\").agg(F.max(F.col(\"ts\")), F.min(F.col(\"ts\"))).withColumn(\"session_hours\", (F.col(\"max(ts)\")-F.col(\"min(ts)\"))/one_hour).groupBy(\"userId\", \"wid\").agg(F.sum(F.col(\"session_hours\")).alias(\"session_hours\"))\n",
    "print(f\"### AGG LAST PAID\")\n",
    "df_paid_agg = df.join(df_sessionstart_agg.withColumnRenamed(\"max_id\", \"id\"), [\"userId\", \"wid\", \"id\"]).select(\"userId\", \"wid\", \"id\", \"paid\").drop(\"id\")\n",
    "df_sessionstart_agg = df_sessionstart_agg.drop(\"max_id\")\n",
    "print(f\"### PUTTING TOGETHER\")\n",
    "df_userweek = df_pg_status_agg.join(df_sessionstart_agg, [\"userId\", \"wid\"])\n",
    "df_userweek = df_userweek.join(df_sessionhours_agg, [\"userId\", \"wid\"])\n",
    "df_userweek = df_userweek.join(df_paid_agg, [\"userId\", \"wid\"])\n",
    "df_userweek = df_userweek.join(df_user, [\"userId\"])\n",
    "\n",
    "print(f\"### PERSIST\")\n",
    "df_userweek_persist = df_userweek.persist()\n",
    "df_userweek = df_userweek_persist\n",
    "df_persist.unpersist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d445f9-0f20-4f27-8d20-b30b50356599",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Here are all imports which are needed in the cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "75e62365-bffe-4b21-827b-087d467adc6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier, RandomForestClassificationModel, DecisionTreeClassifier, DecisionTreeClassificationModel, LinearSVC\n",
    "from pyspark.ml.feature import RegexTokenizer, VectorAssembler, Normalizer, StandardScaler, MinMaxScaler, MaxAbsScaler\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.types import IntegerType\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5920ed1-6797-4f43-b823-58e0c0d39baf",
   "metadata": {},
   "source": [
    "## Constants\n",
    "\n",
    "constants that are used in the cells below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "a227c25c-2b02-4da3-b1b8-5d1d27d7a774",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# timestamp constants for ts in milliseconds\n",
    "one_hour =        60*60*1000  #     3.600.000\n",
    "one_day =      24*60*60*1000  #    86.400.000\n",
    "one_week =   7*24*60*60*1000  #   604.800.000\n",
    "one_month = 28*24*60*60*1000  # 2.419.200.000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de22fd59-2560-4c32-8aa9-9cfa7a4497ed",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setup three Dataframes\n",
    "\n",
    "We will setup the following three dataframes:\n",
    "* df_label\n",
    "* df_new_history\n",
    "* df_old_history\n",
    "\n",
    "The code will be flexible, so that we can also make experiments about the timeframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "2b2265b0-b4f9-4838-8fcb-3eb4268c1e4b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weeks to aggregate:\n",
      "  label: 0 - 0\n",
      "  newhistory: 1 - 1\n",
      "  oldhistory: 2 - 4\n"
     ]
    }
   ],
   "source": [
    "# weeks to look into the future from the predict-timestamp for label\n",
    "FUTURE_LOOKAHEAD_WEEKS = 1\n",
    "# weeks to look into the past from the predict-timestamp for new history\n",
    "PAST_NEAR_HISTORY_WEEKS = 1\n",
    "# weeks to look into the past from the predict-timestamp for old history\n",
    "PAST_OLD_HISTORY_WEEKS = 4\n",
    "\n",
    "current_week = 1\n",
    "\n",
    "label_week_min = current_week-FUTURE_LOOKAHEAD_WEEKS\n",
    "label_week_max = current_week-1\n",
    "\n",
    "newhistory_week_min = current_week\n",
    "newhistory_week_max = current_week+PAST_NEAR_HISTORY_WEEKS-1\n",
    "\n",
    "oldhistory_week_min = newhistory_week_max+1\n",
    "oldhistory_week_max = current_week+PAST_OLD_HISTORY_WEEKS-1\n",
    "\n",
    "print(f\"Weeks to aggregate:\")\n",
    "print(f\"  label: {label_week_min} - {label_week_max}\")\n",
    "print(f\"  newhistory: {newhistory_week_min} - {newhistory_week_max}\")\n",
    "print(f\"  oldhistory: {oldhistory_week_min} - {oldhistory_week_max}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1899c0-b5f4-407a-bc97-cfd941ccaa4e",
   "metadata": {},
   "source": [
    "## Constant and Current User Info\n",
    "\n",
    "Get the constant and current user info from current week (=newhistory_week_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "70cbefce-3e78-4a5e-b2b4-b82f70482dbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_user = df_userweek.where(F.col(\"wid\") == newhistory_week_min).select(\"userId\", \"paid\", \"usermale\", \"userregistration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f2a479-0a03-4570-beeb-dac170deca4a",
   "metadata": {},
   "source": [
    "Because we are now going into the past (from current week 0 to current week \"newhistory_week_min\"),  \n",
    "the days since the initial \"userregistration\" event have to be adapted.  \n",
    "Going 1 week into the past means the userregistration has to be reduced by 7 days.\n",
    "\n",
    "Theoretically the value can become negative, but then it means, that there is no history data at all. So, the user will not be used for training/prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "1c7c1a71-f627-4e1d-baf3-c8ffcb5c0768",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_user = df_user.withColumn(\"userregistration\", F.col(\"userregistration\")-7*newhistory_week_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13dc24a-5e04-42a2-af96-9bb9980f8bf9",
   "metadata": {},
   "source": [
    "## Helper function to get dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "191c913a-4c86-4d50-99c3-c8bca6b409b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def aggregate_week_data(from_week, to_week):\n",
    "    \"\"\"\n",
    "    Input: from_week, to_week\n",
    "    Output: aggregated sum data for the weeks from_week..to_week (both including)\n",
    "    \"\"\"\n",
    "    dropcols = [\"paid\", \"usermale\", \"userregistration\", \"wid\"]\n",
    "    df_weeks = df_userweek.where((F.col(\"wid\")>=from_week)&(F.col(\"wid\")<=to_week))\n",
    "    if from_week == to_week:\n",
    "        # no aggregation necessary, if there is only one week\n",
    "        return df_weeks.drop(*dropcols)\n",
    "    aggs = [F.sum(F.col(col)).alias(col) for col in df_weeks.columns if not col in [\"userId\", *dropcols]]\n",
    "    df_weeks = df_weeks.groupBy(\"userId\").agg(*aggs)\n",
    "    return df_weeks    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "a7ec6040-d744-485f-a9a0-97fc55a0e327",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_label = aggregate_week_data(label_week_min, label_week_max)\n",
    "df_newhistory = aggregate_week_data(newhistory_week_min, newhistory_week_max)\n",
    "df_oldhistory = aggregate_week_data(oldhistory_week_min, oldhistory_week_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce31fab9-0972-47c5-ba81-cd111f2885bf",
   "metadata": {},
   "source": [
    "## Set Label\n",
    "\n",
    "check in df_label if a user churned and add label column to df_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "a71c0920-dc9f-497b-bfdf-02dd128d5f45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_label = df_label.withColumn(\"label\", F.when(F.col(\"pg_cancellation_confirmation\")+F.col(\"pg_submit_downgrade\")>0, F.lit(1)).otherwise(F.lit(0))).select(\"userid\", \"label\")\n",
    "df_user = df_user.join(df_label, \"userId\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f8956c-3522-41fa-b4a0-d4571f0001ba",
   "metadata": {},
   "source": [
    "## Add history\n",
    "\n",
    "add new and old history to df_user. Rename columns to make them distinct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37b53bd-005a-4239-baa4-0840c06f6fbd",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Helper function\n",
    "\n",
    "Add a prefix to all columns in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "468f085c-857d-48b5-ba87-9b8884934ed5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prefix_columns(df_orig, prefix, do_not_change_cols):\n",
    "    newcols = [prefix+col if not col in do_not_change_cols else col for col in df_orig.columns]\n",
    "    return df_orig.toDF(*newcols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "18bc298a-90f4-4af0-bb94-80e27da491cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_user = df_user.join(prefix_columns(df_newhistory, \"nh_\", [\"userId\"]), \"userId\")\n",
    "df_user = df_user.join(prefix_columns(df_oldhistory, \"oh_\", [\"userId\"]), \"userId\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3071a849-4318-4507-bf83-a1d6aa9c1d09",
   "metadata": {},
   "source": [
    "## Create Feature Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "ef5db5ff-bcc0-4ec7-9dc5-2062fdf45961",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### CREATE FEATURE COLUMN\n",
      "FEATURES = ['paid', 'usermale', 'userregistration', 'nh_pg_about', 'nh_pg_add_friend', 'nh_pg_add_to_playlist', 'nh_pg_cancel', 'nh_pg_cancellation_confirmation', 'nh_pg_downgrade', 'nh_pg_error', 'nh_pg_help', 'nh_pg_home', 'nh_pg_login', 'nh_pg_logout', 'nh_pg_nextsong', 'nh_pg_register', 'nh_pg_roll_advert', 'nh_pg_save_settings', 'nh_pg_settings', 'nh_pg_submit_downgrade', 'nh_pg_submit_registration', 'nh_pg_submit_upgrade', 'nh_pg_thumbs_down', 'nh_pg_thumbs_up', 'nh_pg_upgrade', 'nh_session_hours', 'nh_session_start', 'nh_status_307', 'nh_status_404', 'oh_pg_about', 'oh_pg_add_friend', 'oh_pg_add_to_playlist', 'oh_pg_cancel', 'oh_pg_cancellation_confirmation', 'oh_pg_downgrade', 'oh_pg_error', 'oh_pg_help', 'oh_pg_home', 'oh_pg_login', 'oh_pg_logout', 'oh_pg_nextsong', 'oh_pg_register', 'oh_pg_roll_advert', 'oh_pg_save_settings', 'oh_pg_settings', 'oh_pg_submit_downgrade', 'oh_pg_submit_registration', 'oh_pg_submit_upgrade', 'oh_pg_thumbs_down', 'oh_pg_thumbs_up', 'oh_pg_upgrade', 'oh_session_hours', 'oh_session_start', 'oh_status_307', 'oh_status_404']\n"
     ]
    }
   ],
   "source": [
    "# -----------------\n",
    "\n",
    "print(f\"### CREATE FEATURE COLUMN\")\n",
    "\n",
    "featureCols = [col for col in df_user.columns if not col in [\"userId\", \"label\"]]\n",
    "print(f\"FEATURES = {featureCols}\")\n",
    "\n",
    "assembler = VectorAssembler(inputCols=featureCols, outputCol=\"features\")\n",
    "df_testtrain_vec=assembler.transform(df_user).select(\"userId\", \"label\",\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "f28ef1e4-1d0d-48b3-a348-69142bb4f360",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_testtrain_vec_persist = df_testtrain_vec.persist()\n",
    "df_testtrain_vec = df_testtrain_vec_persist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "4be24069-b4c4-43ed-8ba6-9c048745236a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### TRAIN / TEST SPLIT\n"
     ]
    }
   ],
   "source": [
    "# -----------------\n",
    "\n",
    "print(f\"### TRAIN / TEST SPLIT\")\n",
    "df_train, df_test = df_testtrain_vec.randomSplit([0.7, 0.3], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "9f366f68-7908-494a-846d-6b4110da6fca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 6112\n",
      "  l1: 617\n",
      "  l0: 5495\n",
      "test: 2662\n",
      "  l1: 248\n",
      "  l0: 2414\n"
     ]
    }
   ],
   "source": [
    "print(f\"train: {df_train.count()}\")\n",
    "print(f\"  l1: {df_train.where(df_train.label==1).count()}\")\n",
    "print(f\"  l0: {df_train.where(df_train.label==0).count()}\")\n",
    "print(f\"test: {df_test.count()}\")\n",
    "print(f\"  l1: {df_test.where(df_test.label==1).count()}\")\n",
    "print(f\"  l0: {df_test.where(df_test.label==0).count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "a2aee875-b3ae-44d0-9187-ec8275307aa7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Fit scaler to train dataset\n",
    "#scaler = MaxAbsScaler().setInputCol('features').setOutputCol('scaled_features')\n",
    "#df_train = df_train.drop(\"scaled_features\")\n",
    "#scaler_model = scaler.fit(df_train)\n",
    "## Scale train and test features\n",
    "#df_train = scaler_model.transform(df_train)\n",
    "#df_test = df_test.drop(\"scaled_features\")\n",
    "#df_test = scaler_model.transform(df_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "e42d8964-db81-456d-a2e6-6fd5294c26d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### CLASSIFIERS\n"
     ]
    }
   ],
   "source": [
    "print(f\"### CLASSIFIERS\")\n",
    "\n",
    "def confuse(df_test_pred):\n",
    "    n00 = df_test_pred.where((F.col(\"label\")==0)&(F.col(\"prediction\")==0)).count()\n",
    "    n01 = df_test_pred.where((F.col(\"label\")==0)&(F.col(\"prediction\")==1)).count()\n",
    "    n10 = df_test_pred.where((F.col(\"label\")==1)&(F.col(\"prediction\")==0)).count()\n",
    "    n11 = df_test_pred.where((F.col(\"label\")==1)&(F.col(\"prediction\")==1)).count()\n",
    "    s00 = \"{:5d}\".format(n00)\n",
    "    s01 = \"{:5d}\".format(n01)\n",
    "    s10 = \"{:5d}\".format(n10)\n",
    "    s11 = \"{:5d}\".format(n11)\n",
    "    print(f\"                  \")\n",
    "    print(f\" Confusion Matrix: \")\n",
    "    print(f\"                  \")\n",
    "    print(f\"     | prediction| \")\n",
    "    print(f\"     |   0 |  1  | \")\n",
    "    print(f\" ----+-----+-----+ \")\n",
    "    print(f\" l 0 |{s00}|{s01}| \")\n",
    "    print(f\" b --+-----+-----+ \")\n",
    "    print(f\" l 1 |{s10}|{s11}| \")\n",
    "    print(f\" ----+-----+-----+ \")\n",
    "    print(f\"                   \")\n",
    "    TP = n11\n",
    "    TN = n00\n",
    "    FP = n01\n",
    "    FN = n10\n",
    "    accuracy = 0\n",
    "    if TP+TN+FP+FN!=0:\n",
    "        accuracy = (TP+TN)/(TP+TN+FP+FN)\n",
    "    precision = 0\n",
    "    if TP+FP!=0:\n",
    "        precision = TP/(TP+FP)\n",
    "    recall = 0\n",
    "    if TP+FN!=0:\n",
    "        recall = TP/(TP+FN)\n",
    "    f1 = 0\n",
    "    if precision+recall!=0:\n",
    "        f1 = 2*precision*recall/(precision+recall)\n",
    "    print(f\"CALC\")\n",
    "    print(f\"  accuraccy: {accuracy}\")\n",
    "    print(f\"  precision: {precision}\")\n",
    "    print(f\"  recall:    {recall}\")\n",
    "    print(f\"  f1:        {f1}\")\n",
    "    # https://towardsdatascience.com/matthews-correlation-coefficient-when-to-use-it-and-when-to-avoid-it-310b3c923f7e\n",
    "    mcc = -9\n",
    "    nenn = (TN+FN)*(FP+TP)*(TN+FP)*(FN+TP)\n",
    "    if nenn!=0:   \n",
    "        mcc = (TN*TP-FP*FN)/math.sqrt(nenn)\n",
    "    print(f\"  mcc:       {mcc}\")\n",
    "    return (accuracy, precision, recall, f1)\n",
    "    \n",
    "    \n",
    "def hyper_tune_rf(num_tree_values, max_depth_values):\n",
    "    best_f1 = -1\n",
    "    best_model = None\n",
    "    best_model_name = \"?\"\n",
    "    for num_trees in num_tree_values:\n",
    "        for max_depth in max_depth_values:\n",
    "            model_name = f\"rf_{num_trees}_{max_depth}\"\n",
    "            rf = RandomForestClassifier(featuresCol=\"features\", numTrees=num_trees, maxDepth=max_depth, seed=42)\n",
    "            rf_model = rf.fit(df_train)\n",
    "            predict_test  = rf_model.transform(df_test)\n",
    "            accuracy, precision, recall, f1 = confuse(predict_test)\n",
    "            print(f\"  {model_name}: f1 {f1}\")\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                best_model = rf_model\n",
    "                best_model_name = model_name\n",
    "    print(f\"best f1 {f1} for {best_model_name}\")\n",
    "    return (best_model, best_f1, best_model_name)\n",
    "\n",
    "\n",
    "def hyper_tune_lr(max_iters, reg_params, elastic_net_params):\n",
    "    # https://towardsdatascience.com/beginners-guide-to-linear-regression-with-pyspark-bfc39b45a9e9\n",
    "    evaluator = RegressionEvaluator(predictionCol=\"prediction_orig\", labelCol=\"label\", metricName=\"rmse\") \n",
    "    \n",
    "    best_err = 9999\n",
    "    best_model = None\n",
    "    best_model_name = \"?\"\n",
    "    for  max_iter in  max_iters:\n",
    "        for reg_param in reg_params:\n",
    "            for elastic_net_param in elastic_net_params:\n",
    "                model_name = f\"lr_{max_iter}_{reg_param}_{elastic_net_param}\"\n",
    "                lr = LinearRegression(featuresCol=\"features\", maxIter= max_iter, regParam=reg_param, elasticNetParam=elastic_net_param)\n",
    "                model = lr.fit(df_train)\n",
    "                predict_test  = model.transform(df_test)\n",
    "                predict_test = predict_test.withColumnRenamed(\"prediction\", \"prediction_orig\")\n",
    "                err = evaluator.evaluate(predict_test)\n",
    "                print(f\"err: {err}\")\n",
    "                thr = 0.15\n",
    "                predict_test = predict_test.withColumn(\"prediction\", F.when(F.col(\"prediction_orig\")>=thr,1).otherwise(0))\n",
    "                accuracy, precision, recall, f1 = confuse(predict_test)\n",
    "                print(f\"  {model_name}: f1 {f1}\")\n",
    "                if err < best_err:\n",
    "                    best_err = err\n",
    "                    best_model = model\n",
    "                    best_model_name = model_name\n",
    "    print(f\"best f1 {f1} for {best_model_name}\")\n",
    "    return (best_model, best_err, best_model_name)\n",
    "\n",
    "\n",
    "def hyper_tune_dt(max_depths, max_bins_list):\n",
    "    best_f1 = -1\n",
    "    best_model = None\n",
    "    best_model_name = \"?\"\n",
    "    for  max_depth in max_depths:\n",
    "        for max_bins in max_bins_list:\n",
    "            model_name = f\"dt_{max_depth}_{max_bins}\"\n",
    "            dt = DecisionTreeClassifier(featuresCol=\"features\", maxDepth=max_depth, maxBins=max_bins)\n",
    "            model = dt.fit(df_train)\n",
    "            predict_test  = model.transform(df_test)\n",
    "            accuracy, precision, recall, f1 = confuse(predict_test)\n",
    "            print(f\"  {model_name}: f1 {f1}\")\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                best_model = model\n",
    "                best_model_name = model_name\n",
    "    print(f\"best f1 {f1} for {best_model_name}\")\n",
    "    return (best_model, best_f1, best_model_name)\n",
    "\n",
    "\n",
    "def hyper_tune_sv(max_iters, reg_params):\n",
    "    best_f1 = -1\n",
    "    best_model = None\n",
    "    best_model_name = \"?\"\n",
    "    for  max_iter in max_iters:\n",
    "        for reg_param in reg_params:\n",
    "            model_name = f\"svm_{max_iter}_{reg_param}\"\n",
    "            lsvc = LinearSVC(featuresCol=\"features\", maxIter=max_iter, regParam=reg_param)\n",
    "            model = lsvc.fit(df_train)\n",
    "            predict_test  = model.transform(df_test)\n",
    "            accuracy, precision, recall, f1 = confuse(predict_test)\n",
    "            print(f\"  {model_name}: f1 {f1}\")\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                best_model = model\n",
    "                best_model_name = model_name\n",
    "    print(f\"best f1 {f1} for {best_model_name}\")\n",
    "    return (best_model, best_f1, best_model_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "660a158d-a695-4c3c-99e1-204d16f006e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train_orig = df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "25f4777e-b717-44e3-b41a-ae8ff6411033",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label0: 5495\n",
      "label1: 617\n",
      "oversampling to: 1234/5495\n",
      "oversampling to: 1851/5495\n",
      "oversampling to: 2468/5495\n",
      "oversampling to: 3085/5495\n",
      "oversampling to: 3702/5495\n",
      "oversampling to: 4319/5495\n",
      "oversampling to: 4936/5495\n",
      "oversampling to: 5553/5495\n"
     ]
    }
   ],
   "source": [
    "df_train = df_train_orig\n",
    "df_lab0 = df_train.where(F.col(\"label\") == 0)\n",
    "df_lab1 = df_train.where(F.col(\"label\") == 1)\n",
    "\n",
    "train0cnt = df_lab0.count()\n",
    "train1cnt = df_lab1.count()\n",
    "print(f\"label0: {train0cnt}\")\n",
    "print(f\"label1: {train1cnt}\")\n",
    "\n",
    "oversampled_train = df_train\n",
    "sum1cnt = train1cnt\n",
    "while sum1cnt <= train0cnt:\n",
    "    sum1cnt = sum1cnt+train1cnt\n",
    "    print(f\"oversampling to: {sum1cnt}/{train0cnt}\")\n",
    "    oversampled_train = oversampled_train.union(df_lab1)\n",
    "    \n",
    "df_train = oversampled_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "98acd19d-53e1-4c78-a732-d8984c479011",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  \n",
      " Confusion Matrix: \n",
      "                  \n",
      "     | prediction| \n",
      "     |   0 |  1  | \n",
      " ----+-----+-----+ \n",
      " l 0 | 1596|  818| \n",
      " b --+-----+-----+ \n",
      " l 1 |  113|  135| \n",
      " ----+-----+-----+ \n",
      "                   \n",
      "CALC\n",
      "  accuraccy: 0.6502629601803156\n",
      "  precision: 0.1416579223504722\n",
      "  recall:    0.5443548387096774\n",
      "  f1:        0.22481265611990006\n",
      "  mcc:       0.12459050577482478\n",
      "  rf_20_5: f1 0.22481265611990006\n",
      "best f1 0.22481265611990006 for rf_20_5\n"
     ]
    }
   ],
   "source": [
    "model, f1, model_name = hyper_tune_rf([20], [5])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "63cd2a28-73fb-4545-8a47-154a6fa60e23",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### PREDICT TRAIN\n",
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       1.0|\n",
      "|    0|       1.0|\n",
      "|    0|       1.0|\n",
      "+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "### PREDICT TEST\n",
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "|    0|       1.0|\n",
      "|    0|       0.0|\n",
      "|    0|       1.0|\n",
      "|    0|       0.0|\n",
      "|    0|       1.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       1.0|\n",
      "|    0|       0.0|\n",
      "+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "### EVALUATE PREDICTION\n",
      "+-----+--------------------+----------+--------------------+\n",
      "|label|       rawPrediction|prediction|         probability|\n",
      "+-----+--------------------+----------+--------------------+\n",
      "|    0|[6.53447731026865...|       1.0|[0.32672386551343...|\n",
      "|    0|[11.3230664445512...|       0.0|[0.56615332222756...|\n",
      "|    0|[9.11943749389874...|       1.0|[0.45597187469493...|\n",
      "|    0|[11.6777212220427...|       0.0|[0.58388606110213...|\n",
      "|    0|[5.26143619212753...|       1.0|[0.26307180960637...|\n",
      "+-----+--------------------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "The area under ROC for train set is 0.7559943251784806\n",
      "The area under ROC for test set is 0.6346647245904269\n",
      "### EVAL TRAIN:\n",
      "                  \n",
      " Confusion Matrix: \n",
      "                  \n",
      "     | prediction| \n",
      "     |   0 |  1  | \n",
      " ----+-----+-----+ \n",
      " l 0 | 3746| 1749| \n",
      " b --+-----+-----+ \n",
      " l 1 | 1746| 3807| \n",
      " ----+-----+-----+ \n",
      "                   \n",
      "CALC\n",
      "  accuraccy: 0.683653149891383\n",
      "  precision: 0.6852051835853131\n",
      "  recall:    0.6855753646677472\n",
      "  f1:        0.6853902241425871\n",
      "  mcc:       0.3672871120766154\n",
      "### EVAL TEST:\n",
      "                  \n",
      " Confusion Matrix: \n",
      "                  \n",
      "     | prediction| \n",
      "     |   0 |  1  | \n",
      " ----+-----+-----+ \n",
      " l 0 | 1596|  818| \n",
      " b --+-----+-----+ \n",
      " l 1 |  113|  135| \n",
      " ----+-----+-----+ \n",
      "                   \n",
      "CALC\n",
      "  accuraccy: 0.6502629601803156\n",
      "  precision: 0.1416579223504722\n",
      "  recall:    0.5443548387096774\n",
      "  f1:        0.22481265611990006\n",
      "  mcc:       0.12459050577482478\n"
     ]
    }
   ],
   "source": [
    "# -----------------\n",
    "\n",
    "print(f\"### PREDICT TRAIN\")\n",
    "predict_train = model.transform(df_train)\n",
    "predict_train.select(\"label\", \"prediction\").show(10)\n",
    "print(f\"### PREDICT TEST\")\n",
    "predict_test  = model.transform(df_test)\n",
    "predict_test.select(\"label\", \"prediction\").show(10)\n",
    "\n",
    "# -----------------\n",
    "\n",
    "print(f\"### EVALUATE PREDICTION\")\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol ='rawPrediction', labelCol ='label')\n",
    "predict_test.select(\"label\", \"rawPrediction\", \"prediction\", \"probability\").show(5)\n",
    "print(\"The area under ROC for train set is {}\".format(evaluator.evaluate(predict_train)))\n",
    "print(\"The area under ROC for test set is {}\".format(evaluator.evaluate(predict_test)))\n",
    "\n",
    "print(f\"### EVAL TRAIN:\")\n",
    "confuse(predict_train)\n",
    "print(f\"### EVAL TEST:\")\n",
    "acc, prec, rec, f1 = confuse(predict_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "54651be2-0cae-437d-b15c-05bd63d6bb27",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### SAVE MODEL rf_20_5 22.481265611990008\n",
      "model saved to s3a://udacity-dsnd/sparkify/output/05-model-sparkify_event_data_rf_20_5_f1val0.225\n"
     ]
    }
   ],
   "source": [
    "# -----------------\n",
    "\n",
    "print(f\"### SAVE MODEL {model_name} {f1*100}\")\n",
    "model_url = f'{MODEL_URL}_{model_name}_f1val{round(f1,3)}'\n",
    "model.write().overwrite().save(model_url)\n",
    "print(f\"model saved to {model_url}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "b008cf24-a1ba-47a0-9358-030e895948e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('paid', 0.09554455155152178),\n",
       " ('oh_session_hours', 0.09302707160037064),\n",
       " ('oh_pg_thumbs_up', 0.07582709757031283),\n",
       " ('nh_pg_home', 0.07340830594860079),\n",
       " ('oh_pg_home', 0.0725284688157008),\n",
       " ('oh_session_start', 0.06713461595629495),\n",
       " ('oh_pg_settings', 0.050296622196977205),\n",
       " ('oh_pg_nextsong', 0.03470888922561419),\n",
       " ('nh_pg_logout', 0.031418303669647894),\n",
       " ('oh_pg_logout', 0.029338603181943555),\n",
       " ('oh_pg_thumbs_down', 0.02669508164427074),\n",
       " ('oh_pg_downgrade', 0.0263905768134274),\n",
       " ('oh_pg_add_to_playlist', 0.02189388493915634),\n",
       " ('nh_pg_thumbs_up', 0.021718938628132543),\n",
       " ('nh_pg_downgrade', 0.021610587051586076),\n",
       " ('oh_pg_login', 0.021310639336320675),\n",
       " ('nh_pg_thumbs_down', 0.020975045098593534),\n",
       " ('nh_pg_nextsong', 0.01929290425786421),\n",
       " ('nh_pg_add_to_playlist', 0.01864976547102134),\n",
       " ('nh_status_307', 0.016846133774795467),\n",
       " ('nh_session_hours', 0.016745889155579934),\n",
       " ('nh_pg_add_friend', 0.014856832748766466),\n",
       " ('oh_pg_roll_advert', 0.014159788446922988),\n",
       " ('nh_pg_settings', 0.011455073384124942),\n",
       " ('oh_status_307', 0.011261520204120721),\n",
       " ('oh_pg_about', 0.01081888688907817),\n",
       " ('oh_pg_add_friend', 0.010449979278293572),\n",
       " ('userregistration', 0.010331715693484279),\n",
       " ('nh_session_start', 0.009619201226052369),\n",
       " ('oh_pg_help', 0.00593734543223226),\n",
       " ('nh_pg_upgrade', 0.005483083691995258),\n",
       " ('nh_pg_help', 0.0054691825812952415),\n",
       " ('nh_pg_login', 0.005171923839646339),\n",
       " ('oh_status_404', 0.004981750333382591),\n",
       " ('nh_pg_roll_advert', 0.004403692638058564),\n",
       " ('nh_pg_submit_downgrade', 0.004249893599462164),\n",
       " ('oh_pg_save_settings', 0.0034216769552920688),\n",
       " ('nh_pg_error', 0.0024428874818185698),\n",
       " ('oh_pg_submit_upgrade', 0.0019265281149218618),\n",
       " ('oh_pg_error', 0.0019210946597746476),\n",
       " ('oh_pg_upgrade', 0.0014654235781669683),\n",
       " ('nh_pg_about', 0.0013718261158678826),\n",
       " ('oh_pg_submit_downgrade', 0.0011572777978853636),\n",
       " ('nh_status_404', 0.0010957622431648402),\n",
       " ('usermale', 0.0007544194723368592),\n",
       " ('nh_pg_submit_upgrade', 0.00043125770612211737),\n",
       " ('nh_pg_cancel', 0.0),\n",
       " ('nh_pg_cancellation_confirmation', 0.0),\n",
       " ('nh_pg_register', 0.0),\n",
       " ('nh_pg_save_settings', 0.0),\n",
       " ('nh_pg_submit_registration', 0.0),\n",
       " ('oh_pg_cancel', 0.0),\n",
       " ('oh_pg_cancellation_confirmation', 0.0),\n",
       " ('oh_pg_register', 0.0),\n",
       " ('oh_pg_submit_registration', 0.0)]"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featimp = model.featureImportances\n",
    "nameimp = {}\n",
    "for i in range(len(featimp)):\n",
    "    nameimp[featureCols[i]] = featimp[i]\n",
    "sorted(nameimp.items(), key=lambda x:-x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2614fba5-caf1-4f40-a7f9-7481366b5d03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fcc868-f8e5-4498-87c0-7fd090587025",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1087fe0-1964-4c85-a5de-83be14d5fc6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "87d4cb43-ef96-4630-9844-f463b7b03c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### STOP SPARK SESSION\n"
     ]
    }
   ],
   "source": [
    "print(\"### STOP SPARK SESSION\")\n",
    "spark.stop()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899ac3c4-e4ee-42f3-88c8-a46ae17737df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
