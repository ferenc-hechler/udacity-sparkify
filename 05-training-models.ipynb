{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15fbf17d-596c-419b-ac66-e8126275f46a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# \\[05\\] Training Models\n",
    "\n",
    "## First thoughts how to train\n",
    "\n",
    "We have 8 and a little bit more weeks of event data aggregated per week and user.\n",
    "\n",
    "For training our model we have to make some decisions:\n",
    "\n",
    "### How far in the future is a churn counted as predictable?  \n",
    "\n",
    "A churn that will happen in 2 months can not be predicted from the current data.  \n",
    "My feeling is that one or two weeks in the future should be fine. \n",
    "\n",
    "### How many data from the past do we need to make a prediction?  \n",
    "\n",
    "Very old history data will not have an impact on the current decision of the user.  \n",
    "But we only have two months of data. So, we can not go too far back anyway.  \n",
    "\n",
    "### Which times to make comparisons?\n",
    "\n",
    "What might be important is to detect changes from behaviour in the past to the current behaviour.  \n",
    "So maybe comparing the events from the last week with the events from the last month might give new insights.\n",
    "\n",
    "#### Example 1 Week future, 1 Week new history, 3 Weeks old history\n",
    "\n",
    "The following image should illustrate, how the training data is derived from the full dataset.  \n",
    "The green line is \"now\", the current time.  \n",
    "We want to make a prediction, if the user will churn in the near future (= next week).  \n",
    "The blue line marks the latest history (one week ago till \"now\").  \n",
    "And finally the purple line marks the old history (4-weeks ago till 1-week ago).  \n",
    "\n",
    "![weeks-for-training-datasets](imgs/weeks-for-training-datasets.png)\n",
    "\n",
    "If we limit the history to 4 weeks, we can create multiple trainingdata sets for  \n",
    "predicting the results of week 0 with data from week 1,2,3,4 (as shown in the image).  \n",
    "Predicting the results of week 1 with data from week 2,3,4,5.  \n",
    "...  \n",
    "Predicting the results of week 3 with data from week 4,5,6,7.  \n",
    "\n",
    "So we get 4 times more training data than we have users.\n",
    "Maybe we can also make a prediction for week 4, but then we have only partial (5/7) data of week 8.\n",
    "\n",
    "What might be problematic, if a user already churned (downgraded) already in the history.  \n",
    "Then the comparison between old and latest history might not show differences.  \n",
    "\n",
    "\n",
    "## Starter Model\n",
    "\n",
    "For a first start I will make the following splits:\n",
    "\n",
    "* Label = 1: In the next week the user will have at least one churn\n",
    "* history data: one month of history data will be used to make a prediction\n",
    "* comparison: the last week of the history data will be put into relation with the previous three weeks\n",
    "\n",
    "\n",
    "## Setup Spark Session\n",
    "\n",
    "for a detailed description what is done here see [01-setup-spark-session.ipynb](01-setup-spark-session.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df5acf66-8ff9-4d6f-9d94-f36e3a90c38d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### SETUP SPARK SESSION \"Sparkify\"\n",
      "Spark version: 3.3.2\n"
     ]
    }
   ],
   "source": [
    "EVENT_DATA_URL = \"s3a://udacity-dsnd/sparkify/sparkify_event_data.json\"\n",
    "# EVENT_DATA_URL = \"s3a://udacity-dsnd/sparkify/mini_sparkify_event_data.json\"\n",
    "\n",
    "CLEAN_DATA_URL = EVENT_DATA_URL.replace(\"/sparkify/\", \"/sparkify/output/02-cleaned-\")\n",
    "WEEK_AGGREGATED_DATA_URL = EVENT_DATA_URL.replace(\"/sparkify/\", \"/sparkify/output/04-week-aggregated-\")\n",
    "MODEL_URL = EVENT_DATA_URL.replace(\"/sparkify/\", \"/sparkify/output/05-model-\").replace(\".json\", \"\")\n",
    "\n",
    "EXECUTOR_INSTANCES = 2\n",
    "EXECUTOR_MEM = '6g'\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from cryptography.fernet import Fernet\n",
    "import base64\n",
    "import socket\n",
    "\n",
    "!./install-s3-jars.sh\n",
    "\n",
    "def decrypt(encrypted_text):\n",
    "    \"\"\"\n",
    "    decrypts an encrypted text. The seed (master-password) for decryption is read from the file \".seed.txt\"\n",
    "    \n",
    "    Input: encrypted_text\n",
    "    \n",
    "    Output: the decrypted text. If the text was not encrypted with the same seed, \n",
    "            an exception is raised.\n",
    "    \"\"\"\n",
    "    with open('.seed.txt') as f:\n",
    "        seed = f.read().strip()\n",
    "    return Fernet(base64.b64encode((seed*32)[:32].encode('ascii')).decode('ascii')).decrypt(encrypted_text.encode('ascii')).decode('ascii')\n",
    "\n",
    "AWS_ACCESS_KEY_ID='V6ge1JcQpvyYGJjb'\n",
    "AWS_SECRET_ACCESS_KEY = decrypt('gAAAAABkDFI6865LaVJVgtTYo0aMx9-JTPbTo6cwOUjg5eNNPsZhBDoHbRZ8xuXQT0ImNfvqcecZuoJd1VzYQEpBaxyCnKvosii8O1KeqoL2NwKdKtL_AUfT4eW4dvJVP--VjEvc0gB4')\n",
    "OWN_IP=socket.gethostbyname(socket.gethostname())\n",
    "APP_NAME = \"Sparkify\"\n",
    "SPARK_MASTER = \"spark://bit-spark-master-svc.spark.svc.cluster.local:7077\"\n",
    "S3_HOST = \"minio-api-service.minio.svc\"\n",
    "\n",
    "print(f'### SETUP SPARK SESSION \"{APP_NAME}\"')\n",
    "spark = SparkSession.builder \\\n",
    "    .master(SPARK_MASTER) \\\n",
    "    .config(\"spark.jars\",\"/home/jovyan/jars/aws-java-sdk-bundle-1.11.1026.jar,/home/jovyan/jars/hadoop-aws-3.3.2.jar\") \\\n",
    "    .config(\"spark.driver.host\", OWN_IP) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", S3_HOST) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", AWS_ACCESS_KEY_ID) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", AWS_SECRET_ACCESS_KEY) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.executor.instances\", EXECUTOR_INSTANCES) \\\n",
    "    .config(\"spark.executor.memory\", EXECUTOR_MEM) \\\n",
    "    .appName(APP_NAME).getOrCreate()\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"WARN\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2e1f5c-f14f-45bc-9a55-ced8665a1168",
   "metadata": {},
   "source": [
    "## Get Aggregated Data\n",
    "\n",
    "There are two possibilities, how to get the data aggregated per week.  \n",
    "Load the cleaned data saved in step 04 from S3 or reapply the transformations to the original dataset.  \n",
    "\n",
    "**Only apply one of both possibilities**\n",
    "\n",
    "### Possibility 1 - Load aggregated dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff4fe75b-57c5-411b-9266-1ab208c6e8b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### LOAD DATA s3a://udacity-dsnd/sparkify/output/04-week-aggregated-sparkify_event_data.json\n",
      "### PERSIST\n"
     ]
    }
   ],
   "source": [
    "print(f\"### LOAD DATA {WEEK_AGGREGATED_DATA_URL}\")\n",
    "df_userweek = spark.read.json(WEEK_AGGREGATED_DATA_URL)\n",
    "print(f\"### PERSIST\")\n",
    "df_userweek_persist = df_userweek.persist()\n",
    "df_userweek = df_userweek_persist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f25cdd-3151-4f61-8c8d-b4064a61b467",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Possibility 2 - Load original dataset and Apply Transformations\n",
    "\n",
    "For a detailed description what is done here see [02-data-introspection.ipynb](02-data-introspection.ipynb) and [04-aggregate-data.ipynb](04-aggregate-data.ipynb) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "87704cd4-40ba-48a1-aad4-f2933bf10fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### LOAD DATA s3a://udacity-dsnd/sparkify/mini_sparkify_event_data.json\n",
      "### DROP UNUSED COLUMNS\n",
      "### REMOVE EMPTY USERID\n",
      "### ADD ID\n",
      "### VECTORIZE PAGE FEATURES\n",
      "### VECTORIZE LEVEL FEATURE\n",
      "### VECTORIZE GENDER FEATURE\n",
      "### VECTORIZE STATUS FEATURES\n",
      "### ADD SID\n",
      "### PERSIST\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "print(f\"### LOAD DATA {EVENT_DATA_URL}\")\n",
    "df = spark.read.json(EVENT_DATA_URL)\n",
    "\n",
    "# --- Step 02 Cleanup\n",
    "\n",
    "def norm_colname(name):\n",
    "    \"\"\"\n",
    "    Input: name which can contain spaces with upper and lowercase letters.\n",
    "    Output: all spaces replaced with an underscore and all letters converted to lowercase\n",
    "    \"\"\"\n",
    "    return name.replace(' ', '_').lower()\n",
    "\n",
    "print(f\"### DROP UNUSED COLUMNS\")\n",
    "df = df.drop(\"artist\", \"auth\", \"firstName\", \"lastName\", \"length\", \"location\", \"method\", \"song\", \"userAgent\")\n",
    "print(f\"### REMOVE EMPTY USERID\")\n",
    "df = df.filter(df.userId != '')\n",
    "print(f\"### ADD ID\")\n",
    "w = Window().orderBy(\"ts\")\n",
    "df = df.withColumn(\"id\", F.row_number().over(w))\n",
    "print(f\"### VECTORIZE PAGE FEATURES\")\n",
    "page_features = df.groupBy(\"id\").pivot(\"page\").agg(F.lit(1)).na.fill(0)\n",
    "page_features = page_features.toDF(*((\"pg_\"+norm_colname(col)) if col!=\"id\" else \"id\" for col in page_features.columns))\n",
    "df = df.join(page_features, \"id\")\n",
    "print(f\"### VECTORIZE LEVEL FEATURE\")\n",
    "df = df.withColumn(\"paid\", (df.level == 'paid').cast('int'))\n",
    "df = df.drop(\"level\")\n",
    "print(f\"### VECTORIZE GENDER FEATURE\")\n",
    "df = df.withColumn(\"male\", (df.gender == 'M').cast('int'))\n",
    "df = df.drop(\"gender\")\n",
    "print(f\"### VECTORIZE STATUS FEATURES\")\n",
    "status_features = df.groupBy(\"id\").pivot(\"status\").agg(F.lit(1)).na.fill(0)\n",
    "status_features = status_features.toDF(*((\"status_\"+col) if col != \"id\" else \"id\" for col in status_features.columns)).drop(\"status_200\")\n",
    "df = df.join(status_features, \"id\")\n",
    "df = df.drop(\"status\")\n",
    "print(f\"### ADD SID\")\n",
    "df_sess_user = df.select(\"sessionId\", \"userId\").dropDuplicates()\n",
    "w = Window().orderBy(\"sessionId\", \"userId\")\n",
    "df_sess_user = df_sess_user.withColumn(\"sid\", F.row_number().over(w))\n",
    "df = df.join(df_sess_user, [\"sessionId\", \"userId\"])\n",
    "df_session_start = df.groupBy(\"sid\").agg(F.min(\"id\").alias(\"id\")).drop(\"sid\").withColumn(\"session_start\", F.lit(1).cast(\"int\"))\n",
    "df = df.join(df_session_start, \"id\", how=\"outer\").fillna(0)\n",
    "df = df.drop(\"sessionId\", \"itemInSession\")\n",
    "print(f\"### PERSIST\")\n",
    "df_persist = df.persist()\n",
    "df = df_persist\n",
    "\n",
    "# --- Step 04 Aggregate Week\n",
    "\n",
    "print(f\"### TODO - MOVE TO CLEANUP / MAKE GENERIC\")\n",
    "ts_last = df.agg(F.max(df.ts).alias(\"ts_last\")).collect()[0].ts_last\n",
    "df = df.where(F.col(\"ts\")!=ts_last)\n",
    "print(f\"### GET LAST TS\")\n",
    "ts_first = df.agg(F.min(df.ts).alias(\"ts_first\")).collect()[0].ts_first\n",
    "ts_last = df.agg(F.max(df.ts).alias(\"ts_last\")).collect()[0].ts_last\n",
    "days = (ts_last - ts_first)/one_day\n",
    "print(f\"first timestamp: {datetime.datetime.fromtimestamp(ts_first/1000.0)}\")\n",
    "print(f\"last timestamp: {datetime.datetime.fromtimestamp(ts_last/1000.0)}\")\n",
    "print(f\"days: {days}\")\n",
    "print(f\"### ADD WEEK ID\")\n",
    "df = df.withColumn(\"wid\", F.floor((ts_last-F.col(\"ts\"))/one_week))\n",
    "df.groupBy(\"wid\").count().sort(\"wid\").show()\n",
    "print(f\"### AGG MALE\")\n",
    "df_user = df.groupBy(\"userId\").agg(F.max(F.col(\"male\")).alias(\"usermale\"), F.max((ts_last-F.col(\"registration\"))/one_day).alias(\"userregistration\"))\n",
    "print(f\"### COUNT NUM EVENTS\")\n",
    "aggs = [F.count(F.col(\"id\")).alias(\"num_events\")]\n",
    "df_num_events_agg = df.groupBy(\"userId\", \"wid\").agg(*aggs)\n",
    "print(f\"### AGG PG/STATUS\")\n",
    "sum_cols = [col for col in df.columns if col.startswith(\"pg_\") or col.startswith(\"status_\")]\n",
    "aggs = [F.sum(F.col(col)).alias(col) for col in sum_cols]\n",
    "df_pg_status_agg = df.groupBy(\"userId\", \"wid\").agg(*aggs)\n",
    "print(f\"### AGG SESSIONSTART\")\n",
    "aggs = [F.sum(F.col(\"session_start\")).alias(\"session_start\"), F.max(F.col(\"id\")).alias(\"max_id\")]\n",
    "df_sessionstart_agg = df.groupBy(\"userId\", \"wid\").agg(*aggs)\n",
    "print(f\"### AGG SESSIONHOURS\")\n",
    "df_sessionhours_agg = df.groupBy(\"userId\", \"wid\", \"sid\").agg(F.max(F.col(\"ts\")), F.min(F.col(\"ts\"))).withColumn(\"session_hours\", (F.col(\"max(ts)\")-F.col(\"min(ts)\"))/one_hour).groupBy(\"userId\", \"wid\").agg(F.sum(F.col(\"session_hours\")).alias(\"session_hours\"))\n",
    "print(f\"### AGG LAST PAID\")\n",
    "df_paid_agg = df.join(df_sessionstart_agg.withColumnRenamed(\"max_id\", \"id\"), [\"userId\", \"wid\", \"id\"]).select(\"userId\", \"wid\", \"id\", \"paid\").drop(\"id\")\n",
    "df_sessionstart_agg = df_sessionstart_agg.drop(\"max_id\")\n",
    "print(f\"### PUTTING TOGETHER\")\n",
    "df_userweek = df_pg_status_agg.join(df_sessionstart_agg, [\"userId\", \"wid\"])\n",
    "df_userweek = df_userweek.join(df_sessionhours_agg, [\"userId\", \"wid\"])\n",
    "df_userweek = df_userweek.join(df_paid_agg, [\"userId\", \"wid\"])\n",
    "df_userweek = df_userweek.join(df_num_events_agg, [\"userId\", \"wid\"])\n",
    "df_userweek = df_userweek.join(df_user, [\"userId\"])\n",
    "\n",
    "print(f\"### PERSIST\")\n",
    "df_userweek_persist = df_userweek.persist()\n",
    "df_userweek = df_userweek_persist\n",
    "df_persist.unpersist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d445f9-0f20-4f27-8d20-b30b50356599",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Here are all imports which are needed in the cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75e62365-bffe-4b21-827b-087d467adc6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier, RandomForestClassificationModel, DecisionTreeClassifier, DecisionTreeClassificationModel, LinearSVC\n",
    "from pyspark.ml.feature import RegexTokenizer, VectorAssembler, Normalizer, StandardScaler, MinMaxScaler, MaxAbsScaler\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.types import IntegerType\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5920ed1-6797-4f43-b823-58e0c0d39baf",
   "metadata": {},
   "source": [
    "## Constants\n",
    "\n",
    "constants that are used in the cells below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a227c25c-2b02-4da3-b1b8-5d1d27d7a774",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# timestamp constants for ts in milliseconds\n",
    "one_hour =        60*60*1000  #     3.600.000\n",
    "one_day =      24*60*60*1000  #    86.400.000\n",
    "one_week =   7*24*60*60*1000  #   604.800.000\n",
    "one_month = 28*24*60*60*1000  # 2.419.200.000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de22fd59-2560-4c32-8aa9-9cfa7a4497ed",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setup three Dataframes\n",
    "\n",
    "We will setup the following three dataframes:\n",
    "* df_label\n",
    "* df_new_history\n",
    "* df_old_history\n",
    "\n",
    "The code will be flexible, so that we can also make experiments about the timeframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b2265b0-b4f9-4838-8fcb-3eb4268c1e4b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weeks to aggregate:\n",
      "  label: 0 - 1\n",
      "  newhistory: 2 - 2\n",
      "  oldhistory: 3 - 5\n"
     ]
    }
   ],
   "source": [
    "# weeks to look into the future from the predict-timestamp for label\n",
    "FUTURE_LOOKAHEAD_WEEKS = 2\n",
    "# weeks to look into the past from the predict-timestamp for new history\n",
    "PAST_NEAR_HISTORY_WEEKS = 1\n",
    "# weeks to look into the past from the predict-timestamp for old history\n",
    "PAST_OLD_HISTORY_WEEKS = 3\n",
    "\n",
    "current_week = 2\n",
    "\n",
    "label_week_min = current_week-FUTURE_LOOKAHEAD_WEEKS\n",
    "label_week_max = current_week-1\n",
    "\n",
    "newhistory_week_min = current_week\n",
    "newhistory_week_max = newhistory_week_max+PAST_NEAR_HISTORY_WEEKS-1\n",
    "\n",
    "oldhistory_week_min = newhistory_week_max+1\n",
    "oldhistory_week_max = oldhistory_week_min+PAST_OLD_HISTORY_WEEKS-1\n",
    "\n",
    "print(f\"Weeks to aggregate:\")\n",
    "print(f\"  label: {label_week_min} - {label_week_max}\")\n",
    "print(f\"  newhistory: {newhistory_week_min} - {newhistory_week_max}\")\n",
    "print(f\"  oldhistory: {oldhistory_week_min} - {oldhistory_week_max}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1899c0-b5f4-407a-bc97-cfd941ccaa4e",
   "metadata": {},
   "source": [
    "## Constant and Current User Info\n",
    "\n",
    "Get the constant and current user info from current week (=newhistory_week_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70cbefce-3e78-4a5e-b2b4-b82f70482dbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_user = df_userweek.where(F.col(\"wid\") == newhistory_week_min).select(\"userId\", \"paid\", \"usermale\", \"userregistration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f2a479-0a03-4570-beeb-dac170deca4a",
   "metadata": {},
   "source": [
    "Because we are now setting the \"current_week\" 2 weeks into the past the relative value in days for  \n",
    "\"userregistration\" has to be adapted.  \n",
    "Going 1 week into the past means the userregistration has to be reduced by 7 days.\n",
    "\n",
    "Theoretically the value can become negative, but then it means, that there is no history data at all. So, the user will not be used for training/prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c7c1a71-f627-4e1d-baf3-c8ffcb5c0768",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_user = df_user.withColumn(\"userregistration\", F.col(\"userregistration\")-7*newhistory_week_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13dc24a-5e04-42a2-af96-9bb9980f8bf9",
   "metadata": {},
   "source": [
    "## Helper function to get dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "191c913a-4c86-4d50-99c3-c8bca6b409b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def aggregate_week_data(from_week, to_week):\n",
    "    \"\"\"\n",
    "    Input: from_week, to_week\n",
    "    Output: aggregated sum data for the weeks from_week..to_week (both including)\n",
    "    \"\"\"\n",
    "    dropcols = [\"paid\", \"usermale\", \"userregistration\", \"wid\"]\n",
    "    df_weeks = df_userweek.where((F.col(\"wid\")>=from_week)&(F.col(\"wid\")<=to_week))\n",
    "    if from_week == to_week:\n",
    "        # no aggregation necessary, if there is only one week\n",
    "        return df_weeks.drop(*dropcols)\n",
    "    aggs = [F.sum(F.col(col)).alias(col) for col in df_weeks.columns if not col in [\"userId\", *dropcols]]\n",
    "    df_weeks = df_weeks.groupBy(\"userId\").agg(*aggs)\n",
    "    return df_weeks    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d304aa-706b-4187-af21-078e918aaebe",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create three Datasets\n",
    "\n",
    "Using the helper function we can now split the data into three partitions as explained above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7ec6040-d744-485f-a9a0-97fc55a0e327",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_label = aggregate_week_data(label_week_min, label_week_max)\n",
    "df_newhistory = aggregate_week_data(newhistory_week_min, newhistory_week_max)\n",
    "df_oldhistory = aggregate_week_data(oldhistory_week_min, oldhistory_week_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce31fab9-0972-47c5-ba81-cd111f2885bf",
   "metadata": {},
   "source": [
    "## Set Label\n",
    "\n",
    "Check in df_label if a user churned (\"pg_cancellation_confirmation\" or \"pg_submit_downgrade\" event) and add column \"label\" to df_user containing this info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a71c0920-dc9f-497b-bfdf-02dd128d5f45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_label = df_label.withColumn(\"label\", F.when(F.col(\"pg_cancellation_confirmation\")+F.col(\"pg_submit_downgrade\")>0, F.lit(1)).otherwise(F.lit(0))).select(\"userid\", \"label\")\n",
    "df_user = df_user.join(df_label, \"userId\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f8956c-3522-41fa-b4a0-d4571f0001ba",
   "metadata": {},
   "source": [
    "## Add history\n",
    "\n",
    "add new and old history to df_user. Rename columns to make them distinct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37b53bd-005a-4239-baa4-0840c06f6fbd",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Helper function\n",
    "\n",
    "Add a prefix to all columns in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "468f085c-857d-48b5-ba87-9b8884934ed5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prefix_columns(df_orig, prefix, do_not_change_cols):\n",
    "    \"\"\"\n",
    "    Input:  df_orig - Original DataFrame\n",
    "            prefix - string to be added to all column names\n",
    "            do_not_change_cols - liste of columns which should not be changed\n",
    "    Output: new DataFrame, where all columns were renamed (prefix was added to the name at the beginning)\n",
    "            except for the columns in \"do_not_change_cols\", those are unchanged.\n",
    "    \"\"\"\n",
    "    newcols = [prefix+col if not col in do_not_change_cols else col for col in df_orig.columns]\n",
    "    return df_orig.toDF(*newcols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b047c416-f672-457f-b139-1525152fb30c",
   "metadata": {},
   "source": [
    "Add prefix \"nh_\" to all columns in new-history and prefix \"oh_\" to all columns in old-history:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "18bc298a-90f4-4af0-bb94-80e27da491cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_user = df_user.join(prefix_columns(df_newhistory, \"nh_\", [\"userId\"]), \"userId\")\n",
    "df_user = df_user.join(prefix_columns(df_oldhistory, \"oh_\", [\"userId\"]), \"userId\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8ae202-d3ec-444f-89b2-00e25d799210",
   "metadata": {},
   "source": [
    "## Normalize Column values\n",
    "\n",
    "to make the values from the old and the new history better comparable, the absolute values (number of specific events) is divided by the aggregated session_hour (the time the user spent in Sparkify during the aggregated time interval).\n",
    "As an exception the session_start and session_hours are handled different. They are divided by the weeks of the aggregation interval. so, here, the new history is divided by 1 and the old history is divided by 3. So the session_start events are normalized to \"per-week\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df1b1d8b-4e13-4848-aaae-e635b7c24854",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for c in df_newhistory.columns:\n",
    "    if not c in [\"userId\", \"session_hours\", \"session_start\"]:\n",
    "        df_user = df_user.withColumn(\"nhn_\"+c, F.col(\"nh_\"+c)/F.greatest(F.col(\"nh_session_hours\"), F.lit(0.01)))\n",
    "df_user = df_user.withColumn(\"nhn_session_hours\", F.col(\"nh_session_hours\")/PAST_NEAR_HISTORY_WEEKS)\n",
    "df_user = df_user.withColumn(\"nhn_session_start\", F.col(\"nh_session_start\")/PAST_NEAR_HISTORY_WEEKS)\n",
    "\n",
    "for c in df_oldhistory.columns:\n",
    "    if not c in [\"userId\", \"session_hours\", \"session_start\"]:\n",
    "        df_user = df_user.withColumn(\"ohn_\"+c, F.col(\"oh_\"+c)/F.greatest(F.col(\"oh_session_hours\"), F.lit(0.01)))\n",
    "df_user = df_user.withColumn(\"ohn_session_hours\", F.col(\"oh_session_hours\")/PAST_OLD_HISTORY_WEEKS)\n",
    "df_user = df_user.withColumn(\"ohn_session_start\", F.col(\"oh_session_start\")/PAST_OLD_HISTORY_WEEKS)        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3071a849-4318-4507-bf83-a1d6aa9c1d09",
   "metadata": {},
   "source": [
    "## Create Feature Column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474fb586-432d-41cd-8492-9836da50a9df",
   "metadata": {},
   "source": [
    "Multiple test run have shown, that the best selection for training are the normalized columns \"nhn_...\" and \"ohn_...\".  \n",
    "The columns containing the absolute values are not used for training.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "596e0688-858c-41cc-ab49-fbe51206e743",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "featureCols = [\"paid\", \"usermale\", \"userregistration\"]\n",
    "featureCols = [*featureCols, *[col for col in df_user.columns if col.startswith(\"nhn_\") or  col.startswith(\"ohn_\")]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0e108bc6-30c9-4fd9-92a7-eb6b59a71d9c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['paid',\n",
       " 'usermale',\n",
       " 'userregistration',\n",
       " 'nhn_num_events',\n",
       " 'nhn_pg_about',\n",
       " 'nhn_pg_add_friend',\n",
       " 'nhn_pg_add_to_playlist',\n",
       " 'nhn_pg_cancel',\n",
       " 'nhn_pg_cancellation_confirmation',\n",
       " 'nhn_pg_downgrade',\n",
       " 'nhn_pg_error',\n",
       " 'nhn_pg_help',\n",
       " 'nhn_pg_home',\n",
       " 'nhn_pg_login',\n",
       " 'nhn_pg_logout',\n",
       " 'nhn_pg_nextsong',\n",
       " 'nhn_pg_register',\n",
       " 'nhn_pg_roll_advert',\n",
       " 'nhn_pg_save_settings',\n",
       " 'nhn_pg_settings',\n",
       " 'nhn_pg_submit_downgrade',\n",
       " 'nhn_pg_submit_registration',\n",
       " 'nhn_pg_submit_upgrade',\n",
       " 'nhn_pg_thumbs_down',\n",
       " 'nhn_pg_thumbs_up',\n",
       " 'nhn_pg_upgrade',\n",
       " 'nhn_status_307',\n",
       " 'nhn_status_404',\n",
       " 'nhn_session_hours',\n",
       " 'nhn_session_start',\n",
       " 'ohn_num_events',\n",
       " 'ohn_pg_about',\n",
       " 'ohn_pg_add_friend',\n",
       " 'ohn_pg_add_to_playlist',\n",
       " 'ohn_pg_cancel',\n",
       " 'ohn_pg_cancellation_confirmation',\n",
       " 'ohn_pg_downgrade',\n",
       " 'ohn_pg_error',\n",
       " 'ohn_pg_help',\n",
       " 'ohn_pg_home',\n",
       " 'ohn_pg_login',\n",
       " 'ohn_pg_logout',\n",
       " 'ohn_pg_nextsong',\n",
       " 'ohn_pg_register',\n",
       " 'ohn_pg_roll_advert',\n",
       " 'ohn_pg_save_settings',\n",
       " 'ohn_pg_settings',\n",
       " 'ohn_pg_submit_downgrade',\n",
       " 'ohn_pg_submit_registration',\n",
       " 'ohn_pg_submit_upgrade',\n",
       " 'ohn_pg_thumbs_down',\n",
       " 'ohn_pg_thumbs_up',\n",
       " 'ohn_pg_upgrade',\n",
       " 'ohn_status_307',\n",
       " 'ohn_status_404',\n",
       " 'ohn_session_hours',\n",
       " 'ohn_session_start']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featureCols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1ea2e6-febd-40b1-8fad-31c62313cce1",
   "metadata": {},
   "source": [
    "Pack all feature columns as vector in the column \"feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ef5db5ff-bcc0-4ec7-9dc5-2062fdf45961",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=featureCols, outputCol=\"features\")\n",
    "df_testtrain_vec=assembler.transform(df_user).select(\"userId\", \"label\",\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d312ca71-5ddc-4457-b52a-7fa156395901",
   "metadata": {},
   "source": [
    "## Train / Test split\n",
    "\n",
    "Divide the data into Train- and Testdata. 70% of the data will be used for training, the other 30% are used for testing (validation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4be24069-b4c4-43ed-8ba6-9c048745236a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train, df_test = df_testtrain_vec.randomSplit([0.7, 0.3], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3784334-11e0-4822-9d2f-a646935b2b5a",
   "metadata": {},
   "source": [
    "## Handling unbalanced data\n",
    "\n",
    "The data is unbalanced. There are much more users, who do not churn, than users who churn.  \n",
    "To handle this, we add a \"weight\" column, which can be used during training, to give the data different weights.  \n",
    "The sum of all weights for rows labeled with \"1\" should be equal to the sum of all weights of rows labeld with \"0\".\n",
    "\n",
    "Here is a helper function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f28ef1e4-1d0d-48b3-a348-69142bb4f360",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_weight_col(df_train):\n",
    "    \"\"\"\n",
    "    Input:  Training DataFrame with a column \"label\" containing a binary classification (1 or 0)\n",
    "    Output: Newly created DataFrame, which contains an additional \"weight\" column, which compensates \n",
    "            the different frequency of both partitions.  \n",
    "    \"\"\"\n",
    "    label_counts = df_train.agg(F.sum(F.col(\"label\")).alias(\"l1\"), F.sum(1-F.col(\"label\")).alias(\"l0\")).collect()[0]\n",
    "    l0 = label_counts.l0\n",
    "    l1 = label_counts.l1\n",
    "    w1 = l0 / (l0+l1)\n",
    "    w0 = l1 / (l0+l1)\n",
    "    print(f\"label 0: {l0}, label 1: {l1}\")\n",
    "    df_result = df_train.withColumn(\"weight\", F.when(F.col(\"label\")==1, F.lit(w1)).otherwise(F.lit(w0)))\n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7177578b-cb93-4d04-8a00-803217cb9f70",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label 0: 6315, label 1: 1183\n"
     ]
    }
   ],
   "source": [
    "df_train = add_weight_col(df_train)\n",
    "df_train = df_train.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e42d8964-db81-456d-a2e6-6fd5294c26d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### CLASSIFIERS\n"
     ]
    }
   ],
   "source": [
    "print(f\"### CLASSIFIERS\")\n",
    "\n",
    "def confuse(df_test_pred):\n",
    "    n00 = df_test_pred.where((F.col(\"label\")==0)&(F.col(\"prediction\")==0)).count()\n",
    "    n01 = df_test_pred.where((F.col(\"label\")==0)&(F.col(\"prediction\")==1)).count()\n",
    "    n10 = df_test_pred.where((F.col(\"label\")==1)&(F.col(\"prediction\")==0)).count()\n",
    "    n11 = df_test_pred.where((F.col(\"label\")==1)&(F.col(\"prediction\")==1)).count()\n",
    "    s00 = \"{:5d}\".format(n00)\n",
    "    s01 = \"{:5d}\".format(n01)\n",
    "    s10 = \"{:5d}\".format(n10)\n",
    "    s11 = \"{:5d}\".format(n11)\n",
    "    print(f\"                  \")\n",
    "    print(f\" Confusion Matrix: \")\n",
    "    print(f\"                  \")\n",
    "    print(f\"     | prediction| \")\n",
    "    print(f\"     |   0 |  1  | \")\n",
    "    print(f\" ----+-----+-----+ \")\n",
    "    print(f\" l 0 |{s00}|{s01}| \")\n",
    "    print(f\" b --+-----+-----+ \")\n",
    "    print(f\" l 1 |{s10}|{s11}| \")\n",
    "    print(f\" ----+-----+-----+ \")\n",
    "    print(f\"                   \")\n",
    "    TP = n11\n",
    "    TN = n00\n",
    "    FP = n01\n",
    "    FN = n10\n",
    "    accuracy = 0\n",
    "    if TP+TN+FP+FN!=0:\n",
    "        accuracy = (TP+TN)/(TP+TN+FP+FN)\n",
    "    precision = 0\n",
    "    if TP+FP!=0:\n",
    "        precision = TP/(TP+FP)\n",
    "    recall = 0\n",
    "    if TP+FN!=0:\n",
    "        recall = TP/(TP+FN)\n",
    "    f1 = 0\n",
    "    if precision+recall!=0:\n",
    "        f1 = 2*precision*recall/(precision+recall)\n",
    "    print(f\"CALC\")\n",
    "    print(f\"  accuraccy: {accuracy}\")\n",
    "    print(f\"  precision: {precision}\")\n",
    "    print(f\"  recall:    {recall}\")\n",
    "    print(f\"  f1:        {f1}\")\n",
    "    # https://towardsdatascience.com/matthews-correlation-coefficient-when-to-use-it-and-when-to-avoid-it-310b3c923f7e\n",
    "    mcc = -9\n",
    "    nenn = (TN+FN)*(FP+TP)*(TN+FP)*(FN+TP)\n",
    "    if nenn!=0:   \n",
    "        mcc = (TN*TP-FP*FN)/math.sqrt(nenn)\n",
    "    print(f\"  mcc:       {mcc}\")\n",
    "    return (accuracy, precision, recall, f1)\n",
    "    \n",
    "    \n",
    "def hyper_tune_rf(num_tree_values, max_depth_values):\n",
    "    best_f1 = -1\n",
    "    best_model = None\n",
    "    best_model_name = \"?\"\n",
    "    for num_trees in num_tree_values:\n",
    "        for max_depth in max_depth_values:\n",
    "            model_name = f\"rf_{num_trees}_{max_depth}\"\n",
    "            rf = RandomForestClassifier(featuresCol=\"features\", numTrees=num_trees, maxDepth=max_depth, weightCol = \"weight\", seed=42)\n",
    "            rf_model = rf.fit(df_train)\n",
    "            predict_test  = rf_model.transform(df_test)\n",
    "            accuracy, precision, recall, f1 = confuse(predict_test)\n",
    "            print(f\"  {model_name}: f1 {f1}\")\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                best_model = rf_model\n",
    "                best_model_name = model_name\n",
    "    print(f\"best f1 {f1} for {best_model_name}\")\n",
    "    return (best_model, best_f1, best_model_name)\n",
    "\n",
    "\n",
    "def hyper_tune_lr(max_iters, reg_params, elastic_net_params):\n",
    "    # https://towardsdatascience.com/beginners-guide-to-linear-regression-with-pyspark-bfc39b45a9e9\n",
    "    evaluator = RegressionEvaluator(predictionCol=\"prediction_orig\", labelCol=\"label\", metricName=\"rmse\") \n",
    "    \n",
    "    best_err = 9999\n",
    "    best_model = None\n",
    "    best_model_name = \"?\"\n",
    "    for  max_iter in  max_iters:\n",
    "        for reg_param in reg_params:\n",
    "            for elastic_net_param in elastic_net_params:\n",
    "                model_name = f\"lr_{max_iter}_{reg_param}_{elastic_net_param}\"\n",
    "                lr = LinearRegression(featuresCol=\"features\", maxIter= max_iter, regParam=reg_param, elasticNetParam=elastic_net_param)\n",
    "                model = lr.fit(df_train)\n",
    "                predict_test  = model.transform(df_test)\n",
    "                predict_test = predict_test.withColumnRenamed(\"prediction\", \"prediction_orig\")\n",
    "                err = evaluator.evaluate(predict_test)\n",
    "                print(f\"err: {err}\")\n",
    "                thr = 0.15\n",
    "                predict_test = predict_test.withColumn(\"prediction\", F.when(F.col(\"prediction_orig\")>=thr,1).otherwise(0))\n",
    "                accuracy, precision, recall, f1 = confuse(predict_test)\n",
    "                print(f\"  {model_name}: f1 {f1}\")\n",
    "                if err < best_err:\n",
    "                    best_err = err\n",
    "                    best_model = model\n",
    "                    best_model_name = model_name\n",
    "    print(f\"best f1 {f1} for {best_model_name}\")\n",
    "    return (best_model, best_err, best_model_name)\n",
    "\n",
    "\n",
    "def hyper_tune_dt(max_depths, max_bins_list):\n",
    "    best_f1 = -1\n",
    "    best_model = None\n",
    "    best_model_name = \"?\"\n",
    "    for  max_depth in max_depths:\n",
    "        for max_bins in max_bins_list:\n",
    "            model_name = f\"dt_{max_depth}_{max_bins}\"\n",
    "            dt = DecisionTreeClassifier(featuresCol=\"features\", maxDepth=max_depth, maxBins=max_bins)\n",
    "            model = dt.fit(df_train)\n",
    "            predict_test  = model.transform(df_test)\n",
    "            accuracy, precision, recall, f1 = confuse(predict_test)\n",
    "            print(f\"  {model_name}: f1 {f1}\")\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                best_model = model\n",
    "                best_model_name = model_name\n",
    "    print(f\"best f1 {f1} for {best_model_name}\")\n",
    "    return (best_model, best_f1, best_model_name)\n",
    "\n",
    "\n",
    "def hyper_tune_sv(max_iters, reg_params):\n",
    "    best_f1 = -1\n",
    "    best_model = None\n",
    "    best_model_name = \"?\"\n",
    "    for  max_iter in max_iters:\n",
    "        for reg_param in reg_params:\n",
    "            model_name = f\"svm_{max_iter}_{reg_param}\"\n",
    "            lsvc = LinearSVC(featuresCol=\"features\", maxIter=max_iter, regParam=reg_param)\n",
    "            model = lsvc.fit(df_train)\n",
    "            predict_test  = model.transform(df_test)\n",
    "            accuracy, precision, recall, f1 = confuse(predict_test)\n",
    "            print(f\"  {model_name}: f1 {f1}\")\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                best_model = model\n",
    "                best_model_name = model_name\n",
    "    print(f\"best f1 {f1} for {best_model_name}\")\n",
    "    return (best_model, best_f1, best_model_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a2aee875-b3ae-44d0-9187-ec8275307aa7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Fit scaler to train dataset\n",
    "#scaler = MaxAbsScaler().setInputCol('features').setOutputCol('scaled_features')\n",
    "#df_train = df_train.drop(\"scaled_features\")\n",
    "#scaler_model = scaler.fit(df_train)\n",
    "## Scale train and test features\n",
    "#df_train = scaler_model.transform(df_train)\n",
    "#df_test = df_test.drop(\"scaled_features\")\n",
    "#df_test = scaler_model.transform(df_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1146d7-27eb-4737-ae73-70dcc79803c5",
   "metadata": {},
   "source": [
    "## Train Models\n",
    "\n",
    "Different Models can now be trained and validated against the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "98acd19d-53e1-4c78-a732-d8984c479011",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  \n",
      " Confusion Matrix: \n",
      "                  \n",
      "     | prediction| \n",
      "     |   0 |  1  | \n",
      " ----+-----+-----+ \n",
      " l 0 | 1762|  890| \n",
      " b --+-----+-----+ \n",
      " l 1 |  164|  322| \n",
      " ----+-----+-----+ \n",
      "                   \n",
      "CALC\n",
      "  accuraccy: 0.6641172721478649\n",
      "  precision: 0.26567656765676567\n",
      "  recall:    0.6625514403292181\n",
      "  f1:        0.3792697290930506\n",
      "  mcc:       0.24294854801598947\n",
      "  rf_100_5: f1 0.3792697290930506\n",
      "best f1 0.3792697290930506 for rf_100_5\n"
     ]
    }
   ],
   "source": [
    "model, f1, model_name = hyper_tune_rf([100], [5])  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b146196-e272-4537-90a7-239a5e1df1e8",
   "metadata": {},
   "source": [
    "## Save the model\n",
    "\n",
    "The model is stored and can be loaded for doing predictions on newly collected data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "54651be2-0cae-437d-b15c-05bd63d6bb27",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### SAVE MODEL rf_100_5 37.926972909305064\n",
      "model saved to s3a://udacity-dsnd/sparkify/output/05-model-sparkify_event_data_rf_100_5_f1val0.379\n"
     ]
    }
   ],
   "source": [
    "# -----------------\n",
    "\n",
    "print(f\"### SAVE MODEL {model_name} {f1*100}\")\n",
    "model_url = f'{MODEL_URL}_{model_name}_f1val{round(f1,3)}'\n",
    "model.write().overwrite().save(model_url)\n",
    "print(f\"model saved to {model_url}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8b787c-92a5-4e47-80c9-3432b50c82d7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Feature Importance\n",
    "\n",
    "Which features were selected to make the predictions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b008cf24-a1ba-47a0-9358-030e895948e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ohn_session_hours', 0.14584920440860216),\n",
       " ('ohn_session_start', 0.1071155057665855),\n",
       " ('nhn_session_hours', 0.07361439347198338),\n",
       " ('paid', 0.0664014373321682),\n",
       " ('ohn_pg_downgrade', 0.05502855242220322),\n",
       " ('ohn_pg_about', 0.05126235028010777),\n",
       " ('nhn_session_start', 0.03257332015388863),\n",
       " ('ohn_pg_login', 0.031356496574380596),\n",
       " ('ohn_pg_thumbs_up', 0.02738591484059622),\n",
       " ('ohn_pg_settings', 0.02442703365003172),\n",
       " ('nhn_pg_downgrade', 0.02312319305938885),\n",
       " ('nhn_pg_settings', 0.018972942112722876),\n",
       " ('ohn_pg_logout', 0.017614706100878206),\n",
       " ('nhn_pg_add_friend', 0.01682085101030543),\n",
       " ('nhn_pg_roll_advert', 0.01648004111998699),\n",
       " ('nhn_pg_nextsong', 0.015579254523506893),\n",
       " ('ohn_pg_home', 0.01510688537047101),\n",
       " ('nhn_pg_thumbs_down', 0.01503028877306283),\n",
       " ('nhn_pg_help', 0.014747180288166345),\n",
       " ('nhn_status_307', 0.01407328272069372),\n",
       " ('ohn_pg_add_friend', 0.012779985419448144),\n",
       " ('ohn_num_events', 0.012645668614730378),\n",
       " ('nhn_num_events', 0.012642013781147104),\n",
       " ('ohn_pg_save_settings', 0.011917352264853251),\n",
       " ('ohn_pg_thumbs_down', 0.011884426831654523),\n",
       " ('nhn_pg_thumbs_up', 0.011397431936576573),\n",
       " ('ohn_pg_help', 0.011112454960717014),\n",
       " ('nhn_pg_logout', 0.010984597611903625),\n",
       " ('ohn_pg_add_to_playlist', 0.010760067318115444),\n",
       " ('nhn_pg_add_to_playlist', 0.010564397132042746),\n",
       " ('ohn_pg_roll_advert', 0.010365036209211493),\n",
       " ('nhn_pg_about', 0.009906079308836099),\n",
       " ('ohn_pg_nextsong', 0.009030695878362811),\n",
       " ('userregistration', 0.008565710844058619),\n",
       " ('ohn_status_307', 0.008463314124599622),\n",
       " ('nhn_pg_home', 0.007927557515855447),\n",
       " ('nhn_pg_login', 0.006763653626369911),\n",
       " ('ohn_status_404', 0.005570430473210094),\n",
       " ('ohn_pg_error', 0.005483393842214683),\n",
       " ('ohn_pg_upgrade', 0.004453981181091599),\n",
       " ('ohn_pg_submit_downgrade', 0.004163439793327145),\n",
       " ('nhn_status_404', 0.0039391496779014245),\n",
       " ('nhn_pg_upgrade', 0.0028581182568549143),\n",
       " ('nhn_pg_save_settings', 0.002644063868891976),\n",
       " ('ohn_pg_submit_upgrade', 0.002615950593994764),\n",
       " ('nhn_pg_submit_upgrade', 0.0018142404058419238),\n",
       " ('nhn_pg_error', 0.00168601461340459),\n",
       " ('nhn_pg_submit_downgrade', 0.0012005098314862724),\n",
       " ('ohn_pg_cancel', 0.0008176361543326413),\n",
       " ('nhn_pg_cancel', 0.0006880963656742739),\n",
       " ('ohn_pg_cancellation_confirmation', 0.0006874298025438695),\n",
       " ('nhn_pg_cancellation_confirmation', 0.0006173249428524792),\n",
       " ('usermale', 0.0003193614312271184),\n",
       " ('nhn_pg_register', 0.00016758140693720974),\n",
       " ('nhn_pg_submit_registration', 0.0),\n",
       " ('ohn_pg_register', 0.0),\n",
       " ('ohn_pg_submit_registration', 0.0)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featimp = model.featureImportances\n",
    "nameimp = {}\n",
    "for i in range(len(featimp)):\n",
    "    nameimp[featureCols[i]] = featimp[i]\n",
    "sorted(nameimp.items(), key=lambda x:-x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec09dc0-6b53-4e4c-a2fc-64b9e42bfab4",
   "metadata": {},
   "source": [
    "The selected features make sense session_hours old-history vs. session_hours new-history.  \n",
    "Also \"paid\" is useful, because a downgrade is only possible, if the user has a \"paid\" account.  \n",
    "\"downgrade\" is obvious a good choice   :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef69a4a-8796-499d-bd29-cab609172949",
   "metadata": {},
   "source": [
    "# Comments on the Result\n",
    "\n",
    "I really struggeled hard with the bad results my model has.  \n",
    "\n",
    "I tried multiple optimizations, but in any way, there is either a big number of False-Positives,  \n",
    "which is bad for the precision, or there are too many False-Negatives which is bad for the recall.\n",
    "\n",
    "Looking at the current confusion Matrix\n",
    "\n",
    "```\n",
    "     | prediction| \n",
    "     |   0 |  1  | \n",
    " ----+-----+-----+ \n",
    " l 0 | 1762|  890| \n",
    " b --+-----+-----+ \n",
    " l 1 |  164|  322| \n",
    " ----+-----+-----+ \n",
    "```\n",
    "\n",
    "we can say the following:\n",
    "\n",
    " * \"66% of the users who will churn are detected\"\n",
    " \n",
    "So, if our offer to bind them could convince them to stay, it would be helpful.  \n",
    "On the other hand we have 3 times more False-Postives than True-Positives.  \n",
    "\n",
    "Calculating the costs we can say, that only 1/4 of the costs invested in binding users to Sparkify\n",
    "hits the right persons. \n",
    "\n",
    "So, the calcuation is:\n",
    "\n",
    "```\n",
    "CHURNERS * 2/3 * BINDING_SUCCESS --> Users kept in Sparkify (paid or free)\n",
    "COST = COST_OF_OFFER * CHURNERS * 2.5\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1087fe0-1964-4c85-a5de-83be14d5fc6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87d4cb43-ef96-4630-9844-f463b7b03c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### STOP SPARK SESSION\n"
     ]
    }
   ],
   "source": [
    "print(\"### STOP SPARK SESSION\")\n",
    "spark.stop()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899ac3c4-e4ee-42f3-88c8-a46ae17737df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
